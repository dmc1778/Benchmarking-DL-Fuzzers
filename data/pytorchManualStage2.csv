pytorch,https://github.com/pytorch/pytorch/issues/132386,2024-08-01T07:02:52Z,torch cpu float16 range is not aligned with scipy on polygamma.,torch.polygamma,2.5.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/133163,2024-08-10T11:43:57Z,`torch.load` with `weights_only=True` does not allow `bytes` or `bytearray`,torch.load,2.5.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/133114,2024-08-09T18:52:59Z,Conv1D: `CUDNN_BACKEND_OPERATION: cudnnFinalize Failed cudnn_status: CUDNN_STATUS_BAD_PARAM`,torch.nn.functional.grad.conv1d_weight,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/133010,2024-08-08T16:21:49Z,torch.multiprocessing.start_processes is blocking with large input arguments,torch.multiprocessing.start_processes,2.5.0,12.2
pytorch,https://github.com/pytorch/pytorch/issues/132857,2024-08-07T11:32:59Z,torch.library.opcheck generates gradients with strides of 0,torch.library.opcheck,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132695,2024-08-05T20:19:16Z,Backward on nested jagged tensor vector multiplication throws error on 0 dim (but not last dim),torch.nested.as_nested_tensor,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132644,2024-08-05T09:40:07Z,`torch.distributed.pipelining` hang and timeout in CPU gloo backend,torch.distributed.pipelining,2.5.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/132640,2024-08-05T09:13:06Z,CUDA Invalid Memory Access caused by torch.distributed.barrier() with NCCL,torch.distributed.barrier,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132618,2024-08-04T22:11:07Z,Reducing jagged tensor using unbind results in error in torch.compile,torch.compile,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132618,2024-08-04T22:11:07Z,Reducing jagged tensor using unbind results in error in torch.compile,torch.nested.as_nested_tensor,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132618,2024-08-04T22:11:07Z,Reducing jagged tensor using unbind results in error in torch.compile,torch.jagged,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132618,2024-08-04T22:11:07Z,Reducing jagged tensor using unbind results in error in torch.compile,torch.unbind,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132618,2024-08-04T22:11:07Z,Reducing jagged tensor using unbind results in error in torch.compile,torch.sum,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132559,2024-08-02T23:04:39Z,How to fix tensor.numpy() not supported for torch.export with strict=False,torch.export,2.5.0,12
pytorch,https://github.com/pytorch/pytorch/issues/132471,2024-08-02T02:32:50Z,fully_shard with a gloo group segfault,torch.distributed._composable.fsd,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132471,2024-08-02T02:32:50Z,fully_shard with a gloo group segfault,fully_shard,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132360,2024-08-01T00:38:22Z,[AOTI] AOTI doesn't work well with torch.select,torch.select,2.5.0,12
pytorch,https://github.com/pytorch/pytorch/issues/132218,2024-07-31T07:08:42Z,[profiler] CUDA runtime op has wrong device time because of lazy init,torch.profiler.profile,2.5.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/132047,2024-07-29T14:57:56Z,Inconsistent behavior of gumbel_softmax on cpu vs. gpu,torch.gumbel_softmax,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132045,2024-07-29T14:41:45Z,`torch.scatter`'s behavior is different on cpu and gpu on torch 2.5.0.dev20240708+cu121,torch.scatter,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132041,2024-07-29T13:47:08Z,`torch.nn.functional.max_unpool2d`'s behavior is different on cpu and gpu on torch 2.5.0.dev20240708+cu121,torch.nn.functional.max_unpool2d,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132031,2024-07-29T11:10:31Z,`torch.cross`'s behavior is different on cpu and gpu on torch 2.5.0.dev20240708+cu121,torch.cross,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132030,2024-07-29T10:36:08Z,"torch.onnx.is_in_onnx_export(): RuntimeError: 0 INTERNAL ASSERT FAILED at ""../torch/csrc/jit/ir/alias_analysis.cpp"":608, please report a bug to PyTorch. We don't have an op for aten::mul but it isn't a special case.",torch.onnx.is_in_onnx_export,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132029,2024-07-29T10:29:29Z,torch.QUInt4x2Storage: RuntimeError cuda_dispatch_ptr INTERNAL ASSERT FAILED on torch 2.5.0.dev20240708+cu121,torch.quantize_per_tensor,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132029,2024-07-29T10:29:29Z,torch.QUInt4x2Storage: RuntimeError cuda_dispatch_ptr INTERNAL ASSERT FAILED on torch 2.5.0.dev20240708+cu121,torch.quint4x2,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132028,2024-07-29T10:22:26Z,torch.sspaddmm: self.is_sparse() INTERNAL ASSERT FAILED on torch 2.5.0.dev20240708+cu121,torch.sspaddmm,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/131975,2024-07-27T08:19:10Z,torch.compile should not recompiles when `.requires_grad=True` under `torch.no_grad()` context,torch.compile,2.5.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/131805,2024-07-25T20:24:02Z,[Dynamo] Bad accuracy detected for torch.flip on CUDA,torch.flip,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/131654,2024-07-24T14:49:04Z,torch.export.save doesn't support torch.float8_e4m3fn dtype,torch.export.save,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/131515,2024-07-23T20:43:37Z,torch.export.export failed with Dynamic shape on torch.ops.prims.broadcast_in_dim.default,torch.ops.prims.broadcast_in_dim.default,2.5.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/131319,2024-07-22T12:50:25Z,"RuntimeError: false INTERNAL ASSERT FAILED at ""../aten/src/ATen/native/BatchLinearAlgebra.cpp"":498",torch.triangular_solve,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/131318,2024-07-22T12:30:51Z,"RuntimeError: INTERNAL ASSERT FAILED at ""../aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp"":1306",torch.sum,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130975,2024-07-17T19:24:29Z,[torch.export] Torch Export produces incorrect program when python generators are used.,torch.export.export,2.5.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/130920,2024-07-17T09:45:14Z,Inconsistent floating point guards with numpy scalars,torch._dynamo.optimize,2.5.0,12.2
pytorch,https://github.com/pytorch/pytorch/issues/130829,2024-07-16T10:34:24Z,"`unbind_copy` gives unexpected results on 1-dimensional inputs, or 0-dimensional outputs",torch.unbind_copy,2.5.0,12.2
pytorch,https://github.com/pytorch/pytorch/issues/130825,2024-07-16T08:42:42Z,torch.compile fails for complex nested_tensor code,torch.autograd.backward,2.5.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/130825,2024-07-16T08:42:42Z,torch.compile fails for complex nested_tensor code,torch.nn.modules.module,2.5.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/130825,2024-07-16T08:42:42Z,torch.compile fails for complex nested_tensor code,torch._dynamo.eval_frame,2.5.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/130825,2024-07-16T08:42:42Z,torch.compile fails for complex nested_tensor code,torch._dynamo.convert_frame,2.5.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/130825,2024-07-16T08:42:42Z,torch.compile fails for complex nested_tensor code,torch._dynamo.utils,2.5.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/130825,2024-07-16T08:42:42Z,torch.compile fails for complex nested_tensor code,torch._dynamo.symbolic_convert,2.5.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/130825,2024-07-16T08:42:42Z,torch.compile fails for complex nested_tensor code,torch._dynamo.variables.constant,2.5.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/130825,2024-07-16T08:42:42Z,torch.compile fails for complex nested_tensor code,torch._dynamo.variables.tensor,2.5.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/130701,2024-07-14T22:16:33Z,fx.wrap() doesn't really work for things in torch/*,torch.fx.wrap,2.5.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/130538,2024-07-11T14:01:36Z,"nested_tensor LayerNorm fails with shape (*,*,j1,*) (but works with (*,j1,*,*))",torch.nn.LayerNorm,2.5.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/130538,2024-07-11T14:01:36Z,"nested_tensor LayerNorm fails with shape (*,*,j1,*) (but works with (*,j1,*,*))",torch.nested.nested_tensor,2.5.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/130538,2024-07-11T14:01:36Z,"nested_tensor LayerNorm fails with shape (*,*,j1,*) (but works with (*,j1,*,*))",torch.nn.functional.layer_norm,2.5.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/130538,2024-07-11T14:01:36Z,"nested_tensor LayerNorm fails with shape (*,*,j1,*) (but works with (*,j1,*,*))",torch.layer_norm,2.5.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/130394,2024-07-09T23:05:28Z,[Inductor] Incorrect strides for function containing `torch.empty_strided`. Causes output to crash.,torch.empty_strided,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130152,2024-07-05T16:44:29Z,[export] Running torch._inductor.compile directly on an ExportedProgram fails with non-Tensor inputs,torch._inductor.compile,2.5.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/130123,2024-07-04T17:35:35Z,randperm + torch.compile + SAC + CUDA graphs doesn't work,torch.randperm,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129820,2024-06-29T13:39:37Z,torch.lt with out parameter does not return the correct shape,torch.lt,2.5.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/129690,2024-06-27T19:31:23Z,[torch.export] Simple model fails to export with dynamic shapes,torch.export,2.5.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/129673,2024-06-27T16:50:19Z,torch compile error with `torch.Tensor.unsqueeze_`,torch.Tensor.unsqueeze_,2.5.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/129579,2024-06-26T17:13:50Z,scaled_dot_product_attention fails on Ampere arch with head_dim > 128,torch.nn.functional.scaled_dot_product_attention,2.5.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/129534,2024-06-25T23:53:48Z,Dynamo export: Fake tensor broadcast error,torch.onnx.dynamo_export,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128961,2024-06-18T14:44:02Z,`torch.compile` fails with `fullgraph=True` when accessing `getitem` of a `Tensor` subclass,torch.compile,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128944,2024-06-18T10:55:31Z,torch.compile graph break due to unsupported builtin filter function,torch.compile,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128942,2024-06-18T09:42:57Z,torch.compile graph break with unsupported LOAD_BUILD_CLASS,torch._dynamo,2.5.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/133204,2024-08-12T08:44:05Z,Add nondeterministic alert to `.scatter_`,torch.scatter_,2.5.0,12.5
pytorch,https://github.com/pytorch/pytorch/issues/133176,2024-08-11T00:06:42Z,[torch.fx] _FunctionalizationMetadataProp.propagate method can't use non-tensor args,torch.fx.passes.reinplace._FunctionalizationMetadataProp,2.5.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/133063,2024-08-09T01:45:37Z,torch.compile Parsing error results in error,torch.compile,2.4.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/133062,2024-08-09T01:04:23Z,[Export]`torch.export.export` broken for input of dtype uint16,torch.export.export,2.4.0,N.A
pytorch,https://github.com/pytorch/pytorch/issues/133057,2024-08-09T00:41:41Z,[export],torch.ops.aten.channel_shuffle,2.4.0,N.A
pytorch,https://github.com/pytorch/pytorch/issues/132993,2024-08-08T11:44:56Z,torch.export.unflatten of a submodule used twice creates a strange and maybe incorrect graph,torch.export.export,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132993,2024-08-08T11:44:56Z,torch.export.unflatten of a submodule used twice creates a strange and maybe incorrect graph,torch.export.unflatten,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132986,2024-08-08T09:41:51Z,`.eval()` and `.train()` don't set value of `.training` properly on `torch.compile()` module,torch.compile,2.4.0,12.5
pytorch,https://github.com/pytorch/pytorch/issues/132986,2024-08-08T09:41:51Z,`.eval()` and `.train()` don't set value of `.training` properly on `torch.compile()` module,torch.nn.Linear,2.4.0,12.5
pytorch,https://github.com/pytorch/pytorch/issues/132986,2024-08-08T09:41:51Z,`.eval()` and `.train()` don't set value of `.training` properly on `torch.compile()` module,torch.eval,2.4.0,12.5
pytorch,https://github.com/pytorch/pytorch/issues/132986,2024-08-08T09:41:51Z,`.eval()` and `.train()` don't set value of `.training` properly on `torch.compile()` module,torch.train,2.4.0,12.5
pytorch,https://github.com/pytorch/pytorch/issues/132929,2024-08-07T20:59:23Z,Compile + torch.autograd.grad returns no gradients,torch.autograd.grad,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132928,2024-08-07T20:40:23Z,SparseSemiStructuredTensor fails on H100,torch.sparse.SparseSemiStructuredTensor,2.4.0,12.5
pytorch,https://github.com/pytorch/pytorch/issues/132928,2024-08-07T20:40:23Z,SparseSemiStructuredTensor fails on H100,torch.sparse.to_sparse_semi_structured,2.4.0,12.5
pytorch,https://github.com/pytorch/pytorch/issues/132928,2024-08-07T20:40:23Z,SparseSemiStructuredTensor fails on H100,torch.nn.functional.linear,2.4.0,12.5
pytorch,https://github.com/pytorch/pytorch/issues/132628,2024-08-05T05:36:24Z,set_optimizer_state_dict does not support partial state dict,torch.distributed.checkpoint.state_dict.set_optimizer_state_dict,2.4.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/132628,2024-08-05T05:36:24Z,set_optimizer_state_dict does not support partial state dict,torch.distributed.checkpoint.state_dict.get_optimizer_state_dict,2.4.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/132597,2024-08-04T03:27:37Z,storing std of tensor leads to inplace backwards error,torch.std,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132397,2024-08-01T10:53:17Z,tensor.to(device) not copying data correctly between two GPUs,torch.tensor.to,2.4.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/132388,2024-08-01T07:27:13Z,`amp.custom_fwd` interacts poorly with custom `autograd.Function` with `setup_context` separated from `forward`,torch.amp.custom_fwd,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132388,2024-08-01T07:27:13Z,`amp.custom_fwd` interacts poorly with custom `autograd.Function` with `setup_context` separated from `forward`,torch.amp.custom_bwd,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132388,2024-08-01T07:27:13Z,`amp.custom_fwd` interacts poorly with custom `autograd.Function` with `setup_context` separated from `forward`,torch.autograd.Function,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132366,2024-08-01T02:09:56Z,PyTorch's Distributed Checkpoint Cannot Save a Parameter of Size 1,torch.distributed.checkpoint,2.4.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/132366,2024-08-01T02:09:56Z,PyTorch's Distributed Checkpoint Cannot Save a Parameter of Size 1,torch.distributed.checkpoint.state_dict,2.4.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/132366,2024-08-01T02:09:56Z,PyTorch's Distributed Checkpoint Cannot Save a Parameter of Size 1,torch.distributed.fsdp,2.4.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/132148,2024-07-30T16:02:14Z,compile: `torch._subclasses.fake_tensor.FakeTensor` does not inherit from user-defined `torch.Tensor` subclass,torch._subclasses.fake_tensor.FakeTensor,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132020,2024-07-29T07:32:34Z,`torch.Tensor.to` ignores `memory_format` kwarg,torch.Tensor.to,2.4.0,12.5
pytorch,https://github.com/pytorch/pytorch/issues/131976,2024-07-27T08:30:00Z,DistributedCheckpoint's async_save doesn't work with 0-dimensional tensors under FSDP,torch.distributed.checkpoint.async_save,2.4.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/131753,2024-07-25T07:31:48Z,PyTorch complains ```A leaf node was passed to _will_engine_execute_node``` when calculating the gradient of a model warpped by ```FlopCounterMode```.,torch._C._will_engine_execute_node,2.4.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/131650,2024-07-24T14:23:19Z,"Pytorch 2.4, Cuda 12.4: RuntimeError: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1720538439675/work/aten/src/ATen/cuda/CUDAContext.cpp"":49, please report a bug to PyTorch. device=, num_gpus=",torch.cuda.get_device_name,2.4.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/131650,2024-07-24T14:23:19Z,"Pytorch 2.4, Cuda 12.4: RuntimeError: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1720538439675/work/aten/src/ATen/cuda/CUDAContext.cpp"":49, please report a bug to PyTorch. device=, num_gpus=",torch.cuda.device_count,2.4.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/131650,2024-07-24T14:23:19Z,"Pytorch 2.4, Cuda 12.4: RuntimeError: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1720538439675/work/aten/src/ATen/cuda/CUDAContext.cpp"":49, please report a bug to PyTorch. device=, num_gpus=",torch.cuda.init,2.4.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/131650,2024-07-24T14:23:19Z,"Pytorch 2.4, Cuda 12.4: RuntimeError: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1720538439675/work/aten/src/ATen/cuda/CUDAContext.cpp"":49, please report a bug to PyTorch. device=, num_gpus=",torch.device,2.4.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/131349,2024-07-22T18:00:00Z,[ONNX] opset 11 aten::index_put generating invalid add ops with None args,torch.index_put,2.4.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/131150,2024-07-19T07:28:19Z,Torch.compile does not recompile when called with different options but the same backend,torch.compile,2.4.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/130931,2024-07-17T14:09:35Z,torch.export.unflatten fails on sequential models,torch.export.unflatten,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130931,2024-07-17T14:09:35Z,torch.export.unflatten fails on sequential models,torch.export.export,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130861,2024-07-16T21:29:56Z,torch.fx.Tracer.record_stack_traces is broken in torch 2.4.0,torch.fx.Tracer.record_stack_traces,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129206,2024-06-21T05:06:44Z,Bug in calling full_tensor() when model is sharded with tensor parallel and FSDP-2,torch.distributed._tensor.api.DTensor,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129206,2024-06-21T05:06:44Z,Bug in calling full_tensor() when model is sharded with tensor parallel and FSDP-2,torch.distributed.checkpoint.state_dict.get_model_state_dict,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129206,2024-06-21T05:06:44Z,Bug in calling full_tensor() when model is sharded with tensor parallel and FSDP-2,torch.distributed.checkpoint.save,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129206,2024-06-21T05:06:44Z,Bug in calling full_tensor() when model is sharded with tensor parallel and FSDP-2,torch.distributed._composable.fsdp.fully_shard,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129206,2024-06-21T05:06:44Z,Bug in calling full_tensor() when model is sharded with tensor parallel and FSDP-2,torch.distributed.device_mesh.init_device_mesh,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129206,2024-06-21T05:06:44Z,Bug in calling full_tensor() when model is sharded with tensor parallel and FSDP-2,torch.nn.Parameter.full_tensor,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129206,2024-06-21T05:06:44Z,Bug in calling full_tensor() when model is sharded with tensor parallel and FSDP-2,torch.distributed.device_mesh,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129206,2024-06-21T05:06:44Z,Bug in calling full_tensor() when model is sharded with tensor parallel and FSDP-2,torch.distributed.get_rank,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128556,2024-06-12T21:36:19Z,Dynamo graph breaks when passing a constant value to a nn module initialized within a function,torch._dynamo,2.4.0,12
pytorch,https://github.com/pytorch/pytorch/issues/128381,2024-06-11T01:55:48Z,FP16/BF16 - FP8 mixed matmul causes core dump,torch._inductor.config.force_mixed_mm,2.4.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/127750,2024-06-03T07:47:42Z,aot_function cannot handle index_add,torch.ops.aten.index_add,2.4.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/127196,2024-05-26T15:11:58Z,torch.topk results differ on CPU and CUDA,torch.topk,2.4.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/126848,2024-05-22T05:15:12Z,torch.compile generates wrong code on CPU and compiled code replaces original function,torch.compile,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126685,2024-05-20T15:26:30Z,Dynamo fails to export data-dependent `Tensor.tolist`,torch.onnx.dynamo_export,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126685,2024-05-20T15:26:30Z,Dynamo fails to export data-dependent `Tensor.tolist`,torch.unique,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126685,2024-05-20T15:26:30Z,Dynamo fails to export data-dependent `Tensor.tolist`,torch.tensor_split,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126685,2024-05-20T15:26:30Z,Dynamo fails to export data-dependent `Tensor.tolist`,torch._dynamo.export,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126685,2024-05-20T15:26:30Z,Dynamo fails to export data-dependent `Tensor.tolist`,torch._dynamo.convert_frame._compile,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126685,2024-05-20T15:26:30Z,Dynamo fails to export data-dependent `Tensor.tolist`,torch._dynamo.variables.tensor.tolist,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126514,2024-05-17T06:35:31Z,"Weights become NaN with torch.compile optimizer capturable=True, lr=0.0, nn.Embedding",torch.compile,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126514,2024-05-17T06:35:31Z,"Weights become NaN with torch.compile optimizer capturable=True, lr=0.0, nn.Embedding",torch.nn.Embedding,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126242,2024-05-14T23:34:30Z,"torch.export 'inline in skipfiles: Signature.bind | bind /usr/lib/python3.10/inspect.py, skipped according trace_rules.lookup SKIP_DIRS'",torch.export,2.4.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/126071,2024-05-13T14:36:52Z,Dynamo Export: Some weights are exported as extra inputs,torch.export.export,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126071,2024-05-13T14:36:52Z,Dynamo Export: Some weights are exported as extra inputs,torch.onnx.dynamo_export,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126071,2024-05-13T14:36:52Z,Dynamo Export: Some weights are exported as extra inputs,torch.onnx.export,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126024,2024-05-12T11:57:46Z,Can not use torch.compile with dynamic=Ture when using multi-threads,torch.compile,2.4.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/125539,2024-05-04T22:27:10Z,`triton` package that ships with torch is not compatible with `torch.ops.matmul`,torch.ops.matmul,2.4.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/124796,2024-04-23T22:37:21Z,Dynamo Export Support for Qwen/Qwen-7B-Chat: Mutating module attribute _ntk_alpha_cached_list during export,torch._dynamo.export,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/124793,2024-04-23T22:26:35Z,Dynamo Export Support for Google/Gemma-2B: Mutating module attribute inv_freq during export,torch._dynamo.export,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/124714,2024-04-23T07:47:18Z,`torch.distributed` hangs when using `torch.distributed.barrier` before any other communication primitives.,torch.distributed.barrier,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/124550,2024-04-20T17:20:08Z,`torch.func.linearize` fails when linearizing functions of gradients,torch.func.linearize,2.4.0,12.1.1
pytorch,https://github.com/pytorch/pytorch/issues/124507,2024-04-19T18:14:58Z,torch.export of ResNet with dynamic height fails due to constraint violation,torch.export,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/124291,2024-04-17T15:10:19Z,vmap over getitem indexing raises a RunTime error,torch.vmap,2.4.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/124256,2024-04-17T03:59:57Z,"[torch.compile] Conv1d failed: AssertionError: expected size 33==33, stride 1==50 at dim=1",torch.compile,2.4.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/124247,2024-04-17T02:17:21Z,"[torch.compile] torch._dynamo.exc.TorchRuntimeError: Failed running call_function <method 'numpy' of 'torch._C.TensorBase' objects>(*(FakeTensor(..., size=(32, 3, 64, 64)),), **{})",torch.Tensor.numpy,2.4.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/123996,2024-04-13T02:17:21Z,MSE loss triggers ZeroTensor immutable error inside `torch.func`,torch.nn.functional.mse_loss,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123972,2024-04-12T21:06:00Z,Dynamo export: limited support in Torch.cond,torch.cond,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123972,2024-04-12T21:06:00Z,Dynamo export: limited support in Torch.cond,torch._dynamo.export,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123702,2024-04-10T03:45:45Z,Issue with Vulkan Device for nn.Conv2d Operation,torch.nn.Conv2d,2.4.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/123471,2024-04-05T20:04:36Z,Inductor: expected scalar type Long but found Float for int literal,torch.ops._c10d_functional.all_to_all_single,2.4.0,12
pytorch,https://github.com/pytorch/pytorch/issues/123447,2024-04-05T15:04:35Z,dist.barrier() hangs after calling async_save,torch.distributed.checkpoint.async_save,2.4.0,12
pytorch,https://github.com/pytorch/pytorch/issues/123447,2024-04-05T15:04:35Z,dist.barrier() hangs after calling async_save,torch.distributed.barrier,2.4.0,12
pytorch,https://github.com/pytorch/pytorch/issues/123245,2024-04-03T08:19:36Z,torch.overrides.get_overridable_functions list functions that are not overridable via __torch_function__,torch.overrides.get_overridable_functions,2.4.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/123245,2024-04-03T08:19:36Z,torch.overrides.get_overridable_functions list functions that are not overridable via __torch_function__,torch.utils.generate_methods_for_privateuse1_backend,2.4.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/123122,2024-04-01T20:32:29Z,[torch.export] Unflattened BERT missing positional argument,torch.export.export,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123122,2024-04-01T20:32:29Z,[torch.export] Unflattened BERT missing positional argument,torch.export.unflatten,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123042,2024-03-30T23:54:19Z,ImportError: cannot import name 'triton_key' from 'triton.compiler.compiler',torch._dynamo.config,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/122773,2024-03-27T09:03:46Z,torch.gather - Cannot call numel() on tensor with symbolic sizes/strides,torch.gather,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/133252,2024-08-12T18:45:29Z,"[Torch.Export] [OpenELM] Failed to Export OpenELM: ""Pop from Empty List""",torch.export.export,2.4.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/133166,2024-08-10T13:47:51Z,[torch.compile] Integers stored on nn.Modules as dynamic causing errors,torch.compile,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132395,2024-08-01T09:53:38Z,`torch.multinomial` generates incorrect distribution,torch.multinomial,2.4.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/132395,2024-08-01T09:53:38Z,`torch.multinomial` generates incorrect distribution,torch.exponential_,2.4.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/133267,2024-08-12T20:44:01Z,`torch.ldexp` incorrectly returns infinity for `float64` inputs larger than `float32` max,torch.ldexp,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/133265,2024-08-12T20:36:06Z,`torch.ldexp` incorrectly returns infinity if `exp` is larger than log2 of the max representable number,torch.ldexp,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/133264,2024-08-12T20:26:33Z,`torch.ldexp` upcasts 16-bit inputs to 32 bits.,torch.ldexp,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/133227,2024-08-12T14:15:54Z,Issue Exporting PyTorch Model with pytorch3d.ops.ball_query to ONNX: Bug or Compatibility Problem?,torch.pytorch3d.ops.ball_query,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/133220,2024-08-12T11:07:33Z,error occur when I use torch.onnx.export to generate onnx model,torch.onnx.export,2.3.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/133006,2024-08-08T14:53:43Z,Precision Differences in Using `dropout` and `linear`,torch.dropout,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/133006,2024-08-08T14:53:43Z,Precision Differences in Using `dropout` and `linear`,torch.linear,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132700,2024-08-05T21:26:43Z,Forward Hooks in Transformer Not Working When `model.eval()` and `torch.no_grad()`,torch.nn.TransformerEncoder,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132700,2024-08-05T21:26:43Z,Forward Hooks in Transformer Not Working When `model.eval()` and `torch.no_grad()`,torch.no_grad,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132700,2024-08-05T21:26:43Z,Forward Hooks in Transformer Not Working When `model.eval()` and `torch.no_grad()`,torch.nn.TransformerEncoderLayer,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132614,2024-08-04T13:30:10Z,BUG: `torch.special.gammainc`: returns finite results with NaN input,torch.special.gammainc,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132614,2024-08-04T13:30:10Z,BUG: `torch.special.gammainc`: returns finite results with NaN input,torch.special.gammaincc,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132222,2024-07-31T09:59:23Z,aten::bucketize.Scalar doesn't work in compile mode,torch.bucketize,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132136,2024-07-30T12:31:15Z,`torch.nn.transformer.forward` returns incorrect value inside `torch.no_grad()` blocks.,torch.nn.Transformer,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132014,2024-07-29T03:41:52Z,[FSDP2 Related]`torch.split_with_sizes_copy` of the GPU does not update the version counter of `out` correctly.,torch.split_with_sizes_copy,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/131883,2024-07-26T09:21:09Z,Iterating dataloader fails on sliced dataset,torch.utils.data.DataLoader,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/131491,2024-07-23T18:07:21Z,`torch._assert_async` ignores `assert_msg`,torch._assert_async,2.3.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/131283,2024-07-20T17:59:27Z,"`input` argument of `nan_to_num()` works with `complex` type but `nan`, `posinf` and `neginf` argument don't work with `complex` type",torch.nan_to_num,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/131274,2024-07-20T04:06:54Z,The empty 0D or more D tensor with exceeding `dim`(`dim=100` or `dim=-100`) for `cummax()` works,torch.cummax,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/131273,2024-07-20T03:56:57Z,The empty 0D or more D tensor with exceeding `dim`(`dim=100` or `dim=-100`) for `cummin()` works,torch.cummin,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/131254,2024-07-19T21:32:42Z,Masked Attention has no effect in ``TransformerEncoderLayer``,torch.nn.TransformerEncoderLayer,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/131185,2024-07-19T13:08:32Z,"RuntimeError: Expected grad_output.numel() <= std::numeric_limits<int32_t>::max() to be true, but got false.",torch.cuda.empty_cache,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/131148,2024-07-19T07:04:44Z,"The empty 1D or more D tensor for `median()/nanmedian()` with the deepest `dim` gets errors, not getting NaNs",torch.median,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/131066,2024-07-18T21:59:01Z,Segmentation fault torch::CudaIPCSentData::~CudaIPCSentData(),torch.CudaIPCSentData,2.3.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/131043,2024-07-18T16:42:07Z,Using an empty tensor and `torch.int64` or `torch.bool` for `dtype` of `nanmean()` works while a non-empty tensor doesn't work,torch.nanmean,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/131027,2024-07-18T09:07:37Z,AoTInductor cumsum causes illegal memory access for large tensors,torch.cumsum,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/131009,2024-07-18T02:26:44Z,[cudagraph] an issue related to dynamic shapes for cudagraph backend with sd2.1,torch.compiler.cudagraph_mark_step_begin,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/131009,2024-07-18T02:26:44Z,[cudagraph] an issue related to dynamic shapes for cudagraph backend with sd2.1,torch.compile,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130917,2024-07-17T08:42:38Z,Torch compile can't support Tensor.item,torch.Tensor.item,2.3.1,121
pytorch,https://github.com/pytorch/pytorch/issues/130792,2024-07-16T00:41:57Z,`with torch.device()` modifies the `as_tensor` intended behavior,torch.as_tensor,2.3.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/130726,2024-07-15T10:09:31Z,TorchDispatchMode fails on jit.trace,torch.jit.trace,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130687,2024-07-14T00:25:27Z,"Compilation Fails with torch.sparse and ""fullgraph=True""",torch.sparse,2.3.1,12
pytorch,https://github.com/pytorch/pytorch/issues/130687,2024-07-14T00:25:27Z,"Compilation Fails with torch.sparse and ""fullgraph=True""",torch.compile,2.3.1,12
pytorch,https://github.com/pytorch/pytorch/issues/130622,2024-07-12T14:58:52Z,PyTorch profiler produces a trace that is huge and unreadable by perfetto webui when `torch._dynamo` is imported within the code traced,torch.profiler,2.3.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/130622,2024-07-12T14:58:52Z,PyTorch profiler produces a trace that is huge and unreadable by perfetto webui when `torch._dynamo` is imported within the code traced,torch._dynamo,2.3.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/130513,2024-07-11T02:27:02Z,RuntimeError Occurred In getBar1SizeOfGpu when initializing torch RPC,torch.distributed.rpc.api,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130424,2024-07-10T06:29:43Z,torch.onnx.errors.CheckerError: Unrecognized attribute: axes for operator ReduceMean,torch.onnx.export,2.3.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/129637,2024-06-27T07:33:19Z,`torch.compile()` failing with PyTorch 2.3 for tupled inputs,torch.compile,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129634,2024-06-27T05:52:03Z,Type Error when using from_numpy,torch.from_numpy,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128480,2024-06-12T03:36:24Z,InternalTorchDynamoError on converting llama-2 to onnx using torch.onnx.dynamo_export,torch.onnx.dynamo_export,2.3.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/128396,2024-06-11T06:30:21Z,torch.prod cannot be used with cudagraphs,torch.prod,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128396,2024-06-11T06:30:21Z,torch.prod cannot be used with cudagraphs,torch.cuda.make_graphed_callables,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128396,2024-06-11T06:30:21Z,torch.prod cannot be used with cudagraphs,torch.cuda.graph,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128325,2024-06-10T11:46:29Z,Cannot export onnx model with autograd.functional.jacobian,torch.autograd.functional.jacobian,2.3.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/128325,2024-06-10T11:46:29Z,Cannot export onnx model with autograd.functional.jacobian,torch.onnx.export,2.3.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/128325,2024-06-10T11:46:29Z,Cannot export onnx model with autograd.functional.jacobian,torch.onnx.dynamo_export,2.3.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/128261,2024-06-07T22:24:28Z,JIT script can cause unneeded `requires_grad` inside `no_grad` blocks,torch.jit.script,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128073,2024-06-05T22:07:12Z,inductor error when torch.compile on distrifuser,torch.compile,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/133093,2024-08-09T14:37:43Z,Inconsistent results of Conv2d in pytorch between CPU and CUDA,torch.nn.Conv2d,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/131974,2024-07-27T07:52:48Z,Missing grad_fn information while torch.compile with customized gradient function,torch.compile,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/131740,2024-07-25T04:41:49Z,`._scatter(...)` gives different results in CPU vs GPU environment,torch._scatter,2.3.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/130932,2024-07-17T14:21:54Z,Error using torch.func transforms with DDP (broadcast_buffers=True),torch.func.jacfwd,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130932,2024-07-17T14:21:54Z,Error using torch.func transforms with DDP (broadcast_buffers=True),torch.func.vmap,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130932,2024-07-17T14:21:54Z,Error using torch.func transforms with DDP (broadcast_buffers=True),torch.distributed.init_process_group,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130932,2024-07-17T14:21:54Z,Error using torch.func transforms with DDP (broadcast_buffers=True),torch.cuda.set_device,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130932,2024-07-17T14:21:54Z,Error using torch.func transforms with DDP (broadcast_buffers=True),torch.nn.parallel.DistributedDataParallel,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130932,2024-07-17T14:21:54Z,Error using torch.func transforms with DDP (broadcast_buffers=True),torch.nn.modules.module.Module.forward,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130932,2024-07-17T14:21:54Z,Error using torch.func transforms with DDP (broadcast_buffers=True),torch.nn.parallel.distributed._sync_buffers,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130932,2024-07-17T14:21:54Z,Error using torch.func transforms with DDP (broadcast_buffers=True),torch.nn.parallel.distributed._sync_module_buffers,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130932,2024-07-17T14:21:54Z,Error using torch.func transforms with DDP (broadcast_buffers=True),torch.nn.parallel.distributed._default_broadcast_coalesced,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130932,2024-07-17T14:21:54Z,Error using torch.func transforms with DDP (broadcast_buffers=True),torch.nn.parallel.distributed._distributed_broadcast_coalesced,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130932,2024-07-17T14:21:54Z,Error using torch.func transforms with DDP (broadcast_buffers=True),dist._broadcast_coalesced,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130728,2024-07-15T11:07:06Z,torch.cuda.empty_cache() doesn't work correctly for the first instance of nn.Module set to cuda (and a potential method to deal with that).,torch.cuda.empty_cache,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130609,2024-07-12T05:32:12Z,Setting `dtype` without `dtype=` to `reshape()` using a tensor works even though the doc doesn't explain it and an `int` tensor is returned instead of a `float` tensor,torch.reshape,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130608,2024-07-12T05:16:58Z,Setting `shape` and `dtype` without `dtype=` to `view()` works against error message but an `int` tensor is returned instead of a `float` tensor,torch.Tensor.view,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130448,2024-07-10T16:40:48Z,`n` argument of `randperm()` with a 0D or more D tensor works against the doc,torch.randperm,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130254,2024-07-08T16:54:11Z,"Many types for `start`, `end` and `base` argument of `logspace()` work againt what the doc says",torch.logspace,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130251,2024-07-08T16:26:49Z,[funcol] numerical error when funcol.reduce-scatter a non-contiguous tensor,torch.distributed._functional_collectives.reduce_scatter_tensor,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/129970,2024-07-02T16:54:16Z,Setting the 0D tensor of only one element to `fill_value` of `full()`/`full_like()` works against error message,torch.full_like,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129447,2024-06-25T04:03:37Z,"Pytorch of 2.3.0 version loads bool type mask in triton, encountering error when use tl.where.",torch.tl.where,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129319,2024-06-22T21:09:02Z,`int` type for `dims` of `tile()` without `dims=` works with a tensor against the doc,torch.tile,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129311,2024-06-22T16:56:09Z,The unexpected behavior of `argsort()`,torch.argsort,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129303,2024-06-22T08:00:26Z,Incorrect index from torch.mode,torch.mode,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129298,2024-06-22T04:26:52Z,The unexpected behavior of `sort()`,torch.sort,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129296,2024-06-22T02:10:38Z,`argsort()`/`msort()`/`sort()` can use the 0D tensor of a `complex` type value against error message,torch.sort,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128947,2024-06-18T11:42:32Z,Memory consumption of conv3d grows too quickly with certain input shapes.,torch.nn.Conv3d,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128936,2024-06-18T07:03:37Z,Performance degradation for certain input using Conv2D,torch.nn.Conv2d,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/128791,2024-06-16T07:44:45Z,CUDA error in torch.cdist with compute_mode=donot_use_mm_for_euclid_dist,torch.cdist,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128695,2024-06-14T08:42:00Z,Segmentation fault (core dumped) in `torch._weight_norm_interface`,torch._weight_norm_interface,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/128694,2024-06-14T08:39:45Z,Segmentation fault (core dumped) in `torch._fused_moving_avg_obs_fq_helper`,torch._fused_moving_avg_obs_fq_helper,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/128693,2024-06-14T08:37:52Z,Segmentation fault (core dumped) in `torch.fused_moving_avg_obs_fake_quant`,torch.fused_moving_avg_obs_fake_quant,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/128692,2024-06-14T08:35:54Z,Segmentation fault (core dumped) in `torch._weight_int4pack_mm`,torch._weight_int4pack_mm,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/128691,2024-06-14T08:32:51Z,Segmentation fault (core dumped) in `torch._remove_batch_dim`,torch._remove_batch_dim,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/128690,2024-06-14T08:24:29Z,`torch._transform_bias_rescale_qkv`:FPE,torch._transform_bias_rescale_qkv,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/128641,2024-06-13T20:13:26Z,Backward is not supported by `MaskedTensor`'s `torch.maximum`,torch.maximum,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128557,2024-06-12T21:41:31Z,MaskedTensor do not support _is_any_true`,torch._ops.aten._is_any_true,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128505,2024-06-12T13:02:25Z,torch.onnx.export - `repeat_interleave` produces invalid model,torch.onnx.export,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128505,2024-06-12T13:02:25Z,torch.onnx.export - `repeat_interleave` produces invalid model,torch.repeat_interleave,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128424,2024-06-11T17:23:18Z,`torch.compile` with `reduce-overhead`: very long compile time + GPU memory continuously to grow,torch.compile,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128324,2024-06-10T11:10:01Z,ONNX dynamic sized model export with torch.onnx.dynamo_export fails when .copy_() / roll / fftn is used,torch.onnx.dynamo_export,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/128324,2024-06-10T11:10:01Z,ONNX dynamic sized model export with torch.onnx.dynamo_export fails when .copy_() / roll / fftn is used,torch.roll,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/128324,2024-06-10T11:10:01Z,ONNX dynamic sized model export with torch.onnx.dynamo_export fails when .copy_() / roll / fftn is used,torch.fft.fftn,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/128294,2024-06-08T20:53:36Z,torch.cumprod will silently cast the output data type to int64,torch.cumprod,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/128147,2024-06-06T17:01:00Z,torch.compile Jamba: Scheduler Error in codegen for ComputedBuffer,torch._inductor.scheduler,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/128066,2024-06-05T21:45:10Z,reduce_scatter_tensor with strided inputs produces corrupted results,torch.reduce_scatter_tensor,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/127978,2024-06-04T23:22:23Z,torch.addr outputs NaN when computing on inf vectors,torch.addr,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/127866,2024-06-04T07:28:46Z,[torch.prod] Incorrect cpu calculation results for torch.prod,torch.prod,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/127679,2024-06-01T05:14:59Z,RuntimeError: `jit.freeze` fails to find externally assigned attributes,torch.jit.freeze,2.3.0,8.0.61
pytorch,https://github.com/pytorch/pytorch/issues/127387,2024-05-29T08:04:29Z,"Backwards pass through Beta distribution rsample gives inf for 4 < alpha - 2**16 < 1040, beta = 3/2",torch.distributions.beta.Beta.rsample,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/127375,2024-05-29T04:53:14Z,Different tensor strides can result in surprisingly large discrepancies in Conv2d outputs,torch.nn.Conv2d,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/127282,2024-05-28T10:35:27Z,torch.fx.symbolic_trace doesn't support many Callable types,torch.fx.symbolic_trace,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/127282,2024-05-28T10:35:27Z,torch.fx.symbolic_trace doesn't support many Callable types,torch.fx.Tracer.trace,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/127083,2024-05-24T14:41:59Z,"Setting a `float`, `complex` or `bool` type value to `precision` argument of `set_printoptions()` has problem when creating a `float`, `complex` or `bool` type tensor",torch.set_printoptions,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126972,2024-05-23T11:22:05Z,ONNX Exporter Fails with Handling Complex Tensors,torch.onnx.dynamo_export,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126963,2024-05-23T07:03:33Z,Segmentation fault (core dumped) when using pytorch Conv layers,torch.nn.Conv2d,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/126949,2024-05-23T03:06:28Z,`tensor_split()` with `indices_or_sections=` doesn't work while `tensor_split()` without `indices_or_sections=` works,torch.tensor_split,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126947,2024-05-23T02:55:36Z,`dsplit()` with `indices_or_sections=` doesn't work while `dsplit()` without `indices_or_sections=` works,torch.dsplit,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126766,2024-05-21T09:29:39Z,OOM Message is truncated,torch.zeros,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/126674,2024-05-20T08:56:01Z,"[torch.export] RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",torch.export.load,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/126620,2024-05-18T15:06:38Z,dataloader crashes after several epochs if the trained model contains triton-based operators,torch.utils.data.DistributedSampler,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/126620,2024-05-18T15:06:38Z,dataloader crashes after several epochs if the trained model contains triton-based operators,torch.utils.data.DataLoader,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/126620,2024-05-18T15:06:38Z,dataloader crashes after several epochs if the trained model contains triton-based operators,torch.cuda.amp.GradScaler,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/126620,2024-05-18T15:06:38Z,dataloader crashes after several epochs if the trained model contains triton-based operators,torch.optim.AdamW,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/126620,2024-05-18T15:06:38Z,dataloader crashes after several epochs if the trained model contains triton-based operators,torch.nn.CrossEntropyLoss,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/126620,2024-05-18T15:06:38Z,dataloader crashes after several epochs if the trained model contains triton-based operators,torch.nn.LayerNorm,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/126620,2024-05-18T15:06:38Z,dataloader crashes after several epochs if the trained model contains triton-based operators,torch.cuda.amp.autocast,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/126585,2024-05-17T23:28:07Z,RuntimeError when using Adam(fused=True) with torch.compile,torch.optim.Adam,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126566,2024-05-17T19:35:51Z,`torch.compiler.allow_in_graph` does not create a `call_module` op in fx.Graph in torch 2.3.0,torch.compiler.allow_in_graph,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/126381,2024-05-16T05:13:37Z,All processes running torch.distributed.destroy_process_group() create CUDA context on device 0,torch.distributed.destroy_process_group,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126268,2024-05-15T07:48:53Z,```FlopCounterMode``` returns 0 when inference mode is on during forwardpropagation.,torch.inference_mode,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/126268,2024-05-15T07:48:53Z,```FlopCounterMode``` returns 0 when inference mode is on during forwardpropagation.,torch.no_grad,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/126268,2024-05-15T07:48:53Z,```FlopCounterMode``` returns 0 when inference mode is on during forwardpropagation.,torch.utils.flop_counter.FlopCounterMode,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/126167,2024-05-14T12:42:51Z,`aten::transpose.int` is acting errouneously,torch.aten.transpose.int,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/126154,2024-05-14T09:28:59Z,Optimizer classes not `dill` picklable after using `torch.compile`,torch.compile,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/126154,2024-05-14T09:28:59Z,Optimizer classes not `dill` picklable after using `torch.compile`,torch.optim.AdamW,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/126154,2024-05-14T09:28:59Z,Optimizer classes not `dill` picklable after using `torch.compile`,torch._C._dynamo.eval_frame,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/126032,2024-05-12T16:55:57Z,all_gather with gloo backend does not work in inference mode,torch.distributed.all_gather,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/125718,2024-05-07T22:34:35Z,`torch.compile` and complex numbers,torch.compile,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/125664,2024-05-07T08:21:25Z,Aborted (core dumped) in torch.fft.irfftn/hfftn/ihfftn with malloc(): corrupted top size,torch.fft.irfftn,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/125664,2024-05-07T08:21:25Z,Aborted (core dumped) in torch.fft.irfftn/hfftn/ihfftn with malloc(): corrupted top size,torch.fft.hfftn,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/125664,2024-05-07T08:21:25Z,Aborted (core dumped) in torch.fft.irfftn/hfftn/ihfftn with malloc(): corrupted top size,torch.fft.ihfftn,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/125534,2024-05-04T15:40:30Z,"❓Different results between normal batching and `vmap` while using lower precision (e.g., bfloat16)",torch.vmap,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/124918,2024-04-25T07:45:01Z,Custom Operator Design for torch.compile: Must Output Tensors Always Be Returned?,torch.library.define,2.3.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/124918,2024-04-25T07:45:01Z,Custom Operator Design for torch.compile: Must Output Tensors Always Be Returned?,torch.library.impl,2.3.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/124918,2024-04-25T07:45:01Z,Custom Operator Design for torch.compile: Must Output Tensors Always Be Returned?,torch.library.impl_abstract,2.3.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/124918,2024-04-25T07:45:01Z,Custom Operator Design for torch.compile: Must Output Tensors Always Be Returned?,torch.compile,2.3.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/124918,2024-04-25T07:45:01Z,Custom Operator Design for torch.compile: Must Output Tensors Always Be Returned?,torch.ops.mylib.custom_func.default,2.3.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/124918,2024-04-25T07:45:01Z,Custom Operator Design for torch.compile: Must Output Tensors Always Be Returned?,torch._check,2.3.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/124918,2024-04-25T07:45:01Z,Custom Operator Design for torch.compile: Must Output Tensors Always Be Returned?,torch.allclose,2.3.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/124918,2024-04-25T07:45:01Z,Custom Operator Design for torch.compile: Must Output Tensors Always Be Returned?,torch.empty_like,2.3.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/124918,2024-04-25T07:45:01Z,Custom Operator Design for torch.compile: Must Output Tensors Always Be Returned?,torch.from_numpy,2.3.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/124918,2024-04-25T07:45:01Z,Custom Operator Design for torch.compile: Must Output Tensors Always Be Returned?,torch.randn,2.3.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/124877,2024-04-24T20:05:01Z,SDPA memory efficient kernel returns NaNs when the query and key are different lengths,torch.backends.cuda.sdp_kernel,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/124006,2024-04-13T12:29:07Z,"[2.3 dynamic shapes] backend='inductor' raised: LoweringException: AssertionError: indices must be int64, byte or bool. Got [torch.float32]",torch._dynamo,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/122991,2024-03-29T21:49:09Z,[AOTI] KeyError: 'torch.complex32,torch.complex32,2.3.0,12
pytorch,https://github.com/pytorch/pytorch/issues/121761,2024-03-12T22:00:34Z,[export] Casting `ExportedProgram` module to CUDA after `export` on CPU fails to run inference,torch.export,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121367,2024-03-07T00:15:59Z,`torch.compile` makes triton kernel slower in some cases,torch.compile,2.3.0,12.3
pytorch,https://github.com/pytorch/pytorch/issues/121228,2024-03-05T14:47:28Z,[torch.compile] `merge_unbind_stack` returns a tensor with WRONG shape,torch.compile,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/121228,2024-03-05T14:47:28Z,[torch.compile] `merge_unbind_stack` returns a tensor with WRONG shape,torch.unbind,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/121228,2024-03-05T14:47:28Z,[torch.compile] `merge_unbind_stack` returns a tensor with WRONG shape,torch.stack,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/121228,2024-03-05T14:47:28Z,[torch.compile] `merge_unbind_stack` returns a tensor with WRONG shape,merge_unbind_stack,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/121185,2024-03-05T01:36:16Z,[torch.compile] `merge_stack_tahn_unbind` raises list index out of range error,torch.compile,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/121185,2024-03-05T01:36:16Z,[torch.compile] `merge_stack_tahn_unbind` raises list index out of range error,torch._dynamo.exc.BackendCompilerFailed,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/121185,2024-03-05T01:36:16Z,[torch.compile] `merge_stack_tahn_unbind` raises list index out of range error,merge_stack_tahn_unbind,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/121184,2024-03-05T01:33:23Z,[torch.compile] Compiled Assumption Failed for `efficient_conv_bn_eval`,torch.compile,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/121135,2024-03-04T15:32:45Z,[torch.compile] `randperm_index_add_pattern` doesn't check the shape mismatch between self and source tensor,torch.randperm,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/121135,2024-03-04T15:32:45Z,[torch.compile] `randperm_index_add_pattern` doesn't check the shape mismatch between self and source tensor,torch.index_add,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/121135,2024-03-04T15:32:45Z,[torch.compile] `randperm_index_add_pattern` doesn't check the shape mismatch between self and source tensor,torch.compile,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/121095,2024-03-03T09:38:50Z,Segmentation Fault Due to Empty Stride Argument in `torch._C._nn.slow_conv3d`,torch._C._nn.slow_conv3d,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/120585,2024-02-25T14:31:09Z,Functionalization of `as_strided` disregards its base tensor storage.,torch.as_strided,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/120292,2024-02-21T05:42:21Z,Compilation Failure with torch.nn.functional.rrelu(training=True) in Torch Compile,torch.nn.functional.rrelu,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/120194,2024-02-19T07:12:30Z,model loaded with torch._export.aot_load does not report what file is not found during inference and Cuda driver error.,torch._export.aot_load,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119980,2024-02-15T06:42:56Z,Crash on saving FSDP checkpoint of compiled model,torch.compile,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119980,2024-02-15T06:42:56Z,Crash on saving FSDP checkpoint of compiled model,torch.distributed.fsdp,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119348,2024-02-07T04:07:26Z,torch.compile + user defined triton kernel falsely reports mutated input,torch.compile,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/119343,2024-02-07T02:19:39Z,Use Dtensor and ColwiseParallel api to do Tensor Parallel for llama2 model but got communication error.,torch.distributed.tensor.parallel.ColwiseParallel,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/119343,2024-02-07T02:19:39Z,Use Dtensor and ColwiseParallel api to do Tensor Parallel for llama2 model but got communication error.,torch.distributed.tensor.parallel.RowwiseParallel,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/119343,2024-02-07T02:19:39Z,Use Dtensor and ColwiseParallel api to do Tensor Parallel for llama2 model but got communication error.,torch.distributed._tensor.DTensor,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/119162,2024-02-05T03:55:34Z,torch.compile changes the dtype of key matrix in attention implementation with torch.set_default_dtype,torch.compile,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119162,2024-02-05T03:55:34Z,torch.compile changes the dtype of key matrix in attention implementation with torch.set_default_dtype,torch.set_default_dtype,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119162,2024-02-05T03:55:34Z,torch.compile changes the dtype of key matrix in attention implementation with torch.set_default_dtype,torch.bfloat16,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119162,2024-02-05T03:55:34Z,torch.compile changes the dtype of key matrix in attention implementation with torch.set_default_dtype,F.scaled_dot_product_attention,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119152,2024-02-04T22:54:59Z,Dynamo fails on scalar bit shifts when `dynamic=True`,torch.compile,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/118883,2024-02-01T17:37:33Z,Torch Sparse Sampled AddMM Backwards Memory Usage Blowup,torch.sparse.sampled_addmm,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/118856,2024-02-01T11:03:26Z,torch._dynamo.exc.TorchRuntimeError: Failed running call_function of torch.sparse_coo_tensor,torch.sparse_coo_tensor,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/118741,2024-01-31T11:15:12Z,Inconsistent Behavior of `torch.dsplit` with torch.compile,torch.dsplit,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/118741,2024-01-31T11:15:12Z,Inconsistent Behavior of `torch.dsplit` with torch.compile,torch.special.logit,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/118520,2024-01-29T13:14:09Z,[FSDP] High CPU RAM usage with FSDP sharded checkpointing,torch.get_optimizer_state_dict,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/118473,2024-01-27T20:32:59Z,MultiGPU distributed training Hang,torch.distributed.init_process_group,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/118473,2024-01-27T20:32:59Z,MultiGPU distributed training Hang,torch.distributed.barrier,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/118473,2024-01-27T20:32:59Z,MultiGPU distributed training Hang,torch.cuda.set_device,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/118473,2024-01-27T20:32:59Z,MultiGPU distributed training Hang,torch.distributed.new_group,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/118462,2024-01-27T14:40:17Z,Inconsistency in Output for `torch.nn.functional.poisson_nll_loss` in JIT Compiled Model,torch.nn.functional.poisson_nll_loss,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/118462,2024-01-27T14:40:17Z,Inconsistency in Output for `torch.nn.functional.poisson_nll_loss` in JIT Compiled Model,torch.nn.functional.silu,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/118400,2024-01-26T16:37:07Z,Inconsistent output of `torch.lodget` with JIT compilation,torch.logdet,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/118400,2024-01-26T16:37:07Z,Inconsistent output of `torch.lodget` with JIT compilation,torch._C._special.special_bessel_j1,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/118400,2024-01-26T16:37:07Z,Inconsistent output of `torch.lodget` with JIT compilation,torch.jit.trace,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/118175,2024-01-24T07:03:47Z,Different behaviors in `torch.nn.functional.hinge_embedding_loss` between eagermode and torch.compile,torch.nn.functional.hinge_embedding_loss,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/118088,2024-01-23T09:38:06Z,Segmentation fault (SIGSEGV) in c10::detail::getNonDeterministicRandom(bool),torch.c10::detail::getNonDeterministicRandom,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/118088,2024-01-23T09:38:06Z,Segmentation fault (SIGSEGV) in c10::detail::getNonDeterministicRandom(bool),torch.at::CUDAGeneratorImpl::seed,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/118088,2024-01-23T09:38:06Z,Segmentation fault (SIGSEGV) in c10::detail::getNonDeterministicRandom(bool),torch.at::cuda::detail::getDefaultCUDAGenerator,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/118088,2024-01-23T09:38:06Z,Segmentation fault (SIGSEGV) in c10::detail::getNonDeterministicRandom(bool),torch.THCPModule_initExtension,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/118086,2024-01-23T09:10:57Z,Different behaviors with torch.isfinite and torch.tril between eagermode and JIT,torch.isfinite,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/118086,2024-01-23T09:10:57Z,Different behaviors with torch.isfinite and torch.tril between eagermode and JIT,torch.tril,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/117924,2024-01-20T10:25:08Z,[FSDP] `ignore_states` for mate params failed after materialized.,torch.distributed.fsdp.FullyShardedDataParallel,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117921,2024-01-20T09:39:21Z,[FSDP] Ignore meta device module failed with `auto_wrap_policy`,torch.distributed.fsdp.auto_wrap_policy,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117623,2024-01-17T10:06:36Z,different behaviors in cumsum between torch.compile mode and eager mode,torch.cumsum,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/117486,2024-01-15T08:19:33Z,"all_to_all_single stuck when using output_split_sizes = [1, 3] and input_split_sizes = [1, 3]",```,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/117486,2024-01-15T08:19:33Z,"all_to_all_single stuck when using output_split_sizes = [1, 3] and input_split_sizes = [1, 3]",torch.distributed._functional_collectives.all_to_all_single,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/117486,2024-01-15T08:19:33Z,"all_to_all_single stuck when using output_split_sizes = [1, 3] and input_split_sizes = [1, 3]",torch.distributed.all_to_all_single,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/117486,2024-01-15T08:19:33Z,"all_to_all_single stuck when using output_split_sizes = [1, 3] and input_split_sizes = [1, 3]",```,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/117259,2024-01-11T13:42:27Z,```torch.Tensor.sum```: Internal assert failed,torch.Tensor.sum,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117256,2024-01-11T13:09:20Z,```torch.Tensor.round_```: Internal assert failed,torch.Tensor.round_,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117250,2024-01-11T11:27:57Z,```torch.nn.functional.normalize```: Internal assert failed,torch.nn.functional.normalize,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117249,2024-01-11T11:16:44Z,```torch.Tensor.triangular_solve```: Internal assert failed,torch.Tensor.triangular_solve,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117248,2024-01-11T11:06:34Z,```torch.Tensor.matmul```: Internal assert failed,torch.Tensor.matmul,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117247,2024-01-11T10:59:53Z,```torch.nn.Linear``` class: Internal assertion failed,torch.nn.Linear,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117244,2024-01-11T10:33:03Z,```torch.Tensor.index_copy_```: Internal assert failed,torch.Tensor.index_copy_,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117243,2024-01-11T10:16:21Z,```torch.Tensor.cov```: Internal assertion failed,torch.Tensor.cov,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117243,2024-01-11T10:16:21Z,```torch.Tensor.cov```: Internal assertion failed,torch.Tensor.to_sparse,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117242,2024-01-11T10:10:57Z,```torch.tensordot```: Internal assert failed,torch.tensordot,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117239,2024-01-11T08:45:22Z,```torch.nn.Upsample``` class: Internal assert failed,torch.nn.Upsample,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117236,2024-01-11T08:24:29Z,```torch.linalg.lstsq```: Internal assert failed,torch.linalg.lstsq,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117232,2024-01-11T08:07:10Z,```torch.Tensor.renorm```: Internal assert failed,torch.Tensor.renorm,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117229,2024-01-11T07:21:25Z,```torch.nn.functional.pad```: Internal assert failed,torch.nn.functional.pad,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117229,2024-01-11T07:21:25Z,```torch.nn.functional.pad```: Internal assert failed,torch.quantize_per_tensor,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/116351,2023-12-23T07:35:10Z,`torch.quantize_per_channel`: FPE,torch.quantize_per_channel,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/116306,2023-12-21T23:43:27Z,ONNX export error when exporting Vision Transformer model from PyTorch to ONNX format,torch.onnx,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/116253,2023-12-21T08:55:55Z,`torch.Tensor.flipud`: Heap-buffer-overflow,torch.Tensor.flipud,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/116252,2023-12-21T08:47:24Z,`torch.Tensor.fliplr`: Heap-buffer-overflow,torch.Tensor.fliplr,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/116251,2023-12-21T08:41:51Z,`torch.Tensor.flip`: Heap-buffer-overflow,torch.Tensor.flip,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/116250,2023-12-21T08:09:43Z,`torch.Tensor.rot90`: Heap-buffer-overflow,torch.Tensor.rot90,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/116240,2023-12-21T03:06:51Z,[inductor][cpu] RuntimeError: quantized_resize_cpu_ does not have a deterministic implementation,torch.ops.quantized.linear,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/127804,2024-06-03T20:25:59Z,torch.floor_divide throws floating point exception,torch.floor_divide,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/127076,2024-05-24T10:11:51Z,[PT2E Quantization] `prepare_pt2e` produces inconsistent data types for primitive int,torch.ao.quantization.quantize_pt2e.prepare_pt2e,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/128496,2024-06-12T08:26:51Z,torch.jit.script not work for ParameterDict.items,torch.jit.script,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128496,2024-06-12T08:26:51Z,torch.jit.script not work for ParameterDict.items,torch.nn.ParameterDict.items,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128039,2024-06-05T17:20:18Z,`torch.compile` fails with customized Triton Operator on Triton 2.2,torch.compile,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/127605,2024-05-31T07:17:11Z,RuntimeError: CUDA error: an illegal memory access was encountered with specific input shape and Conv1d,torch.nn.Conv1d,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/127197,2024-05-26T16:12:00Z,Tensors of the same index must be on the same device and the same dtype except step tensors that can be CPU and float32 notwithstanding.,torch._C._group_tensors_by_device_and_dtype,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/125835,2024-05-09T10:51:00Z,"When using torch.onnx.export, the result of torch.scatter_add may be wrong for opset11",torch.onnx.export,2.2.2,CPU
pytorch,https://github.com/pytorch/pytorch/issues/125835,2024-05-09T10:51:00Z,"When using torch.onnx.export, the result of torch.scatter_add may be wrong for opset11",torch.scatter_add,2.2.2,CPU
pytorch,https://github.com/pytorch/pytorch/issues/125388,2024-05-02T10:51:24Z,torch.multinomial raises no error when sampling from zero weight and replacement=False.,torch.multinomial,2.2.2,CPU
pytorch,https://github.com/pytorch/pytorch/issues/125375,2024-05-02T03:51:20Z,torch.onnx.dynamo_export fails to convert torchaudio.transforms.MFCC to onnx,torch.onnx.dynamo_export,2.2.2,11.8.0
pytorch,https://github.com/pytorch/pytorch/issues/125375,2024-05-02T03:51:20Z,torch.onnx.dynamo_export fails to convert torchaudio.transforms.MFCC to onnx,torchaudio.transforms.MFCC,2.2.2,11.8.0
pytorch,https://github.com/pytorch/pytorch/issues/125223,2024-04-30T10:29:29Z,torch.uniform_() is single-threaded on CPU,torch.uniform_(),2.2.2,11.8
pytorch,https://github.com/pytorch/pytorch/issues/124778,2024-04-23T21:12:12Z,Dense-sparse broadcasted multiplication fails in the backward pass,torch.sparse.sum,2.2.2,11.8
pytorch,https://github.com/pytorch/pytorch/issues/124778,2024-04-23T21:12:12Z,Dense-sparse broadcasted multiplication fails in the backward pass,torch.as_strided,2.2.2,11.8
pytorch,https://github.com/pytorch/pytorch/issues/124778,2024-04-23T21:12:12Z,Dense-sparse broadcasted multiplication fails in the backward pass,torch.view,2.2.2,11.8
pytorch,https://github.com/pytorch/pytorch/issues/124778,2024-04-23T21:12:12Z,Dense-sparse broadcasted multiplication fails in the backward pass,torch.broadcast_to,2.2.2,11.8
pytorch,https://github.com/pytorch/pytorch/issues/124711,2024-04-23T07:20:27Z,Output Discrepancy between PyTorch Model and Converted ONNX Model,torch.onnx.export,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/124680,2024-04-22T23:42:10Z,Inconsistent results when training a model containing SyncBatchNorm with multiple GPUs,torch.nn.SyncBatchNorm,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/124644,2024-04-22T19:21:25Z,torch.Tensor.remainder raises a floating point exception when divisor is -1,torch.Tensor.remainder,2.2.2,CPU
pytorch,https://github.com/pytorch/pytorch/issues/124044,2024-04-15T07:53:08Z,torch.compile error,torch.compile,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/124019,2024-04-14T05:12:22Z,[FSDP+TP] RuntimeError: 'weight' must be 2-D,torch.distributed.fsdp.api,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123962,2024-04-12T18:31:56Z,FSDP Doesn't Work with model.generate(),torch.distributed.fsdp.FullyShardedDataParallel,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123962,2024-04-12T18:31:56Z,FSDP Doesn't Work with model.generate(),torch.distributed.fsdp.MixedPrecision,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123962,2024-04-12T18:31:56Z,FSDP Doesn't Work with model.generate(),torch.distributed.fsdp.StateDictType,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123962,2024-04-12T18:31:56Z,FSDP Doesn't Work with model.generate(),torch.distributed.fsdp.BackwardPrefetch,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123962,2024-04-12T18:31:56Z,FSDP Doesn't Work with model.generate(),torch.distributed.fsdp.ShardingStrategy,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123962,2024-04-12T18:31:56Z,FSDP Doesn't Work with model.generate(),torch.distributed.fsdp.CPUOffload,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123962,2024-04-12T18:31:56Z,FSDP Doesn't Work with model.generate(),torch.distributed.fsdp.wrap.transformer_auto_wrap_policy,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123911,2024-04-12T05:10:20Z,"`torch.softmax(inp, dtype=torch.float32).to(torch.float16)` is not equivalent to `torch.softmax(inp)` for fp16 input",torch.softmax,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123824,2024-04-11T07:22:28Z,Precision problem in `quantize_fx.convert_fx` and question about the `example_inputs` for `quantize_fx.prepare_fx`.,torch.quantization.quantize_fx.convert_fx,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123824,2024-04-11T07:22:28Z,Precision problem in `quantize_fx.convert_fx` and question about the `example_inputs` for `quantize_fx.prepare_fx`.,torch.quantization.quantize_fx.prepare_fx,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123642,2024-04-09T14:43:52Z,TransformerDecoderLayer fails when on cuda and batch dimension is zero (on cpu is fine),torch.nn.TransformerDecoderLayer,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123628,2024-04-09T05:59:06Z,`torch.onnx.export` doesn't correctly constfold constants and DequantLinear,torch.onnx.export,2.2.2,CPU
pytorch,https://github.com/pytorch/pytorch/issues/123500,2024-04-06T10:05:33Z,"Cannot add 2 sparse_csc_tensor directly, raising RuntimeError",torch.to_sparse_csc,2.2.2,11.8
pytorch,https://github.com/pytorch/pytorch/issues/123242,2024-04-03T04:42:47Z,torch.compile Conv1d AssertionError,torch.compile,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123242,2024-04-03T04:42:47Z,torch.compile Conv1d AssertionError,torch.nn.Conv1d,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132634,2024-08-05T08:14:37Z,`torch.linalg.norm` and `torch.norm` give wrong result with `torch.complex32` type tensor,torch.linalg.norm,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132634,2024-08-05T08:14:37Z,`torch.linalg.norm` and `torch.norm` give wrong result with `torch.complex32` type tensor,torch.norm,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129093,2024-06-19T22:12:54Z,take_along_dim or gather unstable results on cpu with stride 1,torch.take_along_dim,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129093,2024-06-19T22:12:54Z,take_along_dim or gather unstable results on cpu with stride 1,torch.gather,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128610,2024-06-13T14:03:01Z,torch.onnx.export() 1.16 with pytorch 2.2.1 produces invalid model,torch.onnx.export(),2.2.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/128600,2024-06-13T07:59:48Z,jacrev and jacfwd yield different results if one uses torch.no_grad blocks in module,torch.func.jacrev,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128600,2024-06-13T07:59:48Z,jacrev and jacfwd yield different results if one uses torch.no_grad blocks in module,torch.func.jacfwd,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128600,2024-06-13T07:59:48Z,jacrev and jacfwd yield different results if one uses torch.no_grad blocks in module,torch.func.functional_call,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128600,2024-06-13T07:59:48Z,jacrev and jacfwd yield different results if one uses torch.no_grad blocks in module,torch.Tensor.detach,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128600,2024-06-13T07:59:48Z,jacrev and jacfwd yield different results if one uses torch.no_grad blocks in module,torch.func.vjp,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128600,2024-06-13T07:59:48Z,jacrev and jacfwd yield different results if one uses torch.no_grad blocks in module,torch.func.jvp,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126777,2024-05-21T14:37:43Z,One complex number or boolean value of a 1D or more D tensor with `argmax()` and `dim=` works,torch.argmax,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126776,2024-05-21T14:30:32Z,One complex number or boolean value of a 1D or more D tensor with `argmin()` and `dim=` works,torch.argmin,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126759,2024-05-21T07:15:49Z,The unexpected behavior of `max()`,torch.max,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126758,2024-05-21T06:55:22Z,The unexpected behavior of `min()`,torch.min,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126659,2024-05-20T02:09:31Z,`topk()` cannot be used with `complex` or `bool` type of a 0D(only `cuda`) or more D tensor but `topk()` can be used with `complex` or `bool` type of a 0D(only `cpu`) tensor,torch.topk,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126658,2024-05-20T00:00:27Z,`kthvalue()` cannot be used with `complex` or `bool` type of a 1D or more D tensor but `kthvalue()` can be used with `complex` or `bool` type of a 0D tensor,torch.kthvalue,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126021,2024-05-12T10:17:01Z,Incorrect output from `torch.histc` when min and max are equal.,torch.histc,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/124216,2024-04-16T21:14:09Z,[NestedTensor] NJT with lengths set fails torch.compile(),torch.compile,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/124204,2024-04-16T19:26:50Z,RPC + DDP VRAM overflow to rank 0 GPU.,torch.distributed.autograd.backward,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/124133,2024-04-16T01:34:07Z,ZeroRedundancyOptimizer + AdamW Fused can't load state_dict back,torch.distributed.optim.ZeroRedundancyOptimizer,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/124133,2024-04-16T01:34:07Z,ZeroRedundancyOptimizer + AdamW Fused can't load state_dict back,torch.optim.AdamW,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123421,2024-04-05T05:08:55Z,nonzero with dtensor hang,torch.nonzero,2.2.1,11
pytorch,https://github.com/pytorch/pytorch/issues/123070,2024-04-01T04:24:05Z,CUDA out of memory still exist after using FSDP,torch.nn.functional._canonical_mask,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/122478,2024-03-22T04:37:18Z,6470 segmentation fault (core dumped),torch,2.2.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/122183,2024-03-19T09:45:09Z,[FX Graph Mode Quantization] Prepare model fails with Linear-BN when `affine=False`,torch.ao.quantization.quantize_fx,2.2.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/122170,2024-03-19T05:03:33Z,torch.cumsum() super slow on 2d input,torch.cumsum,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/122042,2024-03-17T03:40:22Z,[Incorrect Result] torch.atan(inf) returns NaN value on dtype complex128.,torch.atan,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121966,2024-03-15T10:41:30Z,torch.compile fails when used together with activation checkpointing,torch.compile,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121966,2024-03-15T10:41:30Z,torch.compile fails when used together with activation checkpointing,torch.utils.checkpoint.checkpoint,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121966,2024-03-15T10:41:30Z,torch.compile fails when used together with activation checkpointing,torch.cuda.amp.autocast,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121966,2024-03-15T10:41:30Z,torch.compile fails when used together with activation checkpointing,torch.nn.functional.dropout,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121842,2024-03-13T20:08:55Z,gradcheck fails when inputs are dependent on each other,torch.autograd.gradcheck,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121789,2024-03-13T05:20:59Z,Precision Inconsistent for `half` type compared with NumPy and TensorFlow,torch.nn.functional.leaky_relu,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121725,2024-03-12T13:26:54Z,torch.logsumexp throws RuntimeError instead of reducing all dimensions when dim=None,torch.logsumexp,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121631,2024-03-11T12:41:41Z,[inductor] Incorrect handle of `autocast` results in type mismatch,torch.autocast,2.2.1,12.3
pytorch,https://github.com/pytorch/pytorch/issues/121389,2024-03-07T06:58:36Z,`torch.gather` backward performance extremely low on `torch.bfloat16`,torch.gather,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121343,2024-03-06T21:42:29Z,Floating point exception on torch.lcm,torch.lcm,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121271,2024-03-05T21:29:32Z,`ReduceLROnPlateau` does not call `super().__init__()`,torch.optim.ReduceLROnPlateau,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121256,2024-03-05T19:15:42Z,`torch.compile()` failing with a decoder-based diffusion pipeline,torch.compile,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121230,2024-03-05T14:55:00Z,INTERNAL ASSERT FAILED in torch.pow,torch.pow,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121230,2024-03-05T14:55:00Z,INTERNAL ASSERT FAILED in torch.pow,torch.quantize_per_tensor,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121230,2024-03-05T14:55:00Z,INTERNAL ASSERT FAILED in torch.pow,torch.quantile,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121209,2024-03-05T07:16:59Z,torch.distributed.DistBackendError: NCCL error,torch.distributed.DistBackendError,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121098,2024-03-03T18:55:55Z,torch.onnx.dynamo_export,torch.onnx.dynamo_export,2.2.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/121098,2024-03-03T18:55:55Z,torch.onnx.dynamo_export,torch.fft.rfft,2.2.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/121098,2024-03-03T18:55:55Z,torch.onnx.dynamo_export,torch._dynamo.disable,2.2.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/121098,2024-03-03T18:55:55Z,torch.onnx.dynamo_export,torch.fx.Interpreter,2.2.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/121098,2024-03-03T18:55:55Z,torch.onnx.dynamo_export,torch.ops.aten._native_batch_norm_legit.default,2.2.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/120989,2024-03-01T11:20:55Z,input.is_sparse() INTERNAL ASSERT FAILED,torch.sparse_coo_tensor,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/120989,2024-03-01T11:20:55Z,input.is_sparse() INTERNAL ASSERT FAILED,torch.conj_physical,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/120989,2024-03-01T11:20:55Z,input.is_sparse() INTERNAL ASSERT FAILED,input.is_sparse,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/120554,2024-02-24T15:55:00Z,`Tensor.new_tensor` fails with `jvp` (problem with duals?),torch.Tensor.new_tensor,2.2.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/120554,2024-02-24T15:55:00Z,`Tensor.new_tensor` fails with `jvp` (problem with duals?),torch.func.jvp,2.2.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/120462,2024-02-23T01:53:21Z,torch.compile() drops transformer/Qwen1.5-7B model output quality from good to unusable,torch.compile,2.2.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/129032,2024-06-19T03:39:07Z,torch parallel Broadcast inconsistency,torch.nn.parallel.Broadcast,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/127736,2024-06-03T02:53:33Z,torch.nn.BatchNorm1d missing check on values of parameters eps and momentum,torch.nn.BatchNorm1d,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/127734,2024-06-03T02:36:16Z,nn.ConvTranspose2d does not check for parameter combinations resulting in convolution output with dimension 0,torch.nn.ConvTranspose2d,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/127733,2024-06-03T02:09:21Z,torch.nn.Linear does not check the value of the out_features parameter,torch.nn.Linear,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/127732,2024-06-03T01:41:29Z,torch.nn.ConvTranspose2d/torch.nn.ConvTranspose1d lacks a check for the parameter dilation in the model building process,torch.nn.ConvTranspose2d,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/127732,2024-06-03T01:41:29Z,torch.nn.ConvTranspose2d/torch.nn.ConvTranspose1d lacks a check for the parameter dilation in the model building process,torch.nn.ConvTranspose1d,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/127701,2024-06-02T08:40:44Z,The type of exception thrown by torch.nn.Conv2d does not match the actual situation,torch.nn.Conv2d,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/127221,2024-05-27T09:54:31Z,lacking checking for ConvTranspose's parameters when running with GPUs,torch.nn.ConvTranspose2d,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/127164,2024-05-25T07:15:05Z,randn generates different output for 4x4 tensor size sliced to match shape of direct 2x4 or 4x2 and compare output,torch.randn,2.2.0,12.3
pytorch,https://github.com/pytorch/pytorch/issues/126975,2024-05-23T12:28:19Z,A huge difference between the results of torch.round() on the GPU compared to its results on the CPU and other DL libraries,torch.round,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126775,2024-05-21T14:06:46Z,The significant difference between the outputs of torch.reciprocal running on cpu and gpu,torch.reciprocal,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126673,2024-05-20T07:45:38Z,torch.nn.AdaptiveAvgPool2d lacks checking of input dimension,torch.nn.AdaptiveAvgPool2d,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126141,2024-05-14T03:19:09Z,Inconsistent gradients in reverse mode and forward mode AD when using `MultiheadAttention`,torch.nn.MultiheadAttention,2.2.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/124525,2024-04-19T21:36:33Z,`aot_module` + `torch.onnx.export` bug or user error?,torch.onnx.export,2.2.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/124525,2024-04-19T21:36:33Z,`aot_module` + `torch.onnx.export` bug or user error?,functorch.compile.aot_module,2.2.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/122512,2024-03-22T18:41:05Z,Higher peak memory with torch.compile,torch.compile,2.2.0,12.2
pytorch,https://github.com/pytorch/pytorch/issues/122467,2024-03-22T02:19:31Z,RuntimeError: out_ptr == out_accessor[thread_count_nonzero[tid + 1]].data() INTERNAL ASSERT FAILED,torch.as_tensor,2.2.0,12.2
pytorch,https://github.com/pytorch/pytorch/issues/122467,2024-03-22T02:19:31Z,RuntimeError: out_ptr == out_accessor[thread_count_nonzero[tid + 1]].data() INTERNAL ASSERT FAILED,torch.unbind,2.2.0,12.2
pytorch,https://github.com/pytorch/pytorch/issues/122467,2024-03-22T02:19:31Z,RuntimeError: out_ptr == out_accessor[thread_count_nonzero[tid + 1]].data() INTERNAL ASSERT FAILED,torch.Tensor.to_sparse,2.2.0,12.2
pytorch,https://github.com/pytorch/pytorch/issues/122019,2024-03-16T06:24:36Z,`F.max_pool3d`: Segfault due to incorrect `dilation` check in kernel,torch.nn.functional.max_pool3d,2.2.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/121857,2024-03-13T22:09:35Z,AdamW(fused=True) slower than unfused AdamW,torch.optim.AdamW,2.2.0,12.2
pytorch,https://github.com/pytorch/pytorch/issues/121222,2024-03-05T11:50:33Z,Segmentation Fault in `torch.nn.Conv1d` starting from torch 2.2.0,torch.nn.Conv1d,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/120485,2024-02-23T11:47:02Z,Try to make `jit.trace` working with dynamic control flow but it does not work,torch.jit.trace,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/120075,2024-02-16T10:54:07Z,MultiheadAttention can not be exported into onnx with dynamic shapes,torch.nn.MultiheadAttention,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119845,2024-02-13T22:59:26Z,Segmentation fault in dataloader worker sub-process,torch.utils.data.DataLoader,2.2.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/119778,2024-02-13T15:20:07Z,torch.compile doesn't convert all input scalar types to symbolic values,torch.compile,2.2.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/119698,2024-02-12T18:10:14Z,torch._inductor.triton_heuristics.cached_autotune is not thread safe / suited for multiprocessing via DDP/FSDP,torch._inductor.triton_heuristics.cached_autotune,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119665,2024-02-11T23:49:17Z,module.to(device) does not work under FakeTensorMode,torch.nn.Module,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119460,2024-02-08T16:45:11Z,Torch Dynamo fails to trace `torch.nn.RReLU()`,torch.nn.RReLU,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119453,2024-02-08T14:43:48Z,torch.combinations allocates too much memory,torch.combinations,2.2.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/119363,2024-02-07T09:18:50Z,[ONNX] None as input to `aten::index_put` unsupported,torch.onnx.dynamo_export,2.2.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/119345,2024-02-07T02:23:21Z,all_reduce misaligned address with bfloat16,torch.distributed.all_reduce,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/118736,2024-01-31T08:34:07Z,[typing] `torch.Tensor` subclass types (eg `nn.Parameter`) broken by 2.2 release,torch.Tensor,2.2.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/118058,2024-01-23T02:31:23Z,torch.export doesn't handle functools.partial correctly.,torch.export.export,2.2.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117233,2024-01-11T08:18:55Z,Submit a bug when trying to assign arg 'out=' in api torch._foreach_add_,torch._foreach_add_,2.2.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/115516,2023-12-11T01:50:38Z,[FakeTensor] FakeTensorMode dispatch won't change generator state,torch.Generator,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/115516,2023-12-11T01:50:38Z,[FakeTensor] FakeTensorMode dispatch won't change generator state,torch._C.Generator,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/115516,2023-12-11T01:50:38Z,[FakeTensor] FakeTensorMode dispatch won't change generator state,torch.rand,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/115362,2023-12-07T18:09:09Z,torch._fused_sdp_choice for fake and real tensors returns different results,torch._fused_sdp_choice,2.2.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/115347,2023-12-07T14:52:59Z,[Functorch] vmap over index_select expands the output,torch.vmap,2.2.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/115347,2023-12-07T14:52:59Z,[Functorch] vmap over index_select expands the output,torch.index_select,2.2.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/115307,2023-12-06T23:13:59Z,torch._inductor.config.save_args=True and dynamic shapes do not work together,torch._inductor.config,2.2.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/115307,2023-12-06T23:13:59Z,torch._inductor.config.save_args=True and dynamic shapes do not work together,torch.compile,2.2.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/115307,2023-12-06T23:13:59Z,torch._inductor.config.save_args=True and dynamic shapes do not work together,torch._dynamo.exc.BackendCompilerFailed,2.2.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/115307,2023-12-06T23:13:59Z,torch._inductor.config.save_args=True and dynamic shapes do not work together,torch._inductor.compile_fx,2.2.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/115307,2023-12-06T23:13:59Z,torch._inductor.config.save_args=True and dynamic shapes do not work together,torch._functorch.aot_autograd,2.2.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/115307,2023-12-06T23:13:59Z,torch._inductor.config.save_args=True and dynamic shapes do not work together,torch._dynamo.output_graph,2.2.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/115307,2023-12-06T23:13:59Z,torch._inductor.config.save_args=True and dynamic shapes do not work together,torch._dynamo.symbolic_convert,2.2.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/115307,2023-12-06T23:13:59Z,torch._inductor.config.save_args=True and dynamic shapes do not work together,torch._dynamo.eval_frame,2.2.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/115261,2023-12-06T13:41:13Z,[Inductor][cpu][miscompile] Outputs of torch.matmul abnormally change with extra outputs,torch.matmul,2.2.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/115261,2023-12-06T13:41:13Z,[Inductor][cpu][miscompile] Outputs of torch.matmul abnormally change with extra outputs,torch.add,2.2.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/115261,2023-12-06T13:41:13Z,[Inductor][cpu][miscompile] Outputs of torch.matmul abnormally change with extra outputs,torch.cat,2.2.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/115261,2023-12-06T13:41:13Z,[Inductor][cpu][miscompile] Outputs of torch.matmul abnormally change with extra outputs,torch.max,2.2.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/115249,2023-12-06T07:10:56Z,[AOT Autograd] operator decomposition does not respect training=False flag for torch.nn.functional.dropout,torch.nn.functional.dropout,2.2.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/114928,2023-12-01T03:45:39Z,`torch.nn.utils.rnn.pack_padded_sequence` heap-buffer-overflow,torch.nn.utils.rnn.pack_padded_sequence,2.2.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/114928,2023-12-01T03:45:39Z,`torch.nn.utils.rnn.pack_padded_sequence` heap-buffer-overflow,torch.quantize_per_tensor,2.2.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/114483,2023-11-23T20:55:35Z,dynamo supports Tensor.tolist but not Tensor.item,torch.Tensor.tolist,2.2.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/114483,2023-11-23T20:55:35Z,dynamo supports Tensor.tolist but not Tensor.item,torch.Tensor.item,2.2.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/114105,2023-11-20T07:52:40Z,Unexpected `None` value for stream with dynamo,torch.cuda.current_stream,2.2.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/113945,2023-11-17T12:25:52Z,torch.jit.trace has incorrect execution for += operation during compilation,torch.jit.trace,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/113945,2023-11-17T12:25:52Z,torch.jit.trace has incorrect execution for += operation during compilation,torch.compile,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/113945,2023-11-17T12:25:52Z,torch.jit.trace has incorrect execution for += operation during compilation,torch.onnx.export,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/113560,2023-11-13T15:03:00Z,t.contiguous() ~10 slower in eager mode compared to torch.compile,torch.t,2.2.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/113560,2023-11-13T15:03:00Z,t.contiguous() ~10 slower in eager mode compared to torch.compile,torch.contiguous,2.2.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/113560,2023-11-13T15:03:00Z,t.contiguous() ~10 slower in eager mode compared to torch.compile,torch.compile,2.2.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/113496,2023-11-10T23:17:35Z,"FSDP.forward() fails ""_is_root should not have been set"" error after saving a distributed checkpoint",torch.distributed.checkpoint,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/113496,2023-11-10T23:17:35Z,"FSDP.forward() fails ""_is_root should not have been set"" error after saving a distributed checkpoint",torch.distributed.fsdp.FullyShardedDataParallel,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/113496,2023-11-10T23:17:35Z,"FSDP.forward() fails ""_is_root should not have been set"" error after saving a distributed checkpoint",torch.distributed.fsdp.api.StateDictType,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/113496,2023-11-10T23:17:35Z,"FSDP.forward() fails ""_is_root should not have been set"" error after saving a distributed checkpoint",torch.distributed.fsdp.wrap.ModuleWrapPolicy,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/113326,2023-11-09T02:32:21Z,`torch._C._cuda_getDeviceCount` inflates system memory usage,torch._C._cuda_getDeviceCount,2.2.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/113203,2023-11-07T21:00:23Z,`inference_mode` before training results in FSDP AssertionError,torch.inference_mode,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/113203,2023-11-07T21:00:23Z,`inference_mode` before training results in FSDP AssertionError,torch.distributed.fsdp.FullyShardedDataParallel,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/113180,2023-11-07T18:35:47Z,Higher train loss and worse evaluation metrics when using `torch.compile()`,torch.compile,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/113129,2023-11-07T05:45:59Z,[torch.compile] Dynamic shape behavior is different between using torch.compile with and without compiled_autograd.enable,torch.compile,2.2.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/113129,2023-11-07T05:45:59Z,[torch.compile] Dynamic shape behavior is different between using torch.compile with and without compiled_autograd.enable,compiled_autograd.enable,2.2.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/113067,2023-11-06T20:50:10Z,[ONNX] stft export fails with dynamo_export,torch.onnx.dynamo_export,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/113067,2023-11-06T20:50:10Z,[ONNX] stft export fails with dynamo_export,torch.stft,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/112844,2023-11-03T11:06:39Z,torch.export does not support torchaudio.transforms.Spectrogram,torch.onnx.dynamo_export,2.2.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/112844,2023-11-03T11:06:39Z,torch.export does not support torchaudio.transforms.Spectrogram,torch.jit.isinstance,2.2.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/112844,2023-11-03T11:06:39Z,torch.export does not support torchaudio.transforms.Spectrogram,torchaudio.transforms.Spectrogram,2.2.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/112831,2023-11-03T07:31:45Z,dynamo_export successfully export model but fails at onnx.checker.check_model,torch.onnx.export,2.2.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/112787,2023-11-02T21:00:43Z,"AssertionError: Global state changed while dynamo tracing, please report a bug",torch.get_default_dtype,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/112787,2023-11-02T21:00:43Z,"AssertionError: Global state changed while dynamo tracing, please report a bug",torch.set_default_dtype,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/112787,2023-11-02T21:00:43Z,"AssertionError: Global state changed while dynamo tracing, please report a bug",torch.tensor,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/112787,2023-11-02T21:00:43Z,"AssertionError: Global state changed while dynamo tracing, please report a bug",torch.compile,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/112575,2023-11-01T09:51:57Z,"RuntimeError: ""grid_sampler_2d_cuda"" not implemented for 'BFloat16'",torch.grid_sample,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/112515,2023-10-31T17:00:49Z,[Bug Report]FSDP: An error raises when loading FSDP distributed checkpoint with ignoring modules.,torch.distributed.fsdp.FullyShardedDataParallel,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/112515,2023-10-31T17:00:49Z,[Bug Report]FSDP: An error raises when loading FSDP distributed checkpoint with ignoring modules.,torch.distributed.checkpoint.optimizer.load_sharded_optimizer_state_dict,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/112515,2023-10-31T17:00:49Z,[Bug Report]FSDP: An error raises when loading FSDP distributed checkpoint with ignoring modules.,torch.distributed.checkpoint.save_state_dict,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/112515,2023-10-31T17:00:49Z,[Bug Report]FSDP: An error raises when loading FSDP distributed checkpoint with ignoring modules.,torch.distributed.checkpoint.load_state_dict,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/112495,2023-10-31T12:22:25Z,Compilation Failure of torch.special.exp2 in torch.compile Optimized Mode,torch.special.exp2,2.2.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/112492,2023-10-31T11:21:55Z,Compilation Failure of torch.cumsum in torch.compile Optimized Mode,torch.cumsum,2.2.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/112347,2023-10-29T17:45:28Z,"[dynamo] `.view([..., -1, ...])` fails on Tensors with unbacked SymInts in the shape",torch.view,2.2.0,12
pytorch,https://github.com/pytorch/pytorch/issues/111908,2023-10-24T11:42:42Z,Inconsistent Keyword Arguments behaviors in torch.triangular_solve(),torch.triangular_solve,2.2.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/111900,2023-10-24T07:47:04Z,"OOM when saving model(lora adapter), seems the clause ""FullyShardedDataParallel(model,...)"" will directly cause the OOM.",torch.distributed.fsdp.FullyShardedDataParallel,2.2.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/110356,2023-10-01T13:37:59Z,"Dropout signature inconsistent between `torch.dropout`, `torch.nn.Dropout` and `torch.nn.functional.dropout`",torch.dropout,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/110356,2023-10-01T13:37:59Z,"Dropout signature inconsistent between `torch.dropout`, `torch.nn.Dropout` and `torch.nn.functional.dropout`",torch.nn.Dropout,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/110356,2023-10-01T13:37:59Z,"Dropout signature inconsistent between `torch.dropout`, `torch.nn.Dropout` and `torch.nn.functional.dropout`",torch.nn.functional.dropout,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/110356,2023-10-01T13:37:59Z,"Dropout signature inconsistent between `torch.dropout`, `torch.nn.Dropout` and `torch.nn.functional.dropout`",torch.nn.functional.alpha_dropout,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/110356,2023-10-01T13:37:59Z,"Dropout signature inconsistent between `torch.dropout`, `torch.nn.Dropout` and `torch.nn.functional.dropout`",torch.nn.functional.feature_alpha_dropout,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/113744,2023-11-15T08:49:07Z,Regression: `capture_pre_autograd_graph` does not support empty args and kwargs only anymore,torch._export.capture_pre_autograd_graph,2.2.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/132649,2024-08-05T13:18:55Z,nn.Parallel,torch.nn.DataParallel,2.1.2,11.8
pytorch,https://github.com/pytorch/pytorch/issues/130985,2024-07-17T21:04:12Z,Runtime error when running huggingface pretrained model with torch-xla,torch.cosine_similarity,2.1.2,CPU
pytorch,https://github.com/pytorch/pytorch/issues/128703,2024-06-14T14:50:38Z,autograd with `is_grads_batched=True` fails on GroupNorm,API,2.1.2,CPU
pytorch,https://github.com/pytorch/pytorch/issues/128703,2024-06-14T14:50:38Z,autograd with `is_grads_batched=True` fails on GroupNorm,Name,2.1.2,CPU
pytorch,https://github.com/pytorch/pytorch/issues/128703,2024-06-14T14:50:38Z,autograd with `is_grads_batched=True` fails on GroupNorm,torch.autograd.grad,2.1.2,CPU
pytorch,https://github.com/pytorch/pytorch/issues/124937,2024-04-25T13:54:29Z,Conflict between bias=False and why_not_sparsity_fast_path in Transformer Module,torch.utils.backend_registration._privateuse1_backend_name,2.1.2,CPU
pytorch,https://github.com/pytorch/pytorch/issues/123533,2024-04-07T23:19:06Z,Memory allocation for Conv2d,torch.nn.Conv2d,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/122093,2024-03-18T14:23:32Z,Bug in `torch.compile` with standard type checking tools beartype,torch.compile,2.1.2,CPU
pytorch,https://github.com/pytorch/pytorch/issues/120905,2024-02-29T13:15:40Z,F.conv2d double precision error,torch.nn.functional.conv2d,2.1.2,11.8
pytorch,https://github.com/pytorch/pytorch/issues/120790,2024-02-28T09:43:19Z,MultiheadAttention returns incorrect results for windowed long causal mask for long input,torch.nn.MultiheadAttention,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/120591,2024-02-26T02:20:58Z,torch.distributed.DistBackendError,torch.distributed,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/120354,2024-02-21T23:00:53Z,Performance degradation for `aten::fill_` and `aten::add_` in backward pass,torch.aten::fill_,2.1.2,CPU
pytorch,https://github.com/pytorch/pytorch/issues/120354,2024-02-21T23:00:53Z,Performance degradation for `aten::fill_` and `aten::add_` in backward pass,torch.aten::add_,2.1.2,CPU
pytorch,https://github.com/pytorch/pytorch/issues/119131,2024-02-03T21:05:34Z,"`scaled_dot_product_attention` produces NaNs for math backend, but not memory efficient backend when used in mixed precision.",torch.nn.functional.scaled_dot_product_attention,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119123,2024-02-03T09:55:56Z,"`DataLoader` generic argument is dropped in `__iter__`, making it non type-safe",torch.utils.data.DataLoader,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119123,2024-02-03T09:55:56Z,"`DataLoader` generic argument is dropped in `__iter__`, making it non type-safe",torch.utils.data.Dataset,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119076,2024-02-02T21:50:49Z,torch.bmm fails when multiplying a batch of sparse matrices by a dense tensor,torch.bmm,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119000,2024-02-02T13:39:38Z,Performance regression in torch.nn.Linear,torch.nn.Linear,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119000,2024-02-02T13:39:38Z,Performance regression in torch.nn.Linear,torch.functional.linear,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/118853,2024-02-01T08:47:00Z,MultiheadAttention returns different results for binary mask and float mask,torch.nn.MultiheadAttention,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/118492,2024-01-28T04:50:12Z,`torch.compile(dynamic=True)` compiles forever,torch.compile,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/117961,2024-01-22T04:36:52Z,ReplicatePad2d cuda not support bfloat16,torch.replication_pad2d_cuda,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/117855,2024-01-19T15:15:26Z,"torch.combinations dies with ""numel: integer multiplication overflow""",torch.combinations,2.1.2,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117844,2024-01-19T11:00:27Z,torch.fft.rfft: Unsupported dtype BFloat16,torch.fft.rfft,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/117736,2024-01-18T07:17:04Z,Illegal memory access for nn.Conv2d when intermediate output has size larger than 2**31,torch.nn.Conv2d,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/117540,2024-01-16T12:04:51Z,Unwanted Warning in lr_scheduler.step(),torch.optim.lr_scheduler.CosineAnnealingLR,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116858,2024-01-05T12:32:30Z,torch.compile(...) does not pass models containing RNN layers to custom compilers,torch.compile,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116533,2023-12-29T10:08:30Z,`backward` implicit conversion from tuple to `torch.Tensor` results in an indexing error message,torch.autograd.Function.backward,2.1.2,CPU
pytorch,https://github.com/pytorch/pytorch/issues/116393,2023-12-25T12:50:13Z,Error: IndexError: map::at When using torch.distributed.all_reduce(tensor),torch.distributed.all_reduce,2.1.2,CPU
pytorch,https://github.com/pytorch/pytorch/issues/116336,2023-12-22T21:26:04Z,"ONNX export: Unsupported FX nodes: {'call_function': ['aten._sample_dirichlet.default',",torch.export.export,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116336,2023-12-22T21:26:04Z,"ONNX export: Unsupported FX nodes: {'call_function': ['aten._sample_dirichlet.default',",torch.onnx.dynamo_export,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116336,2023-12-22T21:26:04Z,"ONNX export: Unsupported FX nodes: {'call_function': ['aten._sample_dirichlet.default',",torch.empty,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116336,2023-12-22T21:26:04Z,"ONNX export: Unsupported FX nodes: {'call_function': ['aten._sample_dirichlet.default',",torch.distributions.transformed_distribution.rsample,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116336,2023-12-22T21:26:04Z,"ONNX export: Unsupported FX nodes: {'call_function': ['aten._sample_dirichlet.default',",torch._dynamo.export,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116336,2023-12-22T21:26:04Z,"ONNX export: Unsupported FX nodes: {'call_function': ['aten._sample_dirichlet.default',",torch._dynamo.utils.wrap_fake_exception,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116336,2023-12-22T21:26:04Z,"ONNX export: Unsupported FX nodes: {'call_function': ['aten._sample_dirichlet.default',",torch._dynamo.utils.get_fake_value,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116336,2023-12-22T21:26:04Z,"ONNX export: Unsupported FX nodes: {'call_function': ['aten._sample_dirichlet.default',",torch._dynamo.exc.UserError,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116336,2023-12-22T21:26:04Z,"ONNX export: Unsupported FX nodes: {'call_function': ['aten._sample_dirichlet.default',",torch._dynamo.utils.time_wrapper,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116336,2023-12-22T21:26:04Z,"ONNX export: Unsupported FX nodes: {'call_function': ['aten._sample_dirichlet.default',",torch._dynamo.symbolic_convert.run,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116336,2023-12-22T21:26:04Z,"ONNX export: Unsupported FX nodes: {'call_function': ['aten._sample_dirichlet.default',",torch._dynamo.symbolic_convert.step,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116336,2023-12-22T21:26:04Z,"ONNX export: Unsupported FX nodes: {'call_function': ['aten._sample_dirichlet.default',",torch._dynamo.symbolic_convert.wrapper,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116336,2023-12-22T21:26:04Z,"ONNX export: Unsupported FX nodes: {'call_function': ['aten._sample_dirichlet.default',",torch._dynamo.symbolic_convert.inline_user_function_return,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116336,2023-12-22T21:26:04Z,"ONNX export: Unsupported FX nodes: {'call_function': ['aten._sample_dirichlet.default',",torch._dynamo.symbolic_convert.inline_call,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116336,2023-12-22T21:26:04Z,"ONNX export: Unsupported FX nodes: {'call_function': ['aten._sample_dirichlet.default',",torch._dynamo.symbolic_convert.inline_call_,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116120,2023-12-19T19:48:09Z,bfloat16 on 'grid_sampler_3d_cuda' not implemented,torch.grid_sampler,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116018,2023-12-18T12:51:01Z,Inconsistent behavior of torch.einsum on float32 and complex128,torch.einsum,2.1.2,CPU
pytorch,https://github.com/pytorch/pytorch/issues/129442,2024-06-25T03:04:25Z,Volatile GPU-Util keeps 100% when using torch.distributed.isend/irecv,torch.distributed.isend,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129442,2024-06-25T03:04:25Z,Volatile GPU-Util keeps 100% when using torch.distributed.isend/irecv,torch.distributed.irecv,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116016,2023-12-18T11:46:55Z,[distibuted] torch.distributed.new_group failed with Segmentation fault,torch.distributed.new_group,2.1.2,11
pytorch,https://github.com/pytorch/pytorch/issues/122886,2024-03-28T12:29:02Z,torch.normal ignores default_device,torch.normal,2.1.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/122886,2024-03-28T12:29:02Z,torch.normal ignores default_device,torch.from_numpy,2.1.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/121470,2024-03-08T03:05:24Z,The output of `fuse_modules` and `fuse_modules_qat` is inconsistent,torch.ao.quantization.fuse_modules,2.1.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/121470,2024-03-08T03:05:24Z,The output of `fuse_modules` and `fuse_modules_qat` is inconsistent,torch.ao.quantization.fuse_modules_qat,2.1.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/119983,2024-02-15T11:02:45Z,Using masked_scale in a torch.compile graph without meta registration breaks the graph,torch._masked_scale,2.1.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/119371,2024-02-07T11:50:10Z,Nan output after masked TransforrmerDecoder,torch.nn.TransformerDecoder,2.1.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/119371,2024-02-07T11:50:10Z,Nan output after masked TransforrmerDecoder,torch.nn.TransformerDecoderLayer,2.1.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117209,2024-01-11T03:23:36Z,"torch.onnx.export the torch,nn.TransformerEncoderLayer, the dynamic_axes work not right",torch.onnx.export,2.1.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/117209,2024-01-11T03:23:36Z,"torch.onnx.export the torch,nn.TransformerEncoderLayer, the dynamic_axes work not right",torch.nn.TransformerEncoderLayer,2.1.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/117209,2024-01-11T03:23:36Z,"torch.onnx.export the torch,nn.TransformerEncoderLayer, the dynamic_axes work not right",torch.nn.TransformerEncoder,2.1.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/117130,2024-01-10T18:35:26Z,`torch.cuda.memory_summary()` can give `KeyError`,torch.cuda.memory_summary,2.1.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/117130,2024-01-10T18:35:26Z,`torch.cuda.memory_summary()` can give `KeyError`,torch.cuda.memory_stats,2.1.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116992,2024-01-08T21:14:57Z,torch.fake_quantize_per_tensor_affine with inf input provides wrong output value,torch.fake_quantize_per_tensor_affine,2.1.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/116183,2023-12-20T11:36:14Z,Pipeline parallel doesn't seem to work with mixed precision,torch.distributed.pipeline.sync.Pipe,2.1.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116117,2023-12-19T18:56:45Z,WeightedRandomSampler doesnt keep expected ratio betwen samples,torch.utils.data.WeightedRandomSampler,2.1.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/116055,2023-12-18T20:11:55Z,Operating on CUDA tensors on multiprocessing queue hangs,torch.mean,2.1.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/115914,2023-12-15T07:13:16Z,torch.jit.script fails when parsing torch.SymInt,torch.jit.script,2.1.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/115914,2023-12-15T07:13:16Z,torch.jit.script fails when parsing torch.SymInt,torch.SymInt,2.1.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/115624,2023-12-12T11:36:21Z,`amax` fails on `masked_tensor` with multiple negative dims,torch.amax,2.1.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/115534,2023-12-11T09:54:39Z,Torch.export.export fails with SAM and returns a non-informative Error message,torch.export.export,2.1.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/115484,2023-12-09T19:55:44Z,torch.compile() breaks when using DeepSpeed ZeRO Level 3 sharding,torch.compile(),2.1.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/115416,2023-12-08T12:22:02Z,torch.nn.functional.embedding segmentation fault on large negative ids,torch.nn.functional.embedding,2.1.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/114529,2023-11-25T05:43:02Z,Bug in element-wise multiplication of `torch.sparse_csr_tensor`s on GPU - 0's in result considered significant - PyTorch 2.1.1,torch.sparse_csr_tensor,2.1.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/114035,2023-11-19T00:13:39Z,Load model from jit script format. Repeating inference several times can lead to errors.,torch.jit.trace,2.1.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/114035,2023-11-19T00:13:39Z,Load model from jit script format. Repeating inference several times can lead to errors.,torch.jit.save,2.1.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/114035,2023-11-19T00:13:39Z,Load model from jit script format. Repeating inference several times can lead to errors.,torch.jit.load,2.1.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/114035,2023-11-19T00:13:39Z,Load model from jit script format. Repeating inference several times can lead to errors.,torch._C._jit_override_can_fuse_on_cpu,2.1.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/114035,2023-11-19T00:13:39Z,Load model from jit script format. Repeating inference several times can lead to errors.,torch._C._jit_override_can_fuse_on_gpu,2.1.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/114035,2023-11-19T00:13:39Z,Load model from jit script format. Repeating inference several times can lead to errors.,torch._C._jit_set_texpr_fuser_enabled,2.1.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/114035,2023-11-19T00:13:39Z,Load model from jit script format. Repeating inference several times can lead to errors.,torch._C._jit_set_nvfuser_enabled,2.1.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/119147,2024-02-04T09:48:52Z,Incorrect Results from cdist_backward Operator with Input Dimensions Exceeding int32_max,torch.cdist_backward,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/115472,2023-12-09T01:14:47Z,Potential issue with custom transformer masks when using fast path and batch_first=True,torch.nn.Transformer,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/132240,2024-07-31T14:56:35Z,torch.nn.functional.interpolate completely broken with torch.jit.script and torch.fx,torch.nn.functional.interpolate,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/132240,2024-07-31T14:56:35Z,torch.nn.functional.interpolate completely broken with torch.jit.script and torch.fx,torch.jit.script,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/132240,2024-07-31T14:56:35Z,torch.nn.functional.interpolate completely broken with torch.jit.script and torch.fx,torchvision.models.feature_extraction.create_feature_extractor,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/132240,2024-07-31T14:56:35Z,torch.nn.functional.interpolate completely broken with torch.jit.script and torch.fx,torch.fx,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/129651,2024-06-27T11:01:06Z,`torch.nn.function.one_hot` and `torch.Tensor.as_subclass` API not available under `torch.compile`,torch.nn.function.one_hot,2.1.0,12.2
pytorch,https://github.com/pytorch/pytorch/issues/129651,2024-06-27T11:01:06Z,`torch.nn.function.one_hot` and `torch.Tensor.as_subclass` API not available under `torch.compile`,torch.Tensor.as_subclass,2.1.0,12.2
pytorch,https://github.com/pytorch/pytorch/issues/129648,2024-06-27T10:39:13Z,`torch.compile` mode and eager mode are different in generating tensor API behavior,torch.tensor,2.1.0,12.2
pytorch,https://github.com/pytorch/pytorch/issues/129200,2024-06-21T02:51:49Z,"Export model using onnx.dynamo_export has bug of ""torch._dynamo.exc.Unsupported: call_method TupleVariable() size [ConstantVariable(int)] {}""",torch.onnx.dynamo_export,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128131,2024-06-06T10:26:28Z,Unable to record Memory consumption with `torch.cuda.memory._record_memory_history()`,torch.cuda.memory._record_memory_history,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/128125,2024-06-06T07:46:25Z,optional memory leak with torch.jit.script on models with fft,torch.jit.script,2.1.0,12
pytorch,https://github.com/pytorch/pytorch/issues/128125,2024-06-06T07:46:25Z,optional memory leak with torch.jit.script on models with fft,torch.irfft,2.1.0,12
pytorch,https://github.com/pytorch/pytorch/issues/128125,2024-06-06T07:46:25Z,optional memory leak with torch.jit.script on models with fft,torch.rfft,2.1.0,12
pytorch,https://github.com/pytorch/pytorch/issues/125064,2024-04-26T19:24:35Z,ValueError: weight_norm of 'weight' not found in ParametrizedConvTranspose1d,torch.nn.utils.weight_norm,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/125064,2024-04-26T19:24:35Z,ValueError: weight_norm of 'weight' not found in ParametrizedConvTranspose1d,torch.nn.utils.parametrizations.weight_norm,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/125064,2024-04-26T19:24:35Z,ValueError: weight_norm of 'weight' not found in ParametrizedConvTranspose1d,torch.nn.utils.remove_weight_norm,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/124838,2024-04-24T12:29:22Z,Forwardpropagation of high-dimensional tensors through the nn.Linear module becomes multiple times slower since pytorch 2.1.0,torch.nn.Linear,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/124788,2024-04-23T21:57:16Z,torch.nn.checkpoint.checkpoint ignores default device in backward() call,torch.utils.checkpoint.checkpoint,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/124595,2024-04-22T06:00:18Z,torch._export can't export resnet50 model,torch._export.capture_pre_autograd_graph,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121711,2024-03-12T05:29:35Z,torch.func.grad hangs on nested differentiation,torch.func.grad,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121683,2024-03-11T23:20:03Z,linalg.ldl_factor raises an error with zero tensor,torch.linalg.ldl_factor,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121588,2024-03-09T20:00:55Z,logistic sigmoid function outputs NaN indeterminate on complex input,torch.special.expit,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121588,2024-03-09T20:00:55Z,logistic sigmoid function outputs NaN indeterminate on complex input,torch.nn.functional.sigmoid,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121220,2024-03-05T11:08:24Z,Misleading `log_abs_det_jacobian` shape for `StackTransform`?,torch.distributions.transforms.StackTransform,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121220,2024-03-05T11:08:24Z,Misleading `log_abs_det_jacobian` shape for `StackTransform`?,torch.distributions.transforms.AffineTransform,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121220,2024-03-05T11:08:24Z,Misleading `log_abs_det_jacobian` shape for `StackTransform`?,torch.distributions.transforms.IndependentTransform,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121220,2024-03-05T11:08:24Z,Misleading `log_abs_det_jacobian` shape for `StackTransform`?,torch.distributions.transforms.log_abs_det_jacobian,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121069,2024-03-02T08:06:45Z,torch.compile()' causes 'Accuracy failed: uint8 tensor did not match',torch.compile,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/120597,2024-02-26T05:41:30Z,floating point exception on `torch.fmod` when x is large negative integer and y is `-1`,torch.fmod,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119420,2024-02-07T23:09:01Z,torch.stft sometimes raises RuntimeError: cuFFT error: CUFFT_INTERNAL_ERROR on low free memory,torch.stft,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119360,2024-02-07T07:18:45Z,`torch.onnx.dynamo_export` provides wrong output shape for `torch.fft.irfft`,torch.onnx.dynamo_export,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119360,2024-02-07T07:18:45Z,`torch.onnx.dynamo_export` provides wrong output shape for `torch.fft.irfft`,torch.fft.irfft,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119188,2024-02-05T12:15:59Z,scaled_dot_product_attention: different output between the reference implementation and the one from torch.nn.functional,torch.nn.functional.scaled_dot_product_attention,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/118829,2024-02-01T01:08:40Z,The inference result is wrong when I apply the torch.quantization.fuse_modules api to resnet50 model.,torch.quantization.fuse_modules,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/118628,2024-01-30T08:53:34Z,TransformerEncoderLayer fast path predicts NaN when provided attention bias,torch.nn.TransformerEncoderLayer,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/117617,2024-01-17T07:10:53Z,"torch.topk, when the input contains +nan and -nan, the result is not sure; sometimes -nan is treated greater than normal number, sometimes -nan is the least;",torch.topk,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117348,2024-01-12T12:41:44Z,The bitwise_XX.Scalar_Tensor should be dispatched to core IR but is currently not,torch.ops.aten.bitwise_and.Scalar_Tensor,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116835,2024-01-05T04:47:35Z,torch.compile fullgraph=True is failing for GPTJ model for toy_backend,torch.compile,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116766,2024-01-04T07:12:44Z,[Dynamo][DeepSpeed] torch._dynamo.exc.InternalTorchDynamoError: NestedUserFunctionVariable() has no type,torch._dynamo,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/116697,2024-01-03T12:37:30Z,Tracing GPU usage of `torch.vmap` vectorized operation using `torch.profiler` cause CUDA OOM,torch.vmap,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116697,2024-01-03T12:37:30Z,Tracing GPU usage of `torch.vmap` vectorized operation using `torch.profiler` cause CUDA OOM,torch.profiler.profile,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116697,2024-01-03T12:37:30Z,Tracing GPU usage of `torch.vmap` vectorized operation using `torch.profiler` cause CUDA OOM,torch.cuda.synchronize,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116697,2024-01-03T12:37:30Z,Tracing GPU usage of `torch.vmap` vectorized operation using `torch.profiler` cause CUDA OOM,torch.cuda.empty_cache,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116077,2023-12-19T02:32:23Z,TransformerEncoderLayer and TransformerDecoderLayer do not work,torch.TransformerEncoderLayer,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116077,2023-12-19T02:32:23Z,TransformerEncoderLayer and TransformerDecoderLayer do not work,torch.TransformerDecoderLayer,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/115785,2023-12-13T23:37:58Z,arctan2 fp16 error when optimising,torch.arctan2,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/115492,2023-12-09T23:43:36Z,[dynamic shapes] torch.fx.experimental.symbolic_shapes.ConstraintViolationError from demucs with dynamic shapes,torch.fx.experimental.symbolic_shapes,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/115422,2023-12-08T15:31:20Z,string representation method for empty masked tensors fails,torch.masked.masked_tensor,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/115388,2023-12-07T23:01:16Z,`torch.distributed.destroy_process_group()` hangs after CUDA graph capture of NCCL operations,torch.distributed.destroy_process_group,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/115342,2023-12-07T11:59:20Z,torch.func.grad not working with list comprehension over dictionary,torch.func.grad,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/114943,2023-12-01T10:24:55Z,torch.einsum may choose a strategy for which there is not enough memory,torch.einsum,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/114672,2023-11-28T07:52:42Z,torch.fft.ifft crashes for empty input,torch.fft.ifft,2.1.0,11.3.1
pytorch,https://github.com/pytorch/pytorch/issues/114216,2023-11-21T08:11:06Z,torch.nn.functional.max_pool2d outputs inf,torch.nn.functional.max_pool2d,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/113942,2023-11-17T10:26:02Z,Tensorboard list of tensors as input,torch.jit.trace,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/113444,2023-11-10T13:53:18Z,ONNX Export - miscompilation for complex-valued operators,torch.onnx.dynamo_export,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/113443,2023-11-10T12:32:32Z,`CompileProfiler` reports graph breaks while `dynamo.explain` reports no graph breaks,torch._dynamo.explain,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/113443,2023-11-10T12:32:32Z,`CompileProfiler` reports graph breaks while `dynamo.explain` reports no graph breaks,torch._dynamo.utils.CompileProfiler,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/113415,2023-11-10T02:05:54Z,[inline-inbuilt-nn-modules] Torch compile with DDP errors on parameterized modules,torch.compile,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/113024,2023-11-06T13:49:39Z,Upsample bilinear 2d decomposition does not match native implementation for uint8,torch._C.nn.upsample_bilinear2d,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/113024,2023-11-06T13:49:39Z,Upsample bilinear 2d decomposition does not match native implementation for uint8,torch._decomp.decompositions.upsample_bilinear2d_vec,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/112775,2023-11-02T19:35:40Z,Multi-Threaded GraphModule / torch.fx inference raises an exception,torch.fx.symbolic_trace,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/112569,2023-11-01T06:51:05Z,"RuntimeError: NVML_SUCCESS == r INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1695392035891/work/c10/cuda/CUDACachingAllocator.cpp"":1154, please report a bug to PyTorch",torch.nn.modules.module,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/112569,2023-11-01T06:51:05Z,"RuntimeError: NVML_SUCCESS == r INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1695392035891/work/c10/cuda/CUDACachingAllocator.cpp"":1154, please report a bug to PyTorch",torch.nn.modules.conv,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/112569,2023-11-01T06:51:05Z,"RuntimeError: NVML_SUCCESS == r INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1695392035891/work/c10/cuda/CUDACachingAllocator.cpp"":1154, please report a bug to PyTorch",torch.nn.functional,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/112095,2023-10-25T23:17:57Z,torch.export fails on a model with optional parameter,torch.export.export,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/111925,2023-10-24T15:37:51Z,Resize warning in two argument torch.logical_* with broadcasting,torch.logical_and,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/111901,2023-10-24T08:11:27Z,Multiprocess. DataLoader worker is killed by signal: Segmentation fault.,torch.utils.data.dataloader,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/111824,2023-10-23T18:22:50Z,GroupNorm & InstanceNorm does not handle channels_last correctly,torch.nn.GroupNorm,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/111570,2023-10-19T15:24:58Z,Tensor `.cuda()` very slow with specific array sizes,torch.cuda,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/111509,2023-10-18T21:16:16Z,Sparse Tensor Sum Still Does Not Work for PyTorch Geometric,torch.sum,2.1.0,12.2
pytorch,https://github.com/pytorch/pytorch/issues/111480,2023-10-18T16:11:12Z,torch.jit.script persistently changes default from utf-8 to ascii,torch.jit.script,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/111187,2023-10-13T05:23:31Z,"torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1331, unhandled cuda error (run with NCCL_DEBUG=INFO for details)",torch.distributed,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/111182,2023-10-13T03:24:50Z,Training iresnet with torch.compile is slower than eager mode for torch 2.1.0,torch.compile,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/111142,2023-10-12T16:30:09Z,torch._C.Node' object has no attribute 'cs',torch._C.Node,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/111138,2023-10-12T14:55:22Z,Module states cannot be fully synchronized due to the DDP broadcast_buffers breaking change,torch.nn.parallel.DistributedDataParallel,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/111135,2023-10-12T13:22:34Z,"No op for aten::where with argument types: Tensor, Tensor, bool.",torch.where,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/110959,2023-10-10T17:37:02Z,`model.named_buffers()` fails if module not hashable.,torch.nn.Module.named_buffers,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/110885,2023-10-09T19:29:22Z,Depthwise conv3d slower than normal conv3d,torch.nn.Conv3d,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/110775,2023-10-07T00:04:43Z,torch.distributed.pipeline skip module throws assert error that portal.grad is not None,torch.distributed.pipeline,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/110148,2023-09-27T09:33:35Z,Torch.onnx.dynamo_export stuck at reshape,torch.onnx.dynamo_export,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/109586,2023-09-19T08:21:03Z,Max pool with negative integer inputs and channels_last memory layout gives the wrong values,torch.max_pool2d,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/109497,2023-09-18T13:41:02Z,Very big differences in output of `torch.lobpcg` (values and run-time) compared to SciPy on a very ill-conditioned Laplacian matrix,torch.lobpcg,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/109108,2023-09-12T13:27:01Z,Cannot construct `torch.sparse_coo_tensor` (but `scipy.sparse.coo_matrix` works fine): `TypeError: only integer tensors of a single element can be converted to an index`,torch.sparse_coo_tensor,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/108432,2023-09-01T17:53:14Z,Wrong result of first run with torch.compile() when backend is using torch.jit.trace() and model has inplace operators,torch.compile,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/108432,2023-09-01T17:53:14Z,Wrong result of first run with torch.compile() when backend is using torch.jit.trace() and model has inplace operators,torch.jit.trace,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/108407,2023-09-01T09:19:57Z,torch.einsum() computes different results on cpu and cuda on A100 GPU.,torch.einsum,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/108190,2023-08-29T22:22:30Z,[FSDP] incorrect backward prefetch order when using BackwardPrefetch.BACKWARD_POST,torch.BackwardPrefetch,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/107824,2023-08-23T22:08:03Z,torch.compile() fails when an `autograd.Function` gets called and torch.no_grad() is *not* being used,torch.compile(),2.1.0,12
pytorch,https://github.com/pytorch/pytorch/issues/107824,2023-08-23T22:08:03Z,torch.compile() fails when an `autograd.Function` gets called and torch.no_grad() is *not* being used,torch.no_grad(),2.1.0,12
pytorch,https://github.com/pytorch/pytorch/issues/107824,2023-08-23T22:08:03Z,torch.compile() fails when an `autograd.Function` gets called and torch.no_grad() is *not* being used,torch.autograd.Function,2.1.0,12
pytorch,https://github.com/pytorch/pytorch/issues/107824,2023-08-23T22:08:03Z,torch.compile() fails when an `autograd.Function` gets called and torch.no_grad() is *not* being used,torch.distributed.get_world_size(),2.1.0,12
pytorch,https://github.com/pytorch/pytorch/issues/107824,2023-08-23T22:08:03Z,torch.compile() fails when an `autograd.Function` gets called and torch.no_grad() is *not* being used,torch.distributed.get_rank(),2.1.0,12
pytorch,https://github.com/pytorch/pytorch/issues/107824,2023-08-23T22:08:03Z,torch.compile() fails when an `autograd.Function` gets called and torch.no_grad() is *not* being used,torch.distributed._functional_collectives.all_reduce,2.1.0,12
pytorch,https://github.com/pytorch/pytorch/issues/107824,2023-08-23T22:08:03Z,torch.compile() fails when an `autograd.Function` gets called and torch.no_grad() is *not* being used,torch.Tensor,2.1.0,12
pytorch,https://github.com/pytorch/pytorch/issues/107824,2023-08-23T22:08:03Z,torch.compile() fails when an `autograd.Function` gets called and torch.no_grad() is *not* being used,torch.nn.Module,2.1.0,12
pytorch,https://github.com/pytorch/pytorch/issues/107824,2023-08-23T22:08:03Z,torch.compile() fails when an `autograd.Function` gets called and torch.no_grad() is *not* being used,torch.nn.Linear,2.1.0,12
pytorch,https://github.com/pytorch/pytorch/issues/107824,2023-08-23T22:08:03Z,torch.compile() fails when an `autograd.Function` gets called and torch.no_grad() is *not* being used,torch.nn.SiLU,2.1.0,12
pytorch,https://github.com/pytorch/pytorch/issues/107824,2023-08-23T22:08:03Z,torch.compile() fails when an `autograd.Function` gets called and torch.no_grad() is *not* being used,torch.nn.init.trunc_normal_,2.1.0,12
pytorch,https://github.com/pytorch/pytorch/issues/107821,2023-08-23T21:36:32Z,`torch.distributions.Pareto.sample` sometimes gives `inf`,torch.distributions.Pareto.sample,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/107678,2023-08-22T10:29:26Z,[Torch.fx] Torch fx failed to trace torch extension library,torch.fx.symbolic_trace,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/107661,2023-08-22T04:12:13Z,A backward bug of dtensor seems to be caused by new_empty_strided,torch.distributed._tensor.api.new_empty_strided,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/107503,2023-08-19T02:37:47Z,[nightly][jit] bad constant exponent (e+38.f) in default_program fused_mul_div_add,torch.jit.trace,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/107298,2023-08-16T15:12:16Z,dist.destroy_process_group did not destroy the process group well,torch.dist.destroy_process_group,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/107087,2023-08-12T16:21:00Z,RuntimeError with operations on torch.float8_e5m2 and torch.float_e4m3fn data types,torch.float8_e5m2,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/107087,2023-08-12T16:21:00Z,RuntimeError with operations on torch.float8_e5m2 and torch.float_e4m3fn data types,torch.float8_e4m3fn,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/106584,2023-08-03T23:23:15Z,Lacking commutativity of `tensor.expand` and `tensor.flatten`,torch.flatten,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/106006,2023-07-26T03:43:07Z,[torch.compile] assertion sometimes ignored with inductor backend,torch.compile,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/106006,2023-07-26T03:43:07Z,[torch.compile] assertion sometimes ignored with inductor backend,torch._dynamo,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/105918,2023-07-25T13:01:29Z,Torch.jit : RuntimeError: Unable to extract string literal index for ModuleDict,torch.jit.script,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/105917,2023-07-25T12:49:22Z,Torch.jit.frontend.NotSupportedError: not supporting functions with variable number of arguments.,torch.jit.script,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/105878,2023-07-24T22:55:32Z,FakeTensor detach() gives meta tensor other than FakeTensor under `torch._C._DisableTorchDispatch()`,torch.detach,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/105635,2023-07-20T03:48:09Z,FSDP with gradient checkpointing lead to redundant allgathers during backward,torch.FSDP,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/104732,2023-07-06T20:08:35Z,"torch.jit.trace says ""Arguments for call are invalid"" on torch.ops.aten.sub(3, x, alpha=3)",torch.ops.aten.sub,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/104698,2023-07-06T10:31:43Z,Incorrect Error Message Ordering for nn.AdaptiveAvgPool2d with Incorrect output_size,torch.nn.AdaptiveAvgPool2d,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/104194,2023-06-26T16:37:04Z,View ops on fake tensors can dispatch `detach`es to backend kernels,torch.detach,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/102610,2023-05-31T11:06:47Z,Cannot invoke prims.sum with output_dtype,torch.ops.prims.sum,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/102609,2023-05-31T10:58:54Z,[prims] torch.ops.aten.le decomposition confuses scalars and tensors,torch.ops.aten.le,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/102609,2023-05-31T10:58:54Z,[prims] torch.ops.aten.le decomposition confuses scalars and tensors,torch.ops.prims.le,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/102197,2023-05-24T19:55:43Z,torch.func.jvp fails when acting on a DistributedDataParallel model,torch.func.jvp,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/101356,2023-05-14T08:31:07Z,inductor: inductor conv2d get a different size and stride with eager mod when input channel is zero,torch.nn.Conv2d,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/101031,2023-05-10T00:05:06Z,[PT2] torch.compile doesn't perform horizontal fusion,torch.compile,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/100985,2023-05-09T16:44:54Z,"native_batch_norm has different size results on ""CPU"" vs ""META"" device",torch.ops.aten.native_batch_norm,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/100838,2023-05-07T22:18:48Z,Dynamo infers different return type vs. eager for `torch.ops.aten`,torch.ops.aten.baddbmm,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/100838,2023-05-07T22:18:48Z,Dynamo infers different return type vs. eager for `torch.ops.aten`,torch.ops.aten.full,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/100838,2023-05-07T22:18:48Z,Dynamo infers different return type vs. eager for `torch.ops.aten`,torch.ops.aten.threshold_backward,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/100807,2023-05-06T18:19:52Z,[torch.compile] returns output with WRONG SHAPE after `cat_slice_cat`,torch.compile,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/100051,2023-04-26T01:53:22Z,Issue of HistogramObserver to handle abnormal value,torch.ao.quantization.observer.HistogramObserver,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/99652,2023-04-20T19:17:12Z,DistributedDataParallel doesn't work with complex buffers,torch.nn.parallel.DistributedDataParallel,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/99432,2023-04-18T17:39:17Z,[DTensor] parallelize_module failed with nn.Transformer and the PairwiseParallel plan,torch.distributed.tensor.parallel.parallelize_module,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/99432,2023-04-18T17:39:17Z,[DTensor] parallelize_module failed with nn.Transformer and the PairwiseParallel plan,torch.nn.Transformer,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/99432,2023-04-18T17:39:17Z,[DTensor] parallelize_module failed with nn.Transformer and the PairwiseParallel plan,torch.distributed.tensor.parallel.PairwiseParallel,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/98668,2023-04-08T06:01:09Z,Cannot use `checkpoint_sequential` with `torch.compile`,torch.utils.checkpoint.checkpoint_sequential,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/98422,2023-04-05T16:34:29Z,[FX] Symbolic trace over `torch.Tensor.${fn}` APIs,torch.Tensor.flatten,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/98422,2023-04-05T16:34:29Z,[FX] Symbolic trace over `torch.Tensor.${fn}` APIs,torch.Tensor.flip,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/98136,2023-04-01T08:58:55Z,A Segment Fault can be triggered in torch._grid_sampler_2d_cpu_fallback,torch._grid_sampler_2d_cpu_fallback,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/97329,2023-03-22T08:33:52Z,"torch.ops.aten.pow(2.0, 3) return unexpected value with complex type",torch.ops.aten.pow,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/97286,2023-03-21T21:39:08Z,`jacrev` and `jacfwd` raise an error that `Sparse CSR tensors do not have strides`,torch.jacrev,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97286,2023-03-21T21:39:08Z,`jacrev` and `jacfwd` raise an error that `Sparse CSR tensors do not have strides`,torch.jacfwd,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97283,2023-03-21T20:52:00Z,`jacfwd` fails when computing the gradient for `channels_last` tensor,torch.func.jacfwd,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97088,2023-03-18T11:01:11Z,A Segment Fault can be triggered in torch.embedding,torch.embedding,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97087,2023-03-18T10:58:24Z,A Segment Fault can be triggered in torch.adjoint,torch.adjoint,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96779,2023-03-14T21:36:12Z,Segmentation fault (core dumped) during Torch finetuning (at random step),torch.enable_grad,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96693,2023-03-13T21:46:59Z,"torch.compile mode=""max-autotune"" precision appears to be lower",torch.compile,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/96277,2023-03-08T06:10:16Z,A Segment Fault can be triggered in torch.adaptive_max_pool1d with an edge case,torch.adaptive_max_pool1d,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96276,2023-03-08T06:06:12Z,A Segment Fault can be triggered in torch.geqrf with an edge case,torch.geqrf,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96275,2023-03-08T05:59:47Z,A Segment Fault can be triggered in torch.pinverse,torch.pinverse,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/113840,2023-11-16T04:06:21Z,torch.einsum is stuck in mp.Process,torch.einsum,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/113037,2023-11-06T17:40:49Z,"`TensorIteratorBase::is_scalar` return `false` for empty numpy tensors, but true for empty Torch ones",torch.div,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/130108,2024-07-04T08:44:11Z,[ONNX]: Fail to export onnx when GroupNorm input shape rank=2,torch.onnx.export,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/130108,2024-07-04T08:44:11Z,[ONNX]: Fail to export onnx when GroupNorm input shape rank=2,torch.nn.GroupNorm,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/129649,2024-06-27T10:44:18Z,Low GPU precision,torch.nn.functional.conv3d,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/127223,2024-05-27T11:06:08Z,[BUG] torch.linalg.lstsq returning wrong result.,torch.linalg.lstsq,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/125116,2024-04-28T06:08:54Z,Does torch.nn.Linear check the weights shape before assignment?,torch.nn.Linear,2.0.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/122020,2024-03-16T08:05:29Z,torch.jit.script(model) and torch.jit.trace(model) performance degradation.,torch.jit.script,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/122020,2024-03-16T08:05:29Z,torch.jit.script(model) and torch.jit.trace(model) performance degradation.,torch.jit.trace,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/118303,2024-01-25T17:13:39Z,Inconsistent outputs of `mish` and `log10` between eagermode and torch.jit.trace,torch.nn.functional.mish,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/118303,2024-01-25T17:13:39Z,Inconsistent outputs of `mish` and `log10` between eagermode and torch.jit.trace,torch.log10,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/118302,2024-01-25T17:04:07Z,Inconsistent outputs of `torch.adaptive_avg_pool1d` and `torch.threshold_` between eagermode and torch.jit.trace,torch.adaptive_avg_pool1d,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/118302,2024-01-25T17:04:07Z,Inconsistent outputs of `torch.adaptive_avg_pool1d` and `torch.threshold_` between eagermode and torch.jit.trace,torch.threshold_,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/118300,2024-01-25T16:56:56Z,Inconsistent outputs in `linalg_ldl_factor_ex` and `special_spheriacal_bessel_j0` between eagermode and torch.jit.trace,torch._C._linalg.linalg_ldl_factor_ex,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/118300,2024-01-25T16:56:56Z,Inconsistent outputs in `linalg_ldl_factor_ex` and `special_spheriacal_bessel_j0` between eagermode and torch.jit.trace,torch._C._special.special_spherical_bessel_j0,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/116707,2024-01-03T18:10:12Z,AttributeError: `Adam` object has no attribute `param_groups` with Python 3.11,torch.load,2.0.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/116633,2024-01-02T19:58:34Z,Sparse tensor element-wise multiplication bug when broadcast needed,torch.sparse_tensor.expand,2.0.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/116612,2024-01-02T15:52:40Z,Loaded torch.jit traced module treats dtypes incorrectly in autograd,torch.jit,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/116612,2024-01-02T15:52:40Z,Loaded torch.jit traced module treats dtypes incorrectly in autograd,torch.autograd.grad,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/116612,2024-01-02T15:52:40Z,Loaded torch.jit traced module treats dtypes incorrectly in autograd,torch.matmul,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/116555,2023-12-30T02:58:35Z,The performance of `torch.index_select` and regular indexing differs dramatically based on the size of the tensor it is indexing,torch.index_select,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/115733,2023-12-13T09:31:04Z,RuntimeError: thread_predicates_.find(tv_inp) != thread_predicates_.end(),torch._C._jit_pass_lower_thread_predicate,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/114112,2023-11-20T10:15:43Z,[JIT] - torch.script - 'Optional[Tensor]' object has no attribute or method 'size',torch.jit.script,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/114090,2023-11-20T02:26:16Z,Nested tensors fail on Conv2D,torch.nested.nested_tensor,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/114090,2023-11-20T02:26:16Z,Nested tensors fail on Conv2D,torch.nn.Conv2d,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/113600,2023-11-14T00:43:20Z,Segmentation fault in RPC worker when DataLoader has num_workers > 0,torch.distributed.rpc,2.0.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/112714,2023-11-02T09:43:06Z,log_softmax() on CPU and GPU has expected numerical error when used with low-precision bfloat16,torch.nn.functional.log_softmax,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/112015,2023-10-25T10:23:04Z,op scaled_dot_product_attention case different results,torch.scaled_dot_product_attention,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/110858,2023-10-09T10:03:41Z,Broadcasting matmul is much slower than corresponding einsum,torch.matmul,2.0.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/110858,2023-10-09T10:03:41Z,Broadcasting matmul is much slower than corresponding einsum,torch.einsum,2.0.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/110029,2023-09-25T20:23:34Z,Dataloader resetting with num_workers=1 and persistent_workers=True,torch.utils.data.get_worker_info,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/109453,2023-09-17T06:11:15Z,Inconsistent Behavior of `ConvTranspose2d` on CPU and CUDA,torch.nn.ConvTranspose2d,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/109191,2023-09-13T10:27:51Z,Gradients across different ranks are not synchronized when using DDP,torch.distributed.init_process_group,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/109191,2023-09-13T10:27:51Z,Gradients across different ranks are not synchronized when using DDP,torch.nn.parallel.DistributedDataParallel,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/108840,2023-09-08T05:00:14Z,Tensor Parallel doesn't work with torch.compile,torch.compile,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/108642,2023-09-06T09:45:04Z,torch.topk returned values and indices are reordered if sorted=False,torch.topk,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/108521,2023-09-04T15:54:12Z,"resutl of (torch.mm(a,b) does not match result of (a[:part,:], b)",torch.mm,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/108493,2023-09-03T12:39:25Z,RWKV + Adam exp_avg_sq will change from positive to negative after loss.backward(),torch.optim.Adam,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/108332,2023-08-31T07:48:19Z,RuntimeError: dims.value().size() == self->getMaybeRFactorDomain().size(),torch.jit.script,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/108226,2023-08-30T10:24:22Z,torch.nn.functional.pad() with value type bool,torch.nn.functional.pad,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/108210,2023-08-30T03:12:34Z,Using distributed RPC and DDP together triggers error.,torch.distributed.all_reduce,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/108210,2023-08-30T03:12:34Z,Using distributed RPC and DDP together triggers error.,torch.distributed.rpc.rpc_sync,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/108152,2023-08-29T14:43:21Z,`Tensor.uniform_` uses illegal argument name `from`,torch.Tensor.uniform_,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/107352,2023-08-17T03:02:53Z,dist.scatter is incompatible with transpose/permute operation,torch.dist.scatter,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/107125,2023-08-14T09:18:31Z,combining `vmap` with NN containing `MaxPool2d' leads to discrepancies in output,torch.vmap,2.0.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/107125,2023-08-14T09:18:31Z,combining `vmap` with NN containing `MaxPool2d' leads to discrepancies in output,torch.func.jacrev,2.0.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/107125,2023-08-14T09:18:31Z,combining `vmap` with NN containing `MaxPool2d' leads to discrepancies in output,torch.nn.MaxPool2d,2.0.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/107102,2023-08-13T18:00:25Z,Error when using sparse_coo tensor with optimizer,torch.sparse_coo,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/107102,2023-08-13T18:00:25Z,Error when using sparse_coo tensor with optimizer,torch.optim.Adam,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/107102,2023-08-13T18:00:25Z,Error when using sparse_coo tensor with optimizer,torch.autograd.backward,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/106780,2023-08-08T13:32:31Z,inf and nan are mapped to quant_min in torch.fake_quantize_per_tensor_affine,torch.fake_quantize_per_tensor_affine,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/106690,2023-08-07T04:34:47Z,DDP grads not synced when static_graph=True and module output is a dict subclass?,torch.distributed.DistributedDataParallel,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/106665,2023-08-05T12:53:17Z,Inconsistency between CPU and GPU for `Linear()` layer with input size 0,torch.nn.Linear,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/106630,2023-08-04T18:47:08Z,no_grad() changes output of TransformerDecoder module during evaluation,torch.no_grad,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/106362,2023-08-01T04:00:04Z,Calling ops.aten.embedding_bag() function got silent crash,torch.ops.aten.embedding_bag,2.0.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/106243,2023-07-29T01:03:48Z,OneCycleLR's state_dict includes a full reference to the optimizer,torch.optim.lr_scheduler.OneCycleLR,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/105916,2023-07-25T12:41:36Z,Missing coalesced flag from `torch.autograd.Function.backward`,torch.autograd.Function.backward,2.0.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/105840,2023-07-24T15:00:59Z,[FSDP] FSDP doesn't work (random accuracy performance) when using `param_init_fn` and `sync_module_states=True`,torch.nn.FullyShardedDataParallel,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/105768,2023-07-21T22:03:11Z,torch.compile uses more memory when using less parameters,torch.compile,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/105548,2023-07-19T17:23:18Z,torch.sparse.sampled_addmm doesn't compute gradients for 3D tensors,torch.sparse.sampled_addmm,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/104959,2023-07-11T12:33:15Z,Saving a LightningModule torch.jit.ScriptModule is incompatible with torch.amp.autocast,torch.amp.autocast,2.0.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/104959,2023-07-11T12:33:15Z,Saving a LightningModule torch.jit.ScriptModule is incompatible with torch.amp.autocast,torch.jit.script,2.0.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/104867,2023-07-10T12:19:05Z,rfftn and irfftn operations in pt2 return different results compared to v1.12.1,torch.fft.rfftn,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/104867,2023-07-10T12:19:05Z,rfftn and irfftn operations in pt2 return different results compared to v1.12.1,torch.fft.irfftn,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/104701,2023-07-06T12:17:30Z,System memory leak when using different input size of torch.nn.Conv3d,torch.nn.Conv3d,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/104297,2023-06-27T21:01:43Z,"torch.distributed.all_to_all_single & alltoall_base, size limit INT_MAX",torch.distributed.all_to_all_single,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/104297,2023-06-27T21:01:43Z,"torch.distributed.all_to_all_single & alltoall_base, size limit INT_MAX",torch.distributed.alltoall_base,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/104174,2023-06-26T07:44:44Z,distributed.scatter memory leak in source rank,torch.distributed.scatter,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/103589,2023-06-14T12:57:48Z,`interpolate` with `antialias=True` on CUDA doesn't work if the difference of spatial size is large,torch.nn.functional.interpolate,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/103397,2023-06-11T20:55:04Z,LayerNorm freeze processes using torch multiprocessing,torch.nn.LayerNorm,2.0.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/102948,2023-06-05T03:16:15Z,[onnx] aten::cumprod cannot be exported to ONNX,torch.aten::cumprod,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/102936,2023-06-04T16:17:09Z,torch.cuda.is_available() returns False on GTX 1650 with cuda 11.7 and torch==2.0.0+cpu,torch.cuda.is_available(),2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/102438,2023-05-27T15:25:05Z,DDP multi node multi gpu inconsistent params,torch.distributed,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/101850,2023-05-19T03:12:11Z,Unexpected modification to CPU affinity of Dataloader workers,torch.utils.data.DataLoader,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/101632,2023-05-16T23:58:51Z,torch.profiler.profile has an empty python replay stack under certain circumstances,torch.profiler.profile,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/101428,2023-05-15T17:57:33Z,2D inputs to linear layers run up to 25% slower than 4D ones on some Nvidia GPUs,torch.nn.Linear,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/101160,2023-05-11T06:33:41Z,Fine-tuning HuggingFace wav2vec 2.0 with `torch.compile`,torch.compile,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/110387,2023-10-02T13:34:23Z,Change unsqueeze(0) to preserve memory layout contiguity of original tensor,torch.unsqueeze,2.0.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/107820,2023-08-23T21:32:53Z,`add_image_with_boxes` method from `torch.utils.tensorboard.writer.SummaryWriter` is broken,torch.utils.tensorboard.writer.add_image_with_boxes,2.0.1,12.2
pytorch,https://github.com/pytorch/pytorch/issues/121719,2024-03-12T10:58:11Z,index_copy change value when use_deterministic (true),torch.index_copy,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/121719,2024-03-12T10:58:11Z,index_copy change value when use_deterministic (true),torch.index_put,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/116320,2023-12-22T09:16:23Z,RuntimeError: CUDA error: an illegal memory access was encountered using vmap and model ensembling call for cuda system,torch.autograd.Function,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/99821,2023-04-23T09:56:25Z,Tracer cannot infer type of Seq2SeqLMOutput,torch.jit.trace,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/132067,2024-07-29T17:50:39Z,Pickle loads torch tensor memory leak,torch.loads,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/131265,2024-07-19T23:28:50Z,[torch.compile][HF] torch.compile issue tracker for torch.compile on forward method of Meta Llama model,torch.compile,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/130078,2024-07-04T00:34:09Z,cuda graph error for torch.isin if the input tensors are too large,torch.isin,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/130066,2024-07-03T22:38:09Z,[dynamic-shapes][dynamo] Iist index out of range for constraint,torch.compile,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/130066,2024-07-03T22:38:09Z,[dynamic-shapes][dynamo] Iist index out of range for constraint,torch._dynamo,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/130066,2024-07-03T22:38:09Z,[dynamic-shapes][dynamo] Iist index out of range for constraint,torch.fx.experimental.symbolic_shapes,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/130066,2024-07-03T22:38:09Z,[dynamic-shapes][dynamo] Iist index out of range for constraint,torch._guards,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/130066,2024-07-03T22:38:09Z,[dynamic-shapes][dynamo] Iist index out of range for constraint,torch._dynamo.guards,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/130066,2024-07-03T22:38:09Z,[dynamic-shapes][dynamo] Iist index out of range for constraint,torch._dynamo.convert_frame,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/130066,2024-07-03T22:38:09Z,[dynamic-shapes][dynamo] Iist index out of range for constraint,torch._dynamo.symbolic_convert,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/130066,2024-07-03T22:38:09Z,[dynamic-shapes][dynamo] Iist index out of range for constraint,torch._dynamo.output_graph,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/130066,2024-07-03T22:38:09Z,[dynamic-shapes][dynamo] Iist index out of range for constraint,torch._dynamo.utils,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/129327,2024-06-23T05:42:35Z,[export/dynamo] torch._check fails at compile time when the condition evaluates to False,torch._check,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/128501,2024-06-12T10:56:18Z,torch.onnx.export with dynamic axes fails for torch.nn.InstanceNorm1d with track_running_stats=True,torch.onnx.export,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/128501,2024-06-12T10:56:18Z,torch.onnx.export with dynamic axes fails for torch.nn.InstanceNorm1d with track_running_stats=True,torch.nn.InstanceNorm1d,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/128494,2024-06-12T08:22:26Z,torch.onnx.export fails for torch.meshgrid with indexing='xy',torch.onnx.export,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/128494,2024-06-12T08:22:26Z,torch.onnx.export fails for torch.meshgrid with indexing='xy',torch.meshgrid,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/pull/128317,2024-06-10T02:30:22Z,"In gradient clipping, if DTensors are used, need to first convert them to local tensors",torch.nn.utils.clip_grad_norm_,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/126760,2024-05-21T07:24:37Z,"""torch.orgqr"" performs differently on cpu and gpu",torch.orgqr,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/126748,2024-05-21T03:11:29Z,"""torch.geqrf"" performs differently on cpu and gpu",torch.geqrf,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/126474,2024-05-16T23:11:07Z,[dynamo] Handle inplace op aliasing errors,torch.addcmul,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/124854,2024-04-24T16:32:09Z,[inductor] unexpected cuda:0 device usage when compiling and runing a model on cuda:1,torch.compile,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/124619,2024-04-22T15:06:36Z,torch.compile + dynamic shapes + tensor subclass graph output is broken,torch.compile,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/121793,2024-03-13T07:01:21Z,"When I use torch. compile(), the inference speed of the model becomes very slow, even dozens of times slower",torch.compile,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/113150,2023-11-07T10:44:24Z,"Aborted (core dumped) after Run Pytorch2.0.0, which I compiled by myself",torch::jit::createLlgaKernel,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/113150,2023-11-07T10:44:24Z,"Aborted (core dumped) after Run Pytorch2.0.0, which I compiled by myself",torch::jit::createLlgaGuardKernel,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/113150,2023-11-07T10:44:24Z,"Aborted (core dumped) after Run Pytorch2.0.0, which I compiled by myself",torch::cuda::shared::initCudartBindings,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/113150,2023-11-07T10:44:24Z,"Aborted (core dumped) after Run Pytorch2.0.0, which I compiled by myself",cudaProfilerInitialize,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/111508,2023-10-18T21:13:04Z,LBFGS accuracy difference between CPU and GPU,torch.optim.LBFGS,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/110505,2023-10-04T07:43:21Z,scaled_dot_product returns NaN arrays with eval(),torch.nn.functional.scaled_dot_product,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/108676,2023-09-06T17:45:45Z,RuntimeError when calling conv_transpose2d with groups,torch.nn.functional.conv_transpose2d,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/107081,2023-08-12T08:02:07Z,[FSDP] summon_full_params won't change parameters,torch.FSDP.summon_full_params,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/107026,2023-08-11T09:51:25Z,a bug about tensor stride,torch.rand,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/107026,2023-08-11T09:51:25Z,a bug about tensor stride,torch.transpose,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/107026,2023-08-11T09:51:25Z,a bug about tensor stride,torch.contiguous,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/107026,2023-08-11T09:51:25Z,a bug about tensor stride,torch.is_contiguous,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/106634,2023-08-04T21:04:36Z,Confusing error message for DataLoader with num_workers=0 and non-zero timeout,torch.utils.data.DataLoader,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/104857,2023-07-10T02:44:26Z,Torch's `LayerNorm` and Adam optimizer vs those in tensorflow,torch.LayerNorm,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/104857,2023-07-10T02:44:26Z,Torch's `LayerNorm` and Adam optimizer vs those in tensorflow,torch.optim.Adam,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/104620,2023-07-05T08:36:54Z,`torch.distributed.rpc.backend_registry.register_backend` fails to update `BackendType` enum,torch.distributed.rpc.backend_registry.register_backend,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/102953,2023-06-05T06:17:13Z,TypeError: (): incompatible function arguments,torch.distributed.init_process_group,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/102832,2023-06-02T09:02:01Z,TypeError: (): incompatible function arguments,torch.distributed.init_process_group,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/101385,2023-05-15T06:28:47Z,torch.Tensor.is_sparse returns false for non-COO sparse tensors,torch.Tensor.is_sparse,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/101241,2023-05-12T01:28:50Z,Different results with vmap when using torch.jit.script,torch.jit.script,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/101241,2023-05-12T01:28:50Z,Different results with vmap when using torch.jit.script,torch.jit.fork,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/101241,2023-05-12T01:28:50Z,Different results with vmap when using torch.jit.script,torch.func.vmap,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/101241,2023-05-12T01:28:50Z,Different results with vmap when using torch.jit.script,torch.autograd.grad,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/100932,2023-05-09T01:26:39Z,torch.concat fails with float16 input in autocast(device_type=cpu) context,torch.concat,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/100784,2023-05-06T02:54:35Z,[CUDA RPC] Incorrect results of GPU Tensor transferring using RPC when parallelized with other GPU programs,torch.distributed.rpc,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/100582,2023-05-03T21:29:21Z,"Synchronization issue when combining DPP and RPC - ""Parameter marked twice""",torch.distributed.rpc,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/100582,2023-05-03T21:29:21Z,"Synchronization issue when combining DPP and RPC - ""Parameter marked twice""",torch.nn.parallel.DistributedDataParallel,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/100582,2023-05-03T21:29:21Z,"Synchronization issue when combining DPP and RPC - ""Parameter marked twice""",torch.distributed,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/100158,2023-04-27T09:27:21Z,`torch.sparse_csc_tensor` matrix multiplication produces MKL error SPARSE_STATUS_ALLOC_FAILED when density is too high,torch.sparse_csc_tensor,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/100087,2023-04-26T12:44:11Z,TransformerEncoderLayer behavior inconsistent between training and evaluation mode,torch.nn.TransformerEncoderLayer,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/100085,2023-04-26T12:40:00Z,[regression] torch.norm with out dtype bfloat16 cause runtime error,torch.norm,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/99932,2023-04-24T22:17:50Z,FSDP + gradient clipping raises an odd warning with the simplest model on torch 2.0,torch.distributed.fsdp.FullyShardedDataParallel,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/99932,2023-04-24T22:17:50Z,FSDP + gradient clipping raises an odd warning with the simplest model on torch 2.0,torch.distributed.fsdp.api.ShardingStrategy,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/99556,2023-04-19T19:09:46Z,torch.func.jacrev fails if model contains full_backward_hook,torch.func.jacrev,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/99410,2023-04-18T11:41:22Z,"torch.nn.functional.multilabel_margin_loss cuda lacks checking of ""out of bound""",torch.nn.functional.multilabel_margin_loss,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/99410,2023-04-18T11:41:22Z,"torch.nn.functional.multilabel_margin_loss cuda lacks checking of ""out of bound""",torch.nn.MultiLabelMarginLoss,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/99265,2023-04-16T13:44:32Z,the error message of torch.addcmul is wrong,torch.addcmul,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/99149,2023-04-14T13:59:29Z,"Spectral Normalization can not be applied to Conv{1,2,3}d",torch.nn.utils.parametrizations.spectral_norm,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/99012,2023-04-13T03:40:26Z,"[BUG]Float32 attention mask not working with torch.autocast(""cpu"")",torch.autocast,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/99012,2023-04-13T03:40:26Z,"[BUG]Float32 attention mask not working with torch.autocast(""cpu"")",torch.nn.MultiheadAttention,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/98948,2023-04-12T16:53:22Z,behaviour of `torch.tensor()` changes after editing `Tensor.__getitem__`,torch.tensor,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/98939,2023-04-12T15:19:18Z,"torch.dist with minus norm returns tensor(0.), while with -inf can return result",torch.dist,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/98929,2023-04-12T09:54:46Z,torch.sparse_csr_tensor() stops gradients,torch.sparse_csr_tensor(),2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/98861,2023-04-11T19:00:20Z,Sparse Tensor not working for `torch.cat`,torch.cat,2.0.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/98675,2023-04-08T16:29:08Z,torch.matmul with batched CSR matrix,torch.matmul,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/98557,2023-04-06T23:43:55Z,torch.jit.script codegen warning with cuda and vmap,torch.jit.script,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/98557,2023-04-06T23:43:55Z,torch.jit.script codegen warning with cuda and vmap,torch.vmap,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/98498,2023-04-06T12:36:57Z,"Higher order derivatives not working when setting compute device to `torch.device(""mps"")`",torch.func.functional_call,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/98142,2023-04-01T13:35:10Z,double free or corruption (fasttop),torch.normal,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/98142,2023-04-01T13:35:10Z,double free or corruption (fasttop),torch.rand,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/98142,2023-04-01T13:35:10Z,double free or corruption (fasttop),torch.to,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/98142,2023-04-01T13:35:10Z,double free or corruption (fasttop),torch::kFloat32,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/98142,2023-04-01T13:35:10Z,double free or corruption (fasttop),torch::kCPU,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/98064,2023-03-31T07:13:17Z,Module 'Sequential' has no attribute '_modules' :,torch.jit.script,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97913,2023-03-29T21:40:31Z,BUG torch.jit.annotate on List + torch.stack give wrong DTYPE,torch.jit.annotate,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97913,2023-03-29T21:40:31Z,BUG torch.jit.annotate on List + torch.stack give wrong DTYPE,torch.stack,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97872,2023-03-29T09:47:35Z,nn.linear not support bfloat16,torch.nn.Linear,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/97823,2023-03-28T20:39:52Z,Using `param in param_list` can trigger `non-singleton dimension` error?,torch.nn.Parameter,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97784,2023-03-28T15:15:01Z,torch.compile fails with torch._dynamo.exc.TorchRuntimeError on a function that contains a torch script module,torch.compile,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/97784,2023-03-28T15:15:01Z,torch.compile fails with torch._dynamo.exc.TorchRuntimeError on a function that contains a torch script module,torch._dynamo.exc.TorchRuntimeError,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/97784,2023-03-28T15:15:01Z,torch.compile fails with torch._dynamo.exc.TorchRuntimeError on a function that contains a torch script module,torch.jit.trace,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/97668,2023-03-27T13:54:17Z,sparse_csr_tensor matmul wrong output in bfloat16,torch.sparse_csr_tensor,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97189,2023-03-20T21:57:52Z,torch.randint range for torch.int64 dtype seems wrong,torch.randint,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97128,2023-03-20T06:01:36Z,"MultiHeadAttention, fast path broken with `bias=False` or uneven number of heads",torch.nn.MultiheadAttention,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/97097,2023-03-18T20:42:07Z,torch.cuda.FloatTensor().normal_() generate (partially) different sample on different gpu machines,torch.cuda.FloatTensor().normal_(),2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96789,2023-03-15T00:30:22Z,`cumprod` triggers INTERNAL ASSERT FAILED when `out` is a tensor on cuda but input is on cpu,torch.cumprod,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96769,2023-03-14T20:15:43Z,`sparse.mm` triggers INTERNAL ASSERT FAILED when backwarding,torch.sparse.mm,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96704,2023-03-14T00:31:19Z,`logical_xx` operations trigger INTERNAL ASSERT FAIL when `input` is complex tensor on cuda and `other` is on cpu,torch.logical_and,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96704,2023-03-14T00:31:19Z,`logical_xx` operations trigger INTERNAL ASSERT FAIL when `input` is complex tensor on cuda and `other` is on cpu,torch.logical_or,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96704,2023-03-14T00:31:19Z,`logical_xx` operations trigger INTERNAL ASSERT FAIL when `input` is complex tensor on cuda and `other` is on cpu,torch.logical_xor,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96316,2023-03-08T18:29:32Z,`FractionalMaxPool3d` INTERNAL ASSERT FAILED when computing `jacrev`,torch.nn.FractionalMaxPool3d,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96316,2023-03-08T18:29:32Z,`FractionalMaxPool3d` INTERNAL ASSERT FAILED when computing `jacrev`,torch.func.jacrev,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/95309,2023-02-22T19:34:45Z,`torch.distributed.Store` triggers INTERNAL ASSER FAILED when seting,torch.distributed.Store,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94990,2023-02-16T17:28:50Z,interactions between views + autograd.Function + AOTAutograd causes memory leak,torch.autograd.Function,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/94990,2023-02-16T17:28:50Z,interactions between views + autograd.Function + AOTAutograd causes memory leak,torch.ops.test.foo_fwd,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/94990,2023-02-16T17:28:50Z,interactions between views + autograd.Function + AOTAutograd causes memory leak,torch.ops.test.foo_bwd,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/94990,2023-02-16T17:28:50Z,interactions between views + autograd.Function + AOTAutograd causes memory leak,torch.compile,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/94990,2023-02-16T17:28:50Z,interactions between views + autograd.Function + AOTAutograd causes memory leak,torch._dynamo.graph_break,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/94966,2023-02-16T05:14:28Z,CSR matrix add_ error with RuntimeError: CUDA error: kernel launch failure when calling cusparseXcsrgeam2Nnz,torch.optim.sgd._single_tensor_sgd,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94966,2023-02-16T05:14:28Z,CSR matrix add_ error with RuntimeError: CUDA error: kernel launch failure when calling cusparseXcsrgeam2Nnz,torch.sparse_coo_tensor.add_,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94855,2023-02-14T21:47:26Z,torch.compile breaks reproducibility,torch.compile,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94808,2023-02-14T09:09:10Z,Memory Corruption in torch.lstm caused by edge cases,torch.lstm,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/94773,2023-02-13T22:24:56Z,`torch.compile` doesn't consider the alias tensor created by `tensor[:]`,torch.compile,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94704,2023-02-12T16:32:30Z,`where` triggers INTERNAL ASSERT FAILED when `out` is a long tensor due to mixed types,torch.where,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94698,2023-02-12T06:25:54Z,A segment fault can be triggered in torch.avg_pool1d,torch.avg_pool1d,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/94696,2023-02-12T06:10:39Z,A segment fault can be triggered in torch.max_pool1d_with_indices,torch.max_pool1d_with_indices,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/94669,2023-02-11T08:51:26Z,A segment fault can be triggered in torch.svd,torch.svd,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/94668,2023-02-11T08:48:00Z,A segment fault can be triggered in torch.lstm with edge cases,torch.lstm,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/94594,2023-02-10T13:50:03Z,A segment fault can be triggered in torch.histogramdd,torch.histogramdd,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/94593,2023-02-10T13:46:52Z,Memory corruptions can be triggered in torch._remove_batch_dim,torch._remove_batch_dim,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/94591,2023-02-10T13:37:37Z,Issue with `upsample_nearest2d` decomposition,torch._C._nn.upsample_nearest2d,2.0.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/94590,2023-02-10T13:28:31Z,A Segment Fault can be triggered in torch.affine_grid_generator,torch.affine_grid_generator,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/94451,2023-02-08T23:05:47Z,"`jacrev` raise ""Cannot access storage of TensorWrapper"" error when computing the grad of `storage`",torch.func.jacrev,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94443,2023-02-08T22:12:06Z,A better error msg for `cuda.jiterator` when input is on `cpu`,torch.cuda.jiterator,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94336,2023-02-07T20:49:15Z,`scatter` fails the gradient computation in reverse mode for `src` when `index` is empty,torch.scatter,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94333,2023-02-07T20:29:00Z,cpu log1p for bfloat16 gives wrong result.,torch.log1p,2.0.0,12
pytorch,https://github.com/pytorch/pytorch/issues/94208,2023-02-06T19:30:11Z,`zeros_like` + `fill_` makes the gradient computation in forward mode fail,torch.zeros_like,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94115,2023-02-04T01:58:14Z,`cat` fails the gradient computation in forward mode with empty tensors when used with legacy vmap,torch.cat,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94111,2023-02-04T01:32:54Z,`svd` triggers INTERNAL ASSERT FAILED when computing jacobian in forward mode,torch.svd,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94086,2023-02-03T23:06:55Z,`MSELoss` fails to compute the gradients when inputs have different dtype,torch.nn.MSELoss,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94085,2023-02-03T22:57:22Z,`unfold` fails in forward mode when unfolding a scalar tensor,torch.unfold,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/93982,2023-02-02T19:44:24Z,`linalg.lstsq` fails the gradient computation in forward mode,torch.linalg.lstsq,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/92910,2023-01-24T18:24:24Z,torch.jit.save() generates different contents in a file among different endian machines,torch.jit.save,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/92910,2023-01-24T18:24:24Z,torch.jit.save() generates different contents in a file among different endian machines,torch.jit.load,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/92804,2023-01-23T06:27:18Z,Cuda streams and torch.compile,torch._dynamo.optimize,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/92742,2023-01-21T03:55:09Z,[JIT] Consecutive use of `addmm` Leads to Exception,torch.addmm,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/92740,2023-01-21T03:41:21Z,[JIT] Applying `conv2d` over Constants Leads to Exception,torch.nn.functional.relu,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/92740,2023-01-21T03:41:21Z,[JIT] Applying `conv2d` over Constants Leads to Exception,torch.nn.Conv2d,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/92740,2023-01-21T03:41:21Z,[JIT] Applying `conv2d` over Constants Leads to Exception,aten::conv2d,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/92674,2023-01-20T08:15:59Z,[JIT] `Linear` + `BatchNorm2d` Trigger Inconsistency between Eager Mode and JIT,torch.jit.optimize_for_inference,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/92674,2023-01-20T08:15:59Z,[JIT] `Linear` + `BatchNorm2d` Trigger Inconsistency between Eager Mode and JIT,torch.nn.Linear,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/92674,2023-01-20T08:15:59Z,[JIT] `Linear` + `BatchNorm2d` Trigger Inconsistency between Eager Mode and JIT,torch.nn.BatchNorm2d,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/92594,2023-01-18T23:25:40Z,INTERNAL ASSERT FAILED when mixed dtypes for `addcmul_`,torch.addcmul_,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/91863,2023-01-08T23:31:55Z,torch.Categorical samples indexes with 0 probability when given logits as argument,torch.distributions.Categorical,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/98210,2023-04-03T13:27:17Z,torch.jit.script + legacy executor mode has diff in some pattern,torch.jit.script,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/100850,2023-05-08T05:08:12Z,[BUG] Poor torch.bmm performance on H100,torch.bmm,1.14.0,12
pytorch,https://github.com/pytorch/pytorch/issues/100528,2023-05-03T06:13:43Z,Backward hook execution order changes when input.requires_grad is False,torch.nn.modules.module.register_module_full_backward_pre_hook,1.14.0,12
pytorch,https://github.com/pytorch/pytorch/issues/100528,2023-05-03T06:13:43Z,Backward hook execution order changes when input.requires_grad is False,torch.nn.modules.module.register_module_full_backward_hook,1.14.0,12
pytorch,https://github.com/pytorch/pytorch/issues/92563,2023-01-18T18:46:56Z,[JIT] INTERNAL ASSERT FAILED when `Conv2d` and `clamp` used together,torch.nn.Conv2d,1.14.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/92563,2023-01-18T18:46:56Z,[JIT] INTERNAL ASSERT FAILED when `Conv2d` and `clamp` used together,torch.clamp,1.14.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/92563,2023-01-18T18:46:56Z,[JIT] INTERNAL ASSERT FAILED when `Conv2d` and `clamp` used together,torch.jit.optimize_for_inference,1.14.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/91396,2022-12-27T03:42:21Z,[JIT] Zero-channel conv2d cannot be applied with `optimize_for_inference`,torch.jit.optimize_for_inference,1.14.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/90367,2022-12-07T08:23:55Z,[JIT] INTERNAL ASSERT FAILED `torch.add` with boolean primitive constant,torch.add,1.14.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/90366,2022-12-07T08:16:55Z,[JIT] INTERNAL ASSERT FAILED `torch.mul` with boolean primitive constant,torch.mul,1.14.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/90365,2022-12-07T07:57:54Z,[JIT] INTERNAL ASSERT FAILED when dispatching for `torch.Tensor.view`,torch.Tensor.view,1.14.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/89718,2022-11-27T09:54:16Z,`torch.Tensor.flatten` Trigger Segmentation Fault when trying to provide and output named dim,torch.Tensor.flatten,1.14.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/89630,2022-11-24T14:19:49Z,[dynamo] RuntimeError: Failed running call_function aten.nll_loss_backward(*(FakeTensor(FakeTensor(...,torch.ops.aten.nll_loss_backward,1.14.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/89629,2022-11-24T14:15:32Z,[dynamo] RuntimeError: Failed running call_function aten.convolution_backward(*(FakeTensor(FakeTensor(..,torch.ops.aten.convolution_backward,1.14.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/89254,2022-11-18T01:00:59Z,"`torch.nn.ReplicationPad2D` Report ""invalid configuration argument"" Error under Compute Sanitizer",torch.nn.ReplicationPad2d,1.14.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/89218,2022-11-17T16:57:43Z,"`torch.nn.LayerNorm` Abort with ""invalid device ordinal"" Error",torch.nn.LayerNorm,1.14.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/88950,2022-11-12T16:54:50Z,`torch.nn.functional.embedding_bag` Trigger RuntimeError under UndefinedBehaviorSanitizer,torch.nn.functional.embedding_bag,1.14.0,11.8.89
pytorch,https://github.com/pytorch/pytorch/issues/88949,2022-11-12T16:51:50Z,`torch.set_rng_state` Trigger RuntimeError under UndefinedBehaviorSanitizer,torch.set_rng_state,1.14.0,11.8.89
pytorch,https://github.com/pytorch/pytorch/issues/88947,2022-11-12T16:48:12Z,`torch.Tensor.msort` Trigger RuntimeError under UndefinedBehaviorSanitizer,torch.Tensor.msort,1.14.0,11.8.89
pytorch,https://github.com/pytorch/pytorch/issues/88945,2022-11-12T16:45:29Z,`torch.linalg.eigvals` Trigger RuntimeError under UndefinedBehaviorSanitizer,torch.linalg.eigvals,1.14.0,11.3.1
pytorch,https://github.com/pytorch/pytorch/issues/88944,2022-11-12T16:42:23Z,`torch.topk` Trigger RuntimError under UndefinedBehaviorSanitizer,torch.topk,1.14.0,11.8.89
pytorch,https://github.com/pytorch/pytorch/issues/88943,2022-11-12T16:39:18Z,`torch.vander` Trigger RuntimeError with UndefinedBehaviorSanitizer,torch.vander,1.14.0,11.8.89
pytorch,https://github.com/pytorch/pytorch/issues/88942,2022-11-12T16:34:02Z,`torch.svd_lowrank` Trigger RuntimeError under UndefinedBehaviorSanitizer,torch.svd_lowrank,1.14.0,11.8.89
pytorch,https://github.com/pytorch/pytorch/issues/88941,2022-11-12T16:30:33Z,`torch.linalg.lstsq` Trigger RuntimeError under UndefinedBehaviorSanitizer,torch.linalg.lstsq,1.14.0,11.3.1
pytorch,https://github.com/pytorch/pytorch/issues/88591,2022-11-07T18:05:08Z,`pack_padded_sequence` not compatible with deterministic mode it calls `torch.scatter`,torch.pack_padded_sequence,1.14.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/88591,2022-11-07T18:05:08Z,`pack_padded_sequence` not compatible with deterministic mode it calls `torch.scatter`,torch.scatter,1.14.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/88563,2022-11-07T04:20:47Z,`nn.functional.embedding_bag` Trigger out-of-bound Read under Compute Sanitizer,torch.nn.functional.embedding_bag,1.14.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/88185,2022-11-01T09:27:57Z,`torch.mm` Trigger RuntimeError with UndefinedBehaviorSanitizer,torch.mm,1.14.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/87841,2022-10-27T04:11:29Z,`max_unpool3d` will trigger an assertion fail under compute sanitizer,torch.nn.functional.max_unpool3d,1.14.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/87792,2022-10-26T16:14:52Z,`AvgPool` and `MaxPool` will crash in JIT w/o profiling executor,torch.nn.AdaptiveMaxPool3d,1.14.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/87792,2022-10-26T16:14:52Z,`AvgPool` and `MaxPool` will crash in JIT w/o profiling executor,torch.nn.AdaptiveAvgPool3d,1.14.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/87787,2022-10-26T15:45:52Z,`BatchNorm` a 0-shape tensor will crash in JIT trace w/o profiling executor on cuda,torch.nn.BatchNorm1d,1.14.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/87390,2022-10-20T19:08:02Z,`chunk` a 0-dim tensor will crash in JIT script w/o profiling executor,torch.chunk,1.14.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/95432,2023-02-24T01:47:35Z,Regression bug in `torch.nn.ReLU6` and `torch.nn.Hardtanh` that `inplace=True` doesn't work in PyTorch 1.10.0~1.13.1,torch.nn.ReLU6,1.13.1,11.1
pytorch,https://github.com/pytorch/pytorch/issues/95432,2023-02-24T01:47:35Z,Regression bug in `torch.nn.ReLU6` and `torch.nn.Hardtanh` that `inplace=True` doesn't work in PyTorch 1.10.0~1.13.1,torch.nn.Hardtanh,1.13.1,11.1
pytorch,https://github.com/pytorch/pytorch/issues/131635,2024-07-24T06:43:46Z,torch.onnx.errors.UnsupportedOperatorError,torch.onnx.errors.UnsupportedOperatorError,1.13.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/131635,2024-07-24T06:43:46Z,torch.onnx.errors.UnsupportedOperatorError,torch.aten.randint,1.13.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/122711,2024-03-26T18:08:22Z,`torch.sparse.sum` does not support boolean and int when summing over dense dimensions,torch.sparse.sum,1.13.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/121960,2024-03-15T08:47:00Z,"RuntimeError: ""max_pool2d_channels_last"" not implemented for 'Half'",torch.max_pool2d,1.13.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/117960,2024-01-22T03:30:46Z,"when load data with Distributed Train, when the data is large, it will be raised errors",torch.distributed.elastic.multiprocessing.api,1.13.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117960,2024-01-22T03:30:46Z,"when load data with Distributed Train, when the data is large, it will be raised errors",torch.distributed.launch,1.13.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117960,2024-01-22T03:30:46Z,"when load data with Distributed Train, when the data is large, it will be raised errors",torch.distributed.run,1.13.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117960,2024-01-22T03:30:46Z,"when load data with Distributed Train, when the data is large, it will be raised errors",torch.distributed.launcher.api,1.13.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117920,2024-01-20T08:59:25Z,Could not run 'quantized::embedding_byte' with arguments from the 'QuantizedCPU' backend.,torch.quantization.embedding_byte,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/110363,2023-10-01T19:28:47Z,"nn.BatchNorm2d (track_running_stats = True) causes ""modified by an in-place operation"" error when in torch.nn.parallel.DistributedDataParallel",torch.nn.BatchNorm2d,1.13.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/109193,2023-09-13T11:10:21Z,"F.conv2d(input, weight, bias, self.stride, RuntimeError: cuDNN error: CUDNN_STATUS_MAPPING_ERROR",torch.nn.Conv2d,1.13.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/107556,2023-08-20T17:56:12Z,Integer multiplication overflow when running torch.nn.AdaptiveAvgPool2d,torch.nn.AdaptiveAvgPool2d,1.13.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/107555,2023-08-20T17:54:59Z,Integer multiplication overflow when running torch.nn.MaxUnpool3d,torch.nn.MaxUnpool3d,1.13.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/107554,2023-08-20T17:53:26Z,Integer multiplication overflow when running torch.diagflat,torch.diagflat,1.13.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/107552,2023-08-20T17:37:18Z,Storage size calculation overflowed when running torch.nn.functional.interpolate,torch.nn.functional.interpolate,1.13.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/107550,2023-08-20T17:32:11Z,Integer multiplication overflow when running torch.eye,torch.eye,1.13.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/107548,2023-08-20T16:09:15Z,Integer calculation overflow when running torch.nn.functional.adaptive_avg_pool2d,torch.nn.functional.adaptive_avg_pool2d,1.13.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/107546,2023-08-20T13:15:52Z,Integer overflow when running torch.nn.functional.upsample_bilinear,torch.nn.functional.upsample_bilinear,1.13.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/107545,2023-08-20T13:12:49Z,Integer overflow when running torch.nn.functional.upsample,torch.nn.functional.upsample,1.13.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/107544,2023-08-20T13:03:07Z,Integer overflow when running torch.nn.ReplicationPad3d,torch.nn.ReplicationPad3d,1.13.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/107543,2023-08-20T12:24:46Z,Integer overflow when running torch.nn.AdaptiveAvgPool2d,torch.nn.AdaptiveAvgPool2d,1.13.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/107541,2023-08-20T12:04:34Z,Integer overflow when running torch.nn.MaxUnpool2d,torch.nn.MaxUnpool2d,1.13.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/107540,2023-08-20T12:01:24Z,Index out of bound when running torch.gather,torch.gather,1.13.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/107539,2023-08-20T11:56:07Z,Integer overflow when running torch.nn.functional.max_unpool2d,torch.nn.functional.max_unpool2d,1.13.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/106469,2023-08-02T17:48:04Z,Extreme slowdown of torch.mm for certain sizes and strides with bfloat16,torch.mm,1.13.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/106467,2023-08-02T16:24:15Z,nn.CrossEntropyLoss with invalid target generates corrups memory eventualy leading to CUDA error: an illegal memory access,torch.nn.CrossEntropyLoss,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/103243,2023-06-08T09:23:35Z,torch.cuda.memory_reserved always returns 0 bytes,torch.cuda.memory_reserved,1.13.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/102261,2023-05-25T07:34:07Z,ExponentialLR unexpectedly calls `step()` when init argument `last_epoch` is larger than -1,torch.optim.lr_scheduler.ExponentialLR,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/101075,2023-05-10T14:18:05Z,torch.lobpcg producing different largest eigenvalue than scipy and np.linalg.eig,torch.lobpcg,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/98566,2023-04-07T02:30:08Z,`F.interpolate` and `F.grid_sample` - documentation error and bug,torch.nn.functional.interpolate,1.13.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/98566,2023-04-07T02:30:08Z,`F.interpolate` and `F.grid_sample` - documentation error and bug,torch.nn.functional.grid_sample,1.13.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/98499,2023-04-06T13:03:23Z,`torch.nn.utils.rnn.unpad_sequence` modifies arguments in-place,torch.nn.utils.rnn.unpad_sequence,1.13.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/98075,2023-03-31T10:36:41Z,Unexpected results with torch.nn.functional.layer_norm,torch.nn.functional.layer_norm,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/98004,2023-03-30T19:15:37Z,matmul with CSR matrix in inference mode throws an exception,torch.matmul,1.13.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97432,2023-03-23T09:16:05Z,suspicious memory leak when increase DataLoader's prefetch_factor and enable pin_memory,torch.cudaHostAlloc,1.13.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97111,2023-03-19T16:18:44Z,TransformerEncoder truncates output when some token positions are masked by `src_key_padding_mask` across batch,torch.nn.TransformerEncoder,1.13.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/95776,2023-03-01T12:32:33Z,`torch.Tensor.is_set_to` raises `NotImplementedError` when inputs contain sparse tensor,torch.Tensor.is_set_to,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/95768,2023-03-01T07:23:52Z,Inconsistent behaviour of torch.all(),torch.all,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/95756,2023-03-01T02:51:34Z,`torch.nanmedian` return a negative value when input is empty,torch.nanmedian,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/95756,2023-03-01T02:51:34Z,`torch.nanmedian` return a negative value when input is empty,torch.median,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/95718,2023-02-28T18:13:05Z,functorch.compile.memory_efficient_fusion errors with: RuntimeError: forward() Expected a value of type 'Tensor (inferred)' for argument 'primals_356' but instead found type 'int'.,torch.ops.aten.add,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/95718,2023-02-28T18:13:05Z,functorch.compile.memory_efficient_fusion errors with: RuntimeError: forward() Expected a value of type 'Tensor (inferred)' for argument 'primals_356' but instead found type 'int'.,torch.ops.aten.view,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/95548,2023-02-25T07:11:02Z,"torch.distributions.kumaraswamy.Kumaraswamy generates samples outside its support (0,1)",torch.distributions.kumaraswamy.Kumaraswamy,1.13.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94542,2023-02-09T21:02:31Z,torch.lgamma CUDA driver error,torch.lgamma,1.13.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94511,2023-02-09T14:35:11Z,"Performance does not meet expectations when training OPT-30 with FSDP, there may be problems with cpu offloading",torch.distributed.fsdp.FullyShardedDataParallel,1.13.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/94511,2023-02-09T14:35:11Z,"Performance does not meet expectations when training OPT-30 with FSDP, there may be problems with cpu offloading",torch.distributed.fsdp.CPUOffload,1.13.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/94428,2023-02-08T20:00:39Z,nn.TransformerEncoderLayer fastpath (BetterTransformer) is much slower with src_key_padding_mask,torch.nn.TransformerEncoder,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/94428,2023-02-08T20:00:39Z,nn.TransformerEncoderLayer fastpath (BetterTransformer) is much slower with src_key_padding_mask,torch.nn.TransformerEncoderLayer,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/94388,2023-02-08T10:04:50Z,Inconsistent results when using torch.Tensor.bernoulli with float instead of Tensor probabilities,torch.Tensor.bernoulli,1.13.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94125,2023-02-04T04:45:08Z,A Floating Point Exception can be trigerred in torch._C._nn.slow_conv3d,torch._C._nn.slow_conv3d,1.13.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/93826,2023-02-01T06:51:35Z,torch.jit.script does not work with DataParallel,torch.jit.script,1.13.1,12
pytorch,https://github.com/pytorch/pytorch/issues/93826,2023-02-01T06:51:35Z,torch.jit.script does not work with DataParallel,torch.nn.parallel.data_parallel,1.13.1,12
pytorch,https://github.com/pytorch/pytorch/issues/92835,2023-01-23T18:19:41Z,Double free when running torch.linalg.ldl_solve,torch.linalg.ldl_solve,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/92818,2023-01-23T16:33:34Z,Segfault when running torch.atan2,torch.atan2,1.13.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/92812,2023-01-23T12:35:23Z,"torch.fx fails to trace through ""+"" op between torch.Size and torch.fx.proxy.Proxy",torch.fx.symbolic_trace,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/92812,2023-01-23T12:35:23Z,"torch.fx fails to trace through ""+"" op between torch.Size and torch.fx.proxy.Proxy",torch.fx.proxy.Proxy,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/92812,2023-01-23T12:35:23Z,"torch.fx fails to trace through ""+"" op between torch.Size and torch.fx.proxy.Proxy",torch.Size,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/92783,2023-01-23T00:57:32Z,Process get killed when running torch.combinations,torch.combinations,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/92781,2023-01-23T00:31:23Z,Floating point exception when running torch.nn.AdaptiveMaxPool3d,torch.nn.AdaptiveMaxPool3d,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/92778,2023-01-22T23:41:36Z,Process get killed when running torch.normal,torch.normal,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/92776,2023-01-22T23:33:45Z,segfault when running torch.lu_unpack,torch.lu_unpack,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/92252,2023-01-16T15:01:17Z,"`model.to(""cuda:0"")` does not release all CPU memory",torch.to,1.13.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/92251,2023-01-16T14:52:21Z,"`torch.load(..., map_location=""cuda:0"")` allocates memory on both CPU and GPU",torch.load,1.13.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/92250,2023-01-16T14:27:19Z,torch.cuda.is_available() returns True even if the CUDA hardware can't run pytorch,torch.cuda.is_available,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/92171,2023-01-13T19:50:19Z,PyTorch 1.13.1 hangs with `torch.distributed.init_process_group`,torch.distributed.init_process_group,1.13.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/91633,2023-01-03T15:47:18Z,Segmentation fault when running torch.nn.functional.fractional_max_pool3d on torch 1.13.1,torch.nn.functional.fractional_max_pool3d,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/91245,2022-12-21T13:37:52Z,Segmentation faults in DataLoader (in latest torch version).,torch.utils.data.DataLoader,1.13.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/118723,2024-01-31T03:50:23Z,"Libtorch torch::load(tensor, ""tensor.pt""); error",torch::load,1.13.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/90752,2022-12-13T08:32:36Z,Adam (fused=True) issues,torch.optim.Adam,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/pull/130929,2024-07-17T13:47:28Z,Fix non-torchscriptable `torch.nn.MultiheadAttention` with `bias=False`,torch.nn.MultiheadAttention,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/130610,2024-07-12T07:06:07Z,dataloader hangs with fork and pin memory=True,torch.Generator,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/130610,2024-07-12T07:06:07Z,dataloader hangs with fork and pin memory=True,torch.utils.data.dataloader,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/130610,2024-07-12T07:06:07Z,dataloader hangs with fork and pin memory=True,torch.utils.data._utils.pin_memory,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/130610,2024-07-12T07:06:07Z,dataloader hangs with fork and pin memory=True,torch.autograd.THPPVariable_pin_memory,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/130610,2024-07-12T07:06:07Z,dataloader hangs with fork and pin memory=True,torch.autograd.THPPVariable_set_,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/130610,2024-07-12T07:06:07Z,dataloader hangs with fork and pin memory=True,at::set_num_threads,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/130000,2024-07-03T00:04:55Z,LayoutLMForMaskedLM generates NaN in Max-autotune mode with cuda-graph enabled,torch.compile,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/126691,2024-05-20T16:11:00Z,TORCH_CHECK used within torch.compile does not throw legible errors,torch.compile,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/126034,2024-05-12T23:31:14Z,A scalar with `tensor` argument of `result_type()` doesn't work,torch.result_type,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/125493,2024-05-03T19:49:52Z,Error calling to_sparse_coo() on any subclass of Tensor,torch.to_sparse_coo,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/124275,2024-04-17T09:31:05Z,Performance Impact of `torch.autograd.profiler.record_function` in DataLoader's `_BaseDataLoaderIter` Iteration,torch.autograd.profiler.record_function,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/122552,2024-03-23T10:44:04Z,Could not run 'aten::empty_strided' with arguments from the 'CUDA' backend.,torch.aten.empty_strided,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/121208,2024-03-05T07:11:45Z,Exception: Could not run 'aten::channel_shuffle' with arguments from the 'CUDA' backend.,torch.aten::channel_shuffle,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/pull/121105,2024-03-04T00:52:22Z,Include the bound check and test for slow_conv3d,torch._C._nn.slow_conv3d,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/120814,2024-02-28T19:24:18Z,torch.compile() and optimizer UX issues,torch.compile(),1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/119758,2024-02-13T06:13:24Z,Flight Recorder dump_entries() segfaults when used with coalesced operations,torch.batched_isend_irecv,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/119148,2024-02-04T10:01:46Z,onnx torch.jit.script export problem,torch.jit.script,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/118532,2024-01-29T15:36:51Z,Truth function does not handle tensor inputs,torch.compile,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/115861,2023-12-14T19:16:41Z,gather_object does not work with ThreadPoolExecutor,torch.distributed.gather_object,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/114093,2023-11-20T02:35:26Z,Found nn.LazyBatchNorm1d(0) has inconsistency bug between GPU and CPU testing,torch.nn.LazyBatchNorm1d,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/113808,2023-11-15T21:53:39Z,[ONNX][dynamo_export] ONNX::Celu Half unsupported but export passed w/ invalid model when opmath disabled,torch.aten_celu,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/113808,2023-11-15T21:53:39Z,[ONNX][dynamo_export] ONNX::Celu Half unsupported but export passed w/ invalid model when opmath disabled,torch.onnx.celu,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/110946,2023-10-10T12:55:19Z,RuntimeError: out_ptr == out_accessor[thread_count_nonzero[tid + 1]].data() INTERNAL ASSERT FAILED,torch.TensorAdvancedIndexing,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/114605,2023-09-29T10:28:46Z,vmap: Transform single-element tensor to integer,torch.vmap,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/109009,2023-09-11T08:13:31Z,TypeError: mask must have dtype bool,torch.flatten,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/107256,2023-08-15T20:52:10Z,`torch.float8_e4m3fn` does not support `torch.cat`,torch.float8_e4m3fn,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/107256,2023-08-15T20:52:10Z,`torch.float8_e4m3fn` does not support `torch.cat`,torch.cat,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/107256,2023-08-15T20:52:10Z,`torch.float8_e4m3fn` does not support `torch.cat`,torch.ones,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/106851,2023-08-09T08:05:50Z,RPC all_gather doesn't work with dynamic world size (world_size=None),rpc.api._all_gather,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/104405,2023-06-29T11:50:26Z,Detailed error: Tensor-likes are not close! When use torch.jit.trace,torch.jit.trace,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/104107,2023-06-23T16:21:58Z,Tensor to_sparse fails on large matrices,torch.Tensor.to_sparse,1.13.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/103060,2023-06-06T04:19:11Z,Symbolic trace error about torch.nn.functional.pad,torch.nn.functional.pad,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/102963,2023-06-05T08:47:20Z,torch.svd fails on large matrices,torch.svd,1.13.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/102355,2023-05-26T07:36:55Z,Data type mismatch in `batch_isend_irecv` docstring example,torch.distributed.batch_isend_irecv,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/100145,2023-04-27T03:18:10Z,This flag not work : torch.backends.cudnn.allow_tf32 = False,torch.backends.cudnn.allow_tf32,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/100145,2023-04-27T03:18:10Z,This flag not work : torch.backends.cudnn.allow_tf32 = False,torch.backends.cuda.matmul.allow_tf32,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/98286,2023-04-04T09:01:24Z,"When I use the DDP model, I use a custom loss function, when the batch size changes during training, the process will be stuck.",torch.nn.parallel.DistributedDataParallel,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96448,2023-03-09T21:33:59Z,Unable to move torch.jit.load-ed models to XLA devices,torch.jit.load,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/96448,2023-03-09T21:33:59Z,Unable to move torch.jit.load-ed models to XLA devices,torch.jit.save,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/96448,2023-03-09T21:33:59Z,Unable to move torch.jit.load-ed models to XLA devices,torch.nn.Module.to,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/96110,2023-03-06T17:42:50Z,torch.where behaves differently from in place replacement,torch.where,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/95785,2023-03-01T15:38:37Z,Fully quantized model (`torch.quantization.convert`) produces incorrect output compared to analytical solution,torch.quantization.convert,1.13.0,10.8
pytorch,https://github.com/pytorch/pytorch/issues/95604,2023-02-27T09:22:52Z,"Performance bugs exists in multiple convolution operations(e.g., `Convtranspose2d`) when useing the `groups` argument",torch.nn.ConvTranspose2d,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/95604,2023-02-27T09:22:52Z,"Performance bugs exists in multiple convolution operations(e.g., `Convtranspose2d`) when useing the `groups` argument",torch.nn.LazyConv2d,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/95604,2023-02-27T09:22:52Z,"Performance bugs exists in multiple convolution operations(e.g., `Convtranspose2d`) when useing the `groups` argument",torch.nn.LazyConvTranspose2d,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/95116,2023-02-18T11:48:49Z,"When using `ceil_mode=True`, `torch.nn.AvgPool1d` could get negative shape.",torch.nn.AvgPool1d,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/95116,2023-02-18T11:48:49Z,"When using `ceil_mode=True`, `torch.nn.AvgPool1d` could get negative shape.",torch.nn.MaxPool,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/95108,2023-02-18T01:15:23Z,`torch.nn.LazyLinear` crash when using torch.bfloat16 dtype in pytorch 1.12.0 and 1.13.0,torch.nn.LazyLinear,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/95108,2023-02-18T01:15:23Z,`torch.nn.LazyLinear` crash when using torch.bfloat16 dtype in pytorch 1.12.0 and 1.13.0,torch.bfloat16,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/95108,2023-02-18T01:15:23Z,`torch.nn.LazyLinear` crash when using torch.bfloat16 dtype in pytorch 1.12.0 and 1.13.0,torch.nn.Linear,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/95108,2023-02-18T01:15:23Z,`torch.nn.LazyLinear` crash when using torch.bfloat16 dtype in pytorch 1.12.0 and 1.13.0,torch._C._nn.linear,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/92141,2023-01-13T09:42:19Z,Improve the error message in `linalg.eigh` when using large matrices,torch.linalg.eigh,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/91716,2023-01-04T19:01:31Z,linspace (and arange) behaves differently on GPU and CPU,torch.linspace,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/91716,2023-01-04T19:01:31Z,linspace (and arange) behaves differently on GPU and CPU,torch.arange,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/91629,2023-01-02T20:10:33Z,vmap + nn.SyncBatchNorm.convert_sync_batchnorm,torch.vmap,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/91629,2023-01-02T20:10:33Z,vmap + nn.SyncBatchNorm.convert_sync_batchnorm,torch.nn.SyncBatchNorm.convert_sync_batchnorm,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/91556,2022-12-31T16:11:21Z,Overflow when running torch.nn.AdaptiveMaxPool3d on torch 1.12.0 and 1.13.1,torch.nn.AdaptiveMaxPool3d,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/91375,2022-12-24T21:03:33Z,Internal Assert failed,torch.cuda.CUDAGraphsC10Utils,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/93493,2022-12-21T21:21:05Z,tensor.to_sparse() handling indices incorrectly under dynamo/fake tensor,torch.Tensor.to_sparse,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/91156,2022-12-20T06:12:31Z,`quantile` fails for `float16`/`half` inputs,torch.quantile,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/90998,2022-12-16T14:09:27Z,"Crash in `index_select` with singleton `self`, non-singleton `index`",torch.index_select,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/90916,2022-12-15T12:49:35Z,Functorch does not work with CrossEntropyLoss and label=-100,torch.nn.CrossEntropyLoss,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/90613,2022-12-10T14:35:50Z,`torch.inverse` multi-threading RuntimeError: lazy wrapper should be called at most once,torch.inverse,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/90245,2022-12-06T01:53:56Z,[Distributed] `Invalid scalar type` when `dist.scatter()` boolean tensor,torch.distributed.scatter,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/90245,2022-12-06T01:53:56Z,[Distributed] `Invalid scalar type` when `dist.scatter()` boolean tensor,torch.distributed.all_reduce,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/89757,2022-11-28T05:33:34Z,third-order gradient of torch.pow with tensor args and certain input returns NaN,torch.pow,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/89675,2022-11-25T05:22:27Z,[ONNX] torch.onnx.export snapshots the grads as constants in onnx when op is in cuda device,torch.onnx.export,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/89675,2022-11-25T05:22:27Z,[ONNX] torch.onnx.export snapshots the grads as constants in onnx when op is in cuda device,torch.autograd.grad,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/89673,2022-11-25T02:24:49Z,MPS bug on `torch.transpose` and `torch.log`,torch.transpose,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/89673,2022-11-25T02:24:49Z,MPS bug on `torch.transpose` and `torch.log`,torch.log,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/89634,2022-11-24T14:34:12Z,[ONNX] torch.onnx.export can not export the grad of conv when the op is in CPU,torch.onnx.export,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/89197,2022-11-17T07:12:52Z,Collective operations do not work with `torch.BoolTensor`s on `gloo` and raise `Invalid scalar type`,torch.distributed.gather,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/89197,2022-11-17T07:12:52Z,Collective operations do not work with `torch.BoolTensor`s on `gloo` and raise `Invalid scalar type`,torch.distributed.all_gather,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/89197,2022-11-17T07:12:52Z,Collective operations do not work with `torch.BoolTensor`s on `gloo` and raise `Invalid scalar type`,torch.distributed.reduce,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/89197,2022-11-17T07:12:52Z,Collective operations do not work with `torch.BoolTensor`s on `gloo` and raise `Invalid scalar type`,torch.distributed.all_reduce,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/89197,2022-11-17T07:12:52Z,Collective operations do not work with `torch.BoolTensor`s on `gloo` and raise `Invalid scalar type`,torch.distributed.broadcast,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/89080,2022-11-15T19:15:55Z,Unable to backprop through dense weighted sum of sparse_coo_tensors,torch.sparse.sum,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/88948,2022-11-12T16:48:35Z,torch.linalg.matrix_rank memory leak,torch.linalg.matrix_rank,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/88643,2022-11-08T01:53:29Z,pack_sequence() always fail after set_default_tensor_type to CUDA,torch.nn.utils.rnn.pack_sequence,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/88447,2022-11-03T20:30:11Z,`unique` will reverse the input when `sort=False` on cpu (not sorting),torch.unique,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/88147,2022-10-31T22:22:44Z,Conv2d is not deterministic when input tensor has different strides,torch.nn.Conv2d,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/86597,2022-10-10T14:52:16Z,JIT returns different values for `cos + frac` on cpu,torch.cos,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86597,2022-10-10T14:52:16Z,JIT returns different values for `cos + frac` on cpu,torch.frac,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86597,2022-10-10T14:52:16Z,JIT returns different values for `cos + frac` on cpu,torch.jit.trace,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86597,2022-10-10T14:52:16Z,JIT returns different values for `cos + frac` on cpu,torch.sinh,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86595,2022-10-10T14:38:23Z,JIT model with `relu+div+sgn` will crash when computing the gradient,torch.nn.functional.relu,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86595,2022-10-10T14:38:23Z,JIT model with `relu+div+sgn` will crash when computing the gradient,torch.div,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86595,2022-10-10T14:38:23Z,JIT model with `relu+div+sgn` will crash when computing the gradient,torch.sgn,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86595,2022-10-10T14:38:23Z,JIT model with `relu+div+sgn` will crash when computing the gradient,torch.jit.trace,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86595,2022-10-10T14:38:23Z,JIT model with `relu+div+sgn` will crash when computing the gradient,torch.autograd.functional.jacobian,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86594,2022-10-10T14:35:21Z,JIT model with mean will crash when computing the gradients on cuda,torch.mean,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86554,2022-10-09T14:30:04Z,[NvFuser] would change the output for some inaccurate dtype,torch.fmod,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86554,2022-10-09T14:30:04Z,[NvFuser] would change the output for some inaccurate dtype,torch.add,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86554,2022-10-09T14:30:04Z,[NvFuser] would change the output for some inaccurate dtype,torch.sub,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86554,2022-10-09T14:30:04Z,[NvFuser] would change the output for some inaccurate dtype,torch.jit.trace,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86554,2022-10-09T14:30:04Z,[NvFuser] would change the output for some inaccurate dtype,torch._C._jit_set_nvfuser_single_node_mode,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86553,2022-10-09T14:26:13Z,`topk` will return the wrong value and could read out-of-bound value after jit,torch.topk,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86552,2022-10-09T14:07:16Z,`max_unpool` and `max_pool` will trigger INTERNAL ASSERT FAIL in JIT,torch.nn.functional.max_unpool1d,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86552,2022-10-09T14:07:16Z,`max_unpool` and `max_pool` will trigger INTERNAL ASSERT FAIL in JIT,torch.nn.functional.max_pool,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86551,2022-10-09T14:03:45Z,`MultiLabelMarginLoss` will return incorrect values in JIT after the first run on cuda,torch.nn.MultiLabelMarginLoss,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86551,2022-10-09T14:03:45Z,`MultiLabelMarginLoss` will return incorrect values in JIT after the first run on cuda,torch.nn.functional.relu,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86551,2022-10-09T14:03:45Z,`MultiLabelMarginLoss` will return incorrect values in JIT after the first run on cuda,torch.sin,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86551,2022-10-09T14:03:45Z,`MultiLabelMarginLoss` will return incorrect values in JIT after the first run on cuda,torch.nn.functional.tanhshrink,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86551,2022-10-09T14:03:45Z,`MultiLabelMarginLoss` will return incorrect values in JIT after the first run on cuda,torch.Tensor.resize_as_,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86532,2022-10-08T14:41:03Z,Conv2d will crash by using `jit.trace`,torch.nn.functional.conv2d,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86532,2022-10-08T14:41:03Z,Conv2d will crash by using `jit.trace`,torch.jit.trace,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86529,2022-10-08T14:16:44Z,[NvFuser] JIT model with `mul+atan+sgn` will access illegal memory on cuda when computing gradient,torch.mul,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86529,2022-10-08T14:16:44Z,[NvFuser] JIT model with `mul+atan+sgn` will access illegal memory on cuda when computing gradient,torch.atan,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86529,2022-10-08T14:16:44Z,[NvFuser] JIT model with `mul+atan+sgn` will access illegal memory on cuda when computing gradient,torch.sgn,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86529,2022-10-08T14:16:44Z,[NvFuser] JIT model with `mul+atan+sgn` will access illegal memory on cuda when computing gradient,torch.jit.script,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86529,2022-10-08T14:16:44Z,[NvFuser] JIT model with `mul+atan+sgn` will access illegal memory on cuda when computing gradient,torch.random.manual_seed,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86529,2022-10-08T14:16:44Z,[NvFuser] JIT model with `mul+atan+sgn` will access illegal memory on cuda when computing gradient,torch.empty,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86529,2022-10-08T14:16:44Z,[NvFuser] JIT model with `mul+atan+sgn` will access illegal memory on cuda when computing gradient,torch.uniform_,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86529,2022-10-08T14:16:44Z,[NvFuser] JIT model with `mul+atan+sgn` will access illegal memory on cuda when computing gradient,torch.autograd.functional.jacobian,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86456,2022-10-07T10:50:28Z,`SyncBatchNorm` doesn't work with subclass of `torch.Tensor`,torch.nn.SyncBatchNorm,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/85877,2022-09-29T01:15:09Z,JIT model could return 'NaN' gradient after the first execution,torch.jit.script,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/85877,2022-09-29T01:15:09Z,JIT model could return 'NaN' gradient after the first execution,torch.autograd.functional.jacobian,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/85877,2022-09-29T01:15:09Z,JIT model could return 'NaN' gradient after the first execution,torch.nn.functional.relu,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/85877,2022-09-29T01:15:09Z,JIT model could return 'NaN' gradient after the first execution,torch.cos,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/85877,2022-09-29T01:15:09Z,JIT model could return 'NaN' gradient after the first execution,torch.acos,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/85805,2022-09-28T14:08:38Z,"`vector_norm` will trigger ""Tracing failed sanity checks"" for JIT when ord is boolean tensor",torch.linalg.vector_norm,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/85804,2022-09-28T14:00:56Z,JIT fails to trace `sparse.mm` with a strange error,torch.sparse.mm,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/85656,2022-09-26T20:35:29Z,"[functorch] conv.{1, 2, 3}d should raise errors",torch.nn.functional.conv2d,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/85588,2022-09-24T02:27:47Z,`linalg.norm` cannot compute the grad in forward mode after script,torch.linalg.norm,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/85585,2022-09-24T01:37:47Z,`as_tensor` will return a different dtype with script,torch.as_tensor,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/85570,2022-09-23T21:29:47Z,`mvlgamma_` will fail when compiling with trace `jit`,torch.mvlgamma_,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/85514,2022-09-22T22:09:09Z,The reload `MultiLabelMarginLoss` will have different gradients on cuda,torch.nn.MultiLabelMarginLoss,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/85505,2022-09-22T21:34:38Z,The reload model has different (and strange) forward computation from original model with `LSTMCell`,torch.nn.LSTMCell,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/83493,2022-08-16T01:39:12Z,Calling torch.linalg.cholesky on a CPU tensor requires compiling PyTorch with LAPACK.,torch.linalg.cholesky,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/82894,2022-08-05T17:56:30Z,linalg and lu tests fail when run in parallel on linux cuda,torch.linalg,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/82894,2022-08-05T17:56:30Z,linalg and lu tests fail when run in parallel on linux cuda,torch.lu,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/82894,2022-08-05T17:56:30Z,linalg and lu tests fail when run in parallel on linux cuda,torch.lu_factor,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/82894,2022-08-05T17:56:30Z,linalg and lu tests fail when run in parallel on linux cuda,torch.lu_solve,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/82894,2022-08-05T17:56:30Z,linalg and lu tests fail when run in parallel on linux cuda,torch.linalg.ldl_solve,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/82894,2022-08-05T17:56:30Z,linalg and lu tests fail when run in parallel on linux cuda,torch.cuda.synchronize,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/82894,2022-08-05T17:56:30Z,linalg and lu tests fail when run in parallel on linux cuda,torch.manual_seed,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/82894,2022-08-05T17:56:30Z,linalg and lu tests fail when run in parallel on linux cuda,torch.cuda.manual_seed_all,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/82635,2022-08-02T05:16:53Z,[Torchscript] torch.min returns wrong gradient when inputs are equal,torch.min,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/82635,2022-08-02T05:16:53Z,[Torchscript] torch.min returns wrong gradient when inputs are equal,torch.max,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/82634,2022-08-02T05:11:21Z,[Torchscript] some activations backward are not fused when used with linear,torch.erf,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/82077,2022-07-24T22:26:24Z,Using DDP with num_workers > 0 hangs before entering the first training epoch loop,torch.utils.data.distributed.DistributedSampler,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/82077,2022-07-24T22:26:24Z,Using DDP with num_workers > 0 hangs before entering the first training epoch loop,torch.utils.data.DataLoader,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/82077,2022-07-24T22:26:24Z,Using DDP with num_workers > 0 hangs before entering the first training epoch loop,torch.nn.CrossEntropyLoss,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/82077,2022-07-24T22:26:24Z,Using DDP with num_workers > 0 hangs before entering the first training epoch loop,nn.parallel.DistributedDataParallel,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/82077,2022-07-24T22:26:24Z,Using DDP with num_workers > 0 hangs before entering the first training epoch loop,torch.enable_grad,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/81361,2022-07-12T22:10:11Z,Segfault with fake tensor,torch.handle_torch_function_no_python_arg_parser,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/81361,2022-07-12T22:10:11Z,Segfault with fake tensor,at::_ops::t::redispatch,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/81361,2022-07-12T22:10:11Z,Segfault with fake tensor,torch::ADInplaceOrView::t,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/81361,2022-07-12T22:10:11Z,Segfault with fake tensor,at::_ops::linear::call,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/80580,2022-06-30T01:05:05Z,`torch.ops.aten.find` inconsistent with `str.find`,torch.ops.aten.find,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/80061,2022-06-22T18:43:33Z,pow CUDA tensor raised to CPU scalar tensor result can't backward properly,torch.pow,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/78487,2022-05-30T12:59:44Z,torch.fx: symbolic_trace: ones() received an invalid combination of arguments,torch.fx.symbolic_trace,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/74901,2022-03-29T07:50:03Z,"torch.fx.wrap will not work, when encapsulate the code",torch.fx.wrap,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/74901,2022-03-29T07:50:03Z,"torch.fx.wrap will not work, when encapsulate the code",torch.fx.symbolic_trace,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/73065,2022-02-18T07:36:53Z,`index_fill_` accepts wrong dtype for meta tensors,torch.index_fill_,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/69325,2021-12-02T22:31:31Z,`nn.functional.fractional_max_pool2d` and `nn.functional.fractional_max_pool3d` produce incorrect output on non-contiguous inputs,torch.nn.functional.fractional_max_pool2d,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/69325,2021-12-02T22:31:31Z,`nn.functional.fractional_max_pool2d` and `nn.functional.fractional_max_pool3d` produce incorrect output on non-contiguous inputs,torch.nn.functional.fractional_max_pool3d,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/65683,2021-09-27T09:45:27Z,`torch.nn.functional.l1_loss` fails gradgradcheck for complex inputs,torch.nn.functional.l1_loss,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/63405,2021-08-17T17:07:17Z,torch.arange has issue tracing with bool input,torch.arange,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/61281,2021-07-06T10:26:33Z,DataLoader with IterativeDataset throws an error when providing a BatchSampler,torch.DataLoader,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/53904,2021-03-12T16:44:24Z,[JIT] Support `torch.ones([])`,torch.ones,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/48641,2020-12-01T06:27:32Z,"[bug] `torch.{sinh, cosh}`: Incorrect values for vectorized path",torch.sinh,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/48641,2020-12-01T06:27:32Z,"[bug] `torch.{sinh, cosh}`: Incorrect values for vectorized path",torch.cosh,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/48273,2020-11-19T21:01:12Z,torch.multinomial example is incorrect,torch.multinomial,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/132989,2024-08-08T10:33:37Z,Different random seeds can cause differences in arithmetic precision,torch.manual_seed,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/122729,2024-03-26T21:45:17Z,torch.func transforms don't yet support saved tensor hooks.,torch.func.vmap,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/123158,2024-04-02T08:51:45Z,onnx export error with nn.InstanceNorm2d,torch.nn.InstanceNorm2d,1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/122550,2024-03-23T09:50:14Z,Adam optimizer：RuntimeError: t == DeviceType::CUDA INTERNAL ASSERT FAILED,torch.optim.adam,1.12.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/113370,2023-11-09T17:16:14Z,Incorrect stride when permuting shapes where a zero dimension is present.,torch.empty,1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/113370,2023-11-09T17:16:14Z,Incorrect stride when permuting shapes where a zero dimension is present.,torch.permute,1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/113370,2023-11-09T17:16:14Z,Incorrect stride when permuting shapes where a zero dimension is present.,torch.contiguous,1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/113370,2023-11-09T17:16:14Z,Incorrect stride when permuting shapes where a zero dimension is present.,torch.view_as_complex,1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/103316,2023-06-09T09:58:06Z,binary_cross_entropy (loss) seems to be giving incorrect values for very negative logits,torch.nn.functional.binary_cross_entropy,1.12.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/102894,2023-06-02T23:51:09Z,"BCELoss and BCEWithLogitsLoss differ when one of the input logits is float(""inf"")",torch.nn.BCEWithLogitsLoss,1.12.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/102894,2023-06-02T23:51:09Z,"BCELoss and BCEWithLogitsLoss differ when one of the input logits is float(""inf"")",torch.nn.BCELoss,1.12.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/100411,2023-05-01T20:06:07Z,Sparse Matrix nnz Overflow when casting from COO to CSR,torch.sparse_coo_tensor,1.12.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/100411,2023-05-01T20:06:07Z,Sparse Matrix nnz Overflow when casting from COO to CSR,torch.sparse_csr_tensor,1.12.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/100411,2023-05-01T20:06:07Z,Sparse Matrix nnz Overflow when casting from COO to CSR,to_sparse_csr,1.12.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/95724,2023-02-28T19:15:01Z,"distributed training: lots of ""Exception ignored"" at the end of each epoch",torch.distributed.init_process_group,1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/95724,2023-02-28T19:15:01Z,"distributed training: lots of ""Exception ignored"" at the end of each epoch",torch.utils.data.distributed.DistributedSampler,1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/95724,2023-02-28T19:15:01Z,"distributed training: lots of ""Exception ignored"" at the end of each epoch",torch.distributed.barrier,1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/95724,2023-02-28T19:15:01Z,"distributed training: lots of ""Exception ignored"" at the end of each epoch",torch.multiprocessing.spawn,1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/95460,2023-02-24T09:04:00Z,torch.profiler.tensorboard_trace_handler Generates an incorrect JSON file,torch.profiler.tensorboard_trace_handler,1.12.1,11.2
pytorch,https://github.com/pytorch/pytorch/issues/92866,2023-01-23T23:11:50Z,JIT Function Fails when run a second time,torch.jit.script,1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/91533,2022-12-30T08:02:35Z,Cannot cast float64 to float32,torch.nn.Linear,1.12.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/90999,2022-12-16T14:49:24Z,overflow (?) on cuda tensor after matrix multiplication,torch.clone,1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/90999,2022-12-16T14:49:24Z,overflow (?) on cuda tensor after matrix multiplication,torch.cuda,1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/90999,2022-12-16T14:49:24Z,overflow (?) on cuda tensor after matrix multiplication,torch.rand,1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/90999,2022-12-16T14:49:24Z,overflow (?) on cuda tensor after matrix multiplication,torch.float32,1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/90999,2022-12-16T14:49:24Z,overflow (?) on cuda tensor after matrix multiplication,torch.matmul,1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/90999,2022-12-16T14:49:24Z,overflow (?) on cuda tensor after matrix multiplication,torch.unsqueeze,1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/90999,2022-12-16T14:49:24Z,overflow (?) on cuda tensor after matrix multiplication,torch.argmin,1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/90999,2022-12-16T14:49:24Z,overflow (?) on cuda tensor after matrix multiplication,torch.min,1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/90999,2022-12-16T14:49:24Z,overflow (?) on cuda tensor after matrix multiplication,torch.argmax,1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/90999,2022-12-16T14:49:24Z,overflow (?) on cuda tensor after matrix multiplication,torch.max,1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/90999,2022-12-16T14:49:24Z,overflow (?) on cuda tensor after matrix multiplication,torch.transpose,1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/90793,2022-12-13T20:58:39Z,nn.CrossEntropyLoss error out when the sample size is large,torch.nn.CrossEntropyLoss,1.12.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/90760,2022-12-13T12:26:43Z,Large slow down by not calling `torch.set_num_threads`,torch.set_num_threads,1.12.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/89716,2022-11-27T06:08:57Z,DDP hangs on forward pass of transformer,torch.distributed.init_process_group,1.12.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/89716,2022-11-27T06:08:57Z,DDP hangs on forward pass of transformer,torch.nn.parallel.DistributedDataParallel,1.12.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/89686,2022-11-25T14:51:19Z,MultiProcess tests fail when run on nodes with 1 GPU,torch.distributed.barrier,1.12.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/88648,2022-11-08T03:33:47Z,`MultiMarginLoss` doesn't check the value of `target` on CUDA,torch.nn.MultiMarginLoss,1.12.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/88647,2022-11-08T03:22:27Z,`ConvTranspose` fails on CPU but returns an empty tensor on CUDA,torch.nn.functional.conv_transpose2d,1.12.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/88191,2022-11-01T13:34:29Z,`torch.nn.RReLU` not reporting `lower > upper` on CUDA,torch.nn.RReLU,1.12.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/88144,2022-10-31T22:13:24Z,AvgPool2D output shapes are inconsistent when ceil_mode=True,torch.nn.AvgPool2d,1.12.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/87992,2022-10-28T17:14:10Z,torch.rand(...) is not consistent for large shape dimensions across GPUs (with the same random seed),torch.rand,1.12.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/87961,2022-10-28T05:33:59Z,crash in `torch.package.PackageExporter`,torch.package.PackageExporter,1.12.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/87960,2022-10-28T05:27:50Z,crash when call `torch.set_num_interop_threads` twice,torch.set_num_interop_threads,1.12.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/87556,2022-10-22T19:59:39Z,The behavior of cast `NaN` is different on cpu and cuda,torch.tensor,1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/87556,2022-10-22T19:59:39Z,The behavior of cast `NaN` is different on cpu and cuda,torch.int32,1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/87556,2022-10-22T19:59:39Z,The behavior of cast `NaN` is different on cpu and cuda,torch.clone,1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/87556,2022-10-22T19:59:39Z,The behavior of cast `NaN` is different on cpu and cuda,torch.type,1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/87433,2022-10-21T01:14:51Z,index_select() applied in sparse tensor can't backprop,torch.index_select,1.12.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/86849,2022-10-12T23:28:55Z,`torch.distributed.all_reduce` allocates excess GPU memory when using NCCL backend,torch.distributed.all_reduce,1.12.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/86791,2022-10-12T13:24:15Z,We don't have an op for vulkan_prepack::conv2d_clamp_prepack but it isn't a special case.,torch._C._jit_pass_vulkan_optimize_for_mobile,1.12.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/86465,2022-10-07T17:13:07Z,Wrong results with torch.linalg.inv on batched matrices when using cuda,torch.linalg.inv,1.12.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86449,2022-10-07T05:14:14Z,torch.cuda.empty_cache() is not working,torch.cuda.empty_cache(),1.12.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/86162,2022-10-03T22:44:49Z,torch.nn.functional.one_hot only works for int64,torch.nn.functional.one_hot,1.12.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/85791,2022-09-28T06:31:47Z,nn.CrossEntropyLoss overflow with FP16 and minibatch,torch.nn.CrossEntropyLoss,1.12.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/85642,2022-09-26T17:24:46Z,Could not run 'aten::native_batch_norm' with arguments from the 'SparseCUDA' backend. using batch_norm,torch.aten::native_batch_norm,1.12.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/85329,2022-09-20T08:25:03Z,Crash in `torch.package.PackageExporter`,torch.package.PackageExporter,1.12.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/85082,2022-09-15T10:35:56Z,"[NNC] loop vectorization fails, `Ramp` and `Broadcast` undefined",torch._C._te.Ramp,1.12.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/85082,2022-09-15T10:35:56Z,"[NNC] loop vectorization fails, `Ramp` and `Broadcast` undefined",torch._C._te.Broadcast,1.12.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/85072,2022-09-15T06:02:50Z,Segmentation fault in `torch.jit.wait`,torch.jit.wait,1.12.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/84990,2022-09-14T07:55:01Z,Segmentation fault in `torch.futures.collect_all`,torch.futures.collect_all,1.12.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/84673,2022-09-07T23:38:05Z,Autograd will take `init` module API into account when using `jit`,torch.nn.init.ones_,1.12.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/84628,2022-09-07T12:35:10Z,INTERNAL ASSERT when the type of argument is not considered in JIT,torch.jit.trace,1.12.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/84628,2022-09-07T12:35:10Z,INTERNAL ASSERT when the type of argument is not considered in JIT,torch.add,1.12.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/84530,2022-09-05T06:18:47Z,`tensordot` not working for dtype int32 and lower when there is only 1 element in the given axis,torch.tensordot,1.12.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/84202,2022-08-29T16:30:16Z,Gradient value calculation error in MultiLabelMarginLoss,torch.nn.functional.multilabel_margin_loss,1.12.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/84175,2022-08-27T23:34:52Z,FSDP Forward order differs from that of first run,torch.generate,1.12.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/83733,2022-08-19T11:40:39Z,BCELoss results in autocast CUDA warning,torch.nn.BCELoss,1.12.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/68966,2021-11-28T22:10:39Z,`torch.ldexp` generated tests fail on call to `torch.mul`,torch.ldexp,1.12.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/68966,2021-11-28T22:10:39Z,`torch.ldexp` generated tests fail on call to `torch.mul`,torch.mul,1.12.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/115610,2023-12-12T02:50:14Z,[Bug] Big difference between the output of Conv float precision and double precision,torch.nn.functional.conv2d,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/100705,2023-05-05T10:14:38Z,torch.cuda.amp.GradScaler initialization,torch.cuda.amp.GradScaler,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/pull/131977,2024-07-27T12:40:58Z,Fix bmm_sparse_cuda illegal memory access,torch.bmm_sparse_cuda,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/131679,2024-07-24T17:57:25Z,"torch.export() fails on aten.to(..., copy=True) followed by mutation",torch.ops.aten.to,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/128645,2024-06-13T20:43:38Z,Improving expand w/ unbacked symints,API,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/128645,2024-06-13T20:43:38Z,Improving expand w/ unbacked symints,Name,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/128645,2024-06-13T20:43:38Z,Improving expand w/ unbacked symints,torch.expand,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/114571,2023-11-27T02:29:03Z,inconsistency between nan cast to int32 on CPU and GPU,torch.tensor,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/112376,2023-10-30T08:43:46Z,Contradictory Error Message for stride Argument in torch.conv_transpose3d(),torch.conv_transpose3d,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/111834,2023-10-23T19:34:58Z,"Numerical inaccuracies in ""ddp_apply_optim_in_backward"" unit tests for gloo backend",torch.ddp_apply_optim_in_backward,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/107528,2023-08-19T23:16:26Z,Inconsistent results when running torch.arctanh,torch.arctanh,1.12.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/107433,2023-08-18T02:34:31Z,No checks when running torch.nn.functional.ctc_loss with bogus inputs,torch.nn.functional.ctc_loss,1.12.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/107432,2023-08-18T02:32:42Z,"Inconsistent results when running torch.nn.functional.embedding_bag on CPU (1.12.0, 1.13.0)",torch.nn.functional.embedding_bag,1.12.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/107429,2023-08-18T01:36:33Z,Abort when running torch.set_num_interop_threads,torch.set_num_interop_threads,1.12.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/97004,2023-03-17T06:11:36Z,Small learning rate with `capturable=True` causes Adam optimizer to blow up model parameters.,torch.optim.Adam,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/95590,2023-02-27T03:31:48Z,Confusing error messages from `torch.nn.LazyLinear` in different versions.,torch.nn.LazyLinear,1.12.0,"11.3
11.6"
pytorch,https://github.com/pytorch/pytorch/issues/95590,2023-02-27T03:31:48Z,Confusing error messages from `torch.nn.LazyLinear` in different versions.,torch.nn.LSTMCell,1.12.0,"11.3
11.6"
pytorch,https://github.com/pytorch/pytorch/issues/92828,2023-01-23T17:37:44Z,segfault when running torch.igamma,torch.igamma,1.12.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/92794,2023-01-23T03:01:16Z,Segmentation fault when running torch.ge,torch.ge,1.12.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/91593,2023-01-02T18:03:06Z,Division by zero error when running torch.nn.functional.lp_pool1d,torch.nn.functional.lp_pool1d,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/91552,2022-12-31T14:58:08Z,Overflow when running torch.nn.AdaptiveMaxPool2d,torch.nn.AdaptiveMaxPool2d,1.12.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/88491,2022-11-04T13:31:41Z,torch.set_grad_enabled results in RuntimeError with torch.jit.script,torch.set_grad_enabled,1.12.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/88491,2022-11-04T13:31:41Z,torch.set_grad_enabled results in RuntimeError with torch.jit.script,torch.jit.script,1.12.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/85813,2022-09-28T15:15:59Z,TorchScript error for `Enum` inside a module,torch.jit.script,1.12.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/85792,2022-09-28T06:48:25Z,TorchScript causes range_check error after a few iterations of forward-backward passes,torch.jit.load,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/85792,2022-09-28T06:48:25Z,TorchScript causes range_check error after a few iterations of forward-backward passes,torch.randn,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/85792,2022-09-28T06:48:25Z,TorchScript causes range_check error after a few iterations of forward-backward passes,torch.randn_like,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/85792,2022-09-28T06:48:25Z,TorchScript causes range_check error after a few iterations of forward-backward passes,aten::dropout,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/85792,2022-09-28T06:48:25Z,TorchScript causes range_check error after a few iterations of forward-backward passes,aten::add,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/85792,2022-09-28T06:48:25Z,TorchScript causes range_check error after a few iterations of forward-backward passes,aten::to,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/85792,2022-09-28T06:48:25Z,TorchScript causes range_check error after a few iterations of forward-backward passes,aten::mean,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/85792,2022-09-28T06:48:25Z,TorchScript causes range_check error after a few iterations of forward-backward passes,aten::sub,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/85792,2022-09-28T06:48:25Z,TorchScript causes range_check error after a few iterations of forward-backward passes,aten::pow,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/85792,2022-09-28T06:48:25Z,TorchScript causes range_check error after a few iterations of forward-backward passes,aten::sqrt,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/85792,2022-09-28T06:48:25Z,TorchScript causes range_check error after a few iterations of forward-backward passes,aten::div,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/85792,2022-09-28T06:48:25Z,TorchScript causes range_check error after a few iterations of forward-backward passes,prim::TupleConstruct,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/85792,2022-09-28T06:48:25Z,TorchScript causes range_check error after a few iterations of forward-backward passes,prim::TupleUnpack,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/85792,2022-09-28T06:48:25Z,TorchScript causes range_check error after a few iterations of forward-backward passes,prim::ListConstruct,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/85792,2022-09-28T06:48:25Z,TorchScript causes range_check error after a few iterations of forward-backward passes,prim::Constant,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/85558,2022-09-23T18:32:22Z,torch.Tensor.transpose().contiguous() on dimension of size 1 gives wrong stride,torch.Tensor.transpose,1.12.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/85558,2022-09-23T18:32:22Z,torch.Tensor.transpose().contiguous() on dimension of size 1 gives wrong stride,torch.Tensor.contiguous,1.12.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/83824,2022-08-21T20:49:17Z,RuntimeError: Interrupted system call when doing distributed training,torch.distributed.init_process_group,1.12.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/83818,2022-08-21T07:59:58Z,torch.linalg.eigh crashe for matrices of size 2895×2895 or larger on eigen and M1,torch.linalg.eigh,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/82510,2022-07-29T23:43:46Z,Complex addition result in NaN when it shouldn't,torch.view_as_real,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/82510,2022-07-29T23:43:46Z,Complex addition result in NaN when it shouldn't,torch.view_as_complex,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/82308,2022-07-27T12:52:17Z,torch.einsum gets wrong results randomly when training with multi-gpu,torch.einsum,1.12.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/82282,2022-07-27T00:28:15Z,`torch.matrix_exp` doesn't handle NaN properly,torch.matrix_exp,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/82218,2022-07-26T13:30:30Z,OOM during backward() leads to memory leaks,API,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/82218,2022-07-26T13:30:30Z,OOM during backward() leads to memory leaks,Name,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/82218,2022-07-26T13:30:30Z,OOM during backward() leads to memory leaks,torch.autograd.backward,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/82212,2022-07-26T09:16:42Z,"RuntimeError: ""reflection_pad2d"" not implemented for 'Half' in autocast enabled region",torch.nn.functional.reflection_pad2d,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/82065,2022-07-23T20:00:21Z,optimize_for_mobile vulkan_prepack::conv2d_clamp_prepack,torch.utils.mobile_optimizer.optimize_for_mobile,1.12.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/82065,2022-07-23T20:00:21Z,optimize_for_mobile vulkan_prepack::conv2d_clamp_prepack,torch._C._jit_pass_vulkan_optimize_for_mobile,1.12.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/82065,2022-07-23T20:00:21Z,optimize_for_mobile vulkan_prepack::conv2d_clamp_prepack,vulkan_prepack::conv2d_clamp_prepack,1.12.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/81868,2022-07-21T11:21:16Z,grid_sample and mode='bilinear' induces errors at discrete pixel locations,torch.nn.functional.grid_sample,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/81545,2022-07-15T05:45:01Z,Precision error from torch.distributed.send() to recv(),torch.distributed.send,1.12.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/81545,2022-07-15T05:45:01Z,Precision error from torch.distributed.send() to recv(),torch.distributed.recv,1.12.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/81195,2022-07-11T05:12:01Z,torch._weight_norm with specified dim returns wrong output,torch._weight_norm,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/80946,2022-07-06T10:00:28Z,torch.nn.functional.linear fails for multi-dimensional bias from torch 1.12,torch.nn.functional.linear,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/80861,2022-07-05T09:41:49Z,"jit.freeze throws RuntimeError: stack_out && stack_out->size() == 1 INTERNAL ASSERT FAILED at ""../torch/csrc/jit/passes/frozen_conv_folding.cpp"":281",torch.jit.freeze,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/92033,2022-06-30T22:07:18Z,Unable to use vmap atop torch.distribution functionality,torch.distributions.Independent,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/92033,2022-06-30T22:07:18Z,Unable to use vmap atop torch.distribution functionality,torch.distributions.Normal,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/92033,2022-06-30T22:07:18Z,Unable to use vmap atop torch.distribution functionality,functorch.make_functional_with_buffers,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/92033,2022-06-30T22:07:18Z,Unable to use vmap atop torch.distribution functionality,functorch.jacrev,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/92033,2022-06-30T22:07:18Z,Unable to use vmap atop torch.distribution functionality,functorch.vmap,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/80738,2022-06-30T20:46:37Z,Output for `aten::_native_multi_head_attention` appears inconsistent with entry in `native_functions.yaml`,torch.ops.aten._native_multi_head_attention,1.12.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/80488,2022-06-29T01:16:35Z,Negative values still produced by torch.nn.functional.kl_div,torch.nn.functional.kl_div,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/77838,2022-05-19T08:28:27Z,Sporadic convolution error with dilation=0,torch.nn.Conv2d,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/76705,2022-05-02T23:50:13Z,"Torch `x += y.bmm(z)` is faster than `x.baddbmm_(y, z)`",torch.badbmm_,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/76654,2022-05-02T12:22:21Z,`torch.sort` does not exploit parallelization when invoked without the `dim` parameter.,torch.sort,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/76571,2022-04-29T00:29:32Z,torch.unique() nondeterministic behavior on nan inputs (on GPU),torch.unique,1.12.0,11.1
pytorch,https://github.com/pytorch/pytorch/issues/76527,2022-04-28T10:47:38Z,Non target rank receives result of 'reduce' op when backend is 'gloo',torch.distributed.reduce,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/76347,2022-04-25T22:28:40Z,`index_select` allows negative `index` for sparse but not for strided `self`,torch.index_select,1.12.0,11.2
pytorch,https://github.com/pytorch/pytorch/issues/76043,2022-04-19T15:35:44Z,RuntimeError: bucket_count == per_bucket_sizes.size() INTERNAL ASSERT FAILED,torch.nn.parallel.distributed,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/75903,2022-04-15T18:48:12Z,torch.jit.script'd function very slow on first invocation on latest nightly,torch.jit.script,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/75342,2022-04-06T18:16:29Z,`torch.fx.operator_schemas.normalize_function` is too permissive,torch.fx.operator_schemas.normalize_function,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/74771,2022-03-25T19:27:05Z,[bug] `torch.multinomial` should throw error as documented,torch.multinomial,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/74605,2022-03-23T09:45:16Z,torch.profiler.profile does't work well for CPU model when not using torch.profiler.schedule,torch.profiler.profile,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/74605,2022-03-23T09:45:16Z,torch.profiler.profile does't work well for CPU model when not using torch.profiler.schedule,torch.profiler.schedule,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/68610,2021-11-18T20:26:08Z,`torch.unique_consecutive`: passing positional optional arguments results in empty tensors,torch.unique_consecutive,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/67970,2021-11-07T15:18:50Z,Error when using torch.fx on bert,torch.fx.symbolic_trace,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/67651,2021-11-01T23:47:47Z,MultiHeadAttention in quantizable seems incorrect with batch_first=True,torch.nn.MultiHeadAttention,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/66504,2021-10-12T20:27:16Z,BatchNorm runtimeError: one of the variables needed for gradient computation has been modified by an inplace operation,torch.nn.BatchNorm1d,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/66504,2021-10-12T20:27:16Z,BatchNorm runtimeError: one of the variables needed for gradient computation has been modified by an inplace operation,torch.nn.parallel.DistributedDataParallel,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/66504,2021-10-12T20:27:16Z,BatchNorm runtimeError: one of the variables needed for gradient computation has been modified by an inplace operation,torch.nn.SyncBatchNorm,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/60939,2021-06-29T09:43:20Z,Functional multi_head_attention_forward softmax get nan for fp16 mode,torch.nn.functional.multi_head_attention_forward,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/60939,2021-06-29T09:43:20Z,Functional multi_head_attention_forward softmax get nan for fp16 mode,torch.nn.functional.softmax,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/60277,2021-06-18T17:02:25Z,Sparse CSR tensor should not accept equal column indices in the same row,torch.sparse_csr_tensor,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/53407,2021-03-05T21:18:49Z,"torch.matmul doesn't handle zero-sized inputs in some cases, leading to batched grad failures",torch.matmul,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/51644,2021-02-03T16:53:23Z,"Segmentation Fault: Garbage collector, cuda memory",torch.cuda.memory_allocated,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/47481,2020-11-06T00:55:11Z,[fx] scripting a model with tensor list as input fails,torch.fx.symbolic_trace,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/47481,2020-11-06T00:55:11Z,[fx] scripting a model with tensor list as input fails,torch.jit.script,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/47481,2020-11-06T00:55:11Z,[fx] scripting a model with tensor list as input fails,torch.nn.Module.forward,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/39437,2020-06-03T08:32:56Z,error when specifying sparse=True in embedding,torch.embedding,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/98203,2023-04-03T08:48:21Z,"AssertionError: was expecting embedding dimension of 22, but got 1320",torch.nn.TransformerEncoder,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/93275,2023-01-30T16:05:48Z,Bug in torch.linalg.svd,torch.linalg.svd,1.12.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/76282,2022-04-24T07:57:07Z,`torch.cuda.amp.GradScaler` may skip parameter synchronization required by post localSGD optimizer,torch.cuda.amp.GradScaler,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/116177,2023-12-20T07:23:50Z,OOM error for collection communication primitive provided by torch.distributed,torch.distributed.all_reduce,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/115064,2023-12-04T08:48:48Z,torch.nn.PairwiseDistance，The results vary widely from version to version,torch.nn.PairwiseDistance,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/107133,2023-08-14T12:14:26Z,torch.inverse throws error when DP but not in DDP or single GPU,torch.inverse,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/101968,2023-05-22T09:21:04Z,CrossEntropyLoss output difference on Windows vs. Linux,torch.nn.CrossEntropyLoss,1.11.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/96579,2023-03-11T00:54:04Z,Wrong return type from operation on custom tensor inside registered hook,torch.Tensor,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/96265,2023-03-08T02:17:45Z,RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation,torch.autograd,1.11.0,11.5
pytorch,https://github.com/pytorch/pytorch/issues/90842,2022-12-14T15:12:28Z,nn.MultiheadAttention softmax inconsistent in training mode,torch.nn.MultiheadAttention,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/86124,2022-10-03T16:09:23Z,torch.jit.trace throwing Invalid name for qualified name eror,torch.jit.trace,1.11.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/85671,2022-09-26T21:59:52Z,nn.Embedding weights are not synced across processes with DistributedDataParallel when other parameters are present,torch.nn.Embedding,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/85217,2022-09-17T17:26:23Z,Segmentation fault in native_batch_norm,torch.native_batch_norm,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/85214,2022-09-17T17:26:03Z,Segmentation fault in mkldnn_reorder_conv2d_weight and mkldnn_reorder_conv3d_weight,torch._C._nn.mkldnn_reorder_conv2d_weight,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/85214,2022-09-17T17:26:03Z,Segmentation fault in mkldnn_reorder_conv2d_weight and mkldnn_reorder_conv3d_weight,torch._C._nn.mkldnn_reorder_conv3d_weight,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/84304,2022-08-30T20:39:16Z,op for aten::bitwise_and during torch.jit.trace,torch.aten::bitwise_and,1.11.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/83851,2022-08-22T16:09:04Z,torch fx cannot trace assert for some cases,torch.fx,1.11.0,11.4
pytorch,https://github.com/pytorch/pytorch/issues/82886,2022-08-05T16:02:52Z,CUDA graph capturing fails for nn.Embedding and large batch sizes,torch.nn.Embedding,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/82465,2022-07-29T10:59:19Z,Does torch.utils.checkpoint compatible with torch.cuda.make_graphed_callables?,torch.cuda.make_graphed_callables,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/82465,2022-07-29T10:59:19Z,Does torch.utils.checkpoint compatible with torch.cuda.make_graphed_callables?,torch.utils.checkpoint.checkpoint_sequential,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/82303,2022-07-27T09:19:22Z,Race condition between torch.tensor's view and /= (/= returns incorrect result),torch.Tensor,1.11.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/82185,2022-07-25T23:52:07Z,DataLoader: `pin_memory` should respect object attributes before object collection type,torch.utils.data.DataLoader.pin_memory,1.11.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/82068,2022-07-23T22:02:27Z,upsample_bilinear2d() received an invalid combination of arguments,torch.nn.functional.interpolate,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/81323,2022-07-12T15:10:02Z,`torch.overrides.get_testing_overrides` does not function as intended for native tensor methods/operations,torch.overrides.get_testing_overrides,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/80808,2022-07-03T14:17:11Z,"The result of doing a dot product between two vectors, using einsum, depends on another unrelated vector",torch.einsum,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/80804,2022-07-03T05:03:05Z,`torch.renorm` gives wrong gradient for 0-valued input when `p` is even and `maxnorm=0`.,torch.renorm,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/80803,2022-07-03T04:52:04Z,`hardshrink` gives wrong gradient for 0 input when `lambd` is 0.,torch.hardshrink,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/80762,2022-07-01T07:34:08Z,`atan2` will gradcheck fail when `other` is a tensor with `int8` dtype,torch.atan2,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/80761,2022-07-01T07:22:01Z,`det` will return wrong gradient for `1x1` matrix with 0 value.,torch.det,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/80588,2022-06-30T03:54:25Z,Semi-reproducible random torch.baddbmm NaNs,torch.baddbmm,1.11.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/80302,2022-06-26T16:12:30Z,AttributeError: 'LinearPackedParams' object has no attribute '_modules',torch.nn.modules.module,1.11.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/80208,2022-06-24T05:09:30Z,F.binary_cross_entropy_with_logits unexpected behaviour,torch.F.binary_cross_entropy_with_logits,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/80206,2022-06-24T03:20:22Z,`soft_margin_loss` gives wrong gradient when `target` with dtype uint8,torch.nn.functional.soft_margin_loss,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/80204,2022-06-24T03:18:52Z,`max_unpool` gives wrong gradient when `indices` has duplicate,torch.nn.functional.max_unpool1d,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/79705,2022-06-16T14:25:56Z,gradgradcheck fails for torch.native_layer_norm,torch.native_layer_norm,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/79703,2022-06-16T14:01:23Z,Float and double tensors randomly initialized with the same seed get different values for size >= 16,torch.manual_seed,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/79703,2022-06-16T14:01:23Z,Float and double tensors randomly initialized with the same seed get different values for size >= 16,torch.normal_,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/79542,2022-06-14T17:39:46Z,Unable to use a parameter with torch.sparse_coo layout with DDP,torch.sparse_coo_tensor,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/79542,2022-06-14T17:39:46Z,Unable to use a parameter with torch.sparse_coo layout with DDP,torch.nn.parallel.DistributedDataParallel,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/79388,2022-06-13T01:17:48Z,Init connect timeout when use torch.distributed.run,torch.distributed.run,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/79171,2022-06-09T00:03:29Z,DistributedDataParallel `static_graph=True` fails to handle unused parameters,torch.nn.parallel.distributed.DistributedDataParallel,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/79145,2022-06-08T19:24:47Z,Redundant info are saved when using torch.save to save part of torch.tensor,torch.save,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/78987,2022-06-07T01:26:18Z,DataLoader leaking resources?,torch.utils.data.DataLoader,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/78638,2022-06-01T18:23:38Z,"torch.distributed.init_process_group(backend=""nccl"") NCCL version error",torch.distributed.init_process_group,1.11.0,11
pytorch,https://github.com/pytorch/pytorch/issues/78482,2022-05-30T05:22:21Z,RuntimeError: Event device type CUDA does not match blocking stream’s device type CPU,torch.autograd.backward,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/78444,2022-05-28T01:47:09Z,`layer_norm` triggers INTERNAL ASSERT with input requiring grad + zero-size int tensor,torch.nn.functional.layer_norm,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/78443,2022-05-28T01:29:24Z,`index_fill` will trigger INTERNAL ASSERT when float tensor requiring grad + int tensor,torch.index_fill,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/78346,2022-05-26T07:35:05Z,`gradcheck` fails for `torch.distribution.transform` APIs in forward mode,torch.distributions.transforms.AbsTransform,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/78346,2022-05-26T07:35:05Z,`gradcheck` fails for `torch.distribution.transform` APIs in forward mode,torch.distributions.transforms.SigmoidTransform,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/78346,2022-05-26T07:35:05Z,`gradcheck` fails for `torch.distribution.transform` APIs in forward mode,torch.distributions.transforms.TanhTransform,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/78346,2022-05-26T07:35:05Z,`gradcheck` fails for `torch.distribution.transform` APIs in forward mode,torch.distributions.transforms.SoftmaxTransform,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/78255,2022-05-25T14:33:08Z,Floating point exception in _conv_depthwise2d,torch._C._nn._conv_depthwise2d,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/78249,2022-05-25T10:24:51Z,`max_unpool2d` is not deterministic,torch.nn.MaxUnpool2d,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/78201,2022-05-24T20:43:51Z,nn.Sequential causes fx.replace_pattern to not find any match.,torch.nn.Sequential,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/78201,2022-05-24T20:43:51Z,nn.Sequential causes fx.replace_pattern to not find any match.,torch.fx.replace_pattern,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/78201,2022-05-24T20:43:51Z,nn.Sequential causes fx.replace_pattern to not find any match.,torch.fx.symbolic_trace,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/78201,2022-05-24T20:43:51Z,nn.Sequential causes fx.replace_pattern to not find any match.,torchvision.models.resnet.Bottleneck,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/78153,2022-05-24T01:53:35Z,`pack_sequence` crash,torch.nn.utils.rnn.pack_sequence,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/78151,2022-05-24T01:29:33Z,`ctc_loss` will backward crash,torch.nn.functional.ctc_loss,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/78143,2022-05-24T00:50:01Z,`baddmm` triggers INTERNAL ASSERT FAILED when input requires grad,torch.baddbmm,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/78141,2022-05-24T00:44:42Z,"`matmul, mm` triggers INTERNAL ASSERT FAILED when input requires grad",torch.matmul,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/78141,2022-05-24T00:44:42Z,"`matmul, mm` triggers INTERNAL ASSERT FAILED when input requires grad",torch.mm,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/78131,2022-05-23T21:11:42Z,Segfault in _pad_packed_sequence,torch._pad_packed_sequence,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/78130,2022-05-23T21:10:35Z,Segfault in _grid_sampler_2d_cpu_fallback,torch._grid_sampler_2d_cpu_fallback,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/78129,2022-05-23T21:09:16Z,Segfault in _embedding_bag_forward_only,torch._embedding_bag_forward_only,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/78128,2022-05-23T21:06:50Z,Segfault in torch._C._nn.thnn_conv2d,torch._C._nn.thnn_conv2d,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/78127,2022-05-23T21:05:18Z,Segfault in torch._C._nn.reflection_pad2d,torch._C._nn.reflection_pad2d,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/78126,2022-05-23T21:00:57Z,Segfault in max_unpool3d,torch.max_unpool3d,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/78125,2022-05-23T20:57:10Z,Segfault in grid_sampler_3d,torch.grid_sampler_3d,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/78122,2022-05-23T20:50:38Z,Segfault in bincount,torch.bincount,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/78047,2022-05-21T20:07:36Z,DDP multi host with single GPU each.,torch.cuda.set_device,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/77963,2022-05-20T14:46:58Z,`logaddexp2` fails to backward,torch.logaddexp2,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/77951,2022-05-20T07:48:48Z,`topk` returns different results with the same input twice in cuda,torch.topk,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/77901,2022-05-19T21:23:21Z,Heap corruption in slow_conv_transpose3d,torch._C._nn.slow_conv_transpose3d,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/77900,2022-05-19T21:23:17Z,Floating point exception in slow_conv3d,torch._C._nn.slow_conv3d,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/77899,2022-05-19T21:23:13Z,Floating point exception in native_channel_shuffle,torch.native_channel_shuffle,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/77894,2022-05-19T21:22:46Z,Floating point exception in channel_shuffle,torch.channel_shuffle,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/77893,2022-05-19T21:22:41Z,Segmentation fault in _remove_batch_dim,torch._remove_batch_dim,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/77814,2022-05-19T01:35:36Z,"`addmv, mv` will trigger INTERNAL ASSERT FAILED when input requiring grad",torch.addmv,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/77814,2022-05-19T01:35:36Z,"`addmv, mv` will trigger INTERNAL ASSERT FAILED when input requiring grad",torch.mv,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/77527,2022-05-16T08:24:55Z,CUDA: Illegal memory access in `torch.linalg.solve()`,torch.linalg.solve,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/77515,2022-05-16T00:52:17Z,Inplace Bool API + `sum` will trigger INTERNAL ASSERT FAILED,torch.eq_,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/77515,2022-05-16T00:52:17Z,Inplace Bool API + `sum` will trigger INTERNAL ASSERT FAILED,torch.index_fill_,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/77514,2022-05-16T00:41:37Z,`max_pool1d` can succeed when padding is negative for tensor requiring grad,torch.nn.functional.max_pool1d,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/77397,2022-05-13T01:27:22Z,Large numerical inconsistency for `torch.einsum` on RTX30 series GPU.,torch.einsum,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/77354,2022-05-12T15:49:38Z,"outputs_[i]->uses().empty()INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1646755853042/work/torch/csrc/jit/ir/ir.cpp"":1314, please report a bug to PyTorch.",torch.jit.script,1.11.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/77231,2022-05-11T05:12:17Z,`torch.scatter_add` will succeed when the `index` is a complex tensor,torch.scatter_add,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/77230,2022-05-11T04:11:59Z,fast `gradcheck` fails when outputs that do not require grad precede outputs that do,torch.linalg.slogdet,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/77067,2022-05-09T10:31:19Z,Unable to continue adding modules to `nn.Sequential` after using `del` method,torch.nn.Sequential,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/76798,2022-05-04T08:22:17Z,`gradcheck` for `torch.solve` may trigger INTERNAL ASSERT FAILED,torch.solve,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/76786,2022-05-04T02:35:22Z,"`cumprod, prod` will backward fail if `dtype` argument is different than the dtype of input tensor",torch.cumprod,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/76786,2022-05-04T02:35:22Z,"`cumprod, prod` will backward fail if `dtype` argument is different than the dtype of input tensor",torch.prod,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/76785,2022-05-04T02:31:27Z,"`addr, baddmm, dist, l1_loss` will backward fail when input tensors have different dtypes",torch.addr,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/76785,2022-05-04T02:31:27Z,"`addr, baddmm, dist, l1_loss` will backward fail when input tensors have different dtypes",torch.baddbmm,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/76785,2022-05-04T02:31:27Z,"`addr, baddmm, dist, l1_loss` will backward fail when input tensors have different dtypes",torch.dist,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/76785,2022-05-04T02:31:27Z,"`addr, baddmm, dist, l1_loss` will backward fail when input tensors have different dtypes",torch.nn.functional.l1_loss,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/76783,2022-05-04T02:02:09Z,`gradcheck` fails for `torch.trace`,torch.gradcheck,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/76783,2022-05-04T02:02:09Z,`gradcheck` fails for `torch.trace`,torch.trace,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/76780,2022-05-04T00:47:35Z,`gradcheck` should support the comparison of NaN,torch.autograd.gradcheck,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/76778,2022-05-04T00:28:15Z,`torch.addmv` backward fails,torch.addmv,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/76646,2022-05-02T02:42:47Z,`torch.nn.HuberLoss` backwards unexpectedly fail,torch.nn.HuberLoss,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/76646,2022-05-02T02:42:47Z,`torch.nn.HuberLoss` backwards unexpectedly fail,torch.nn.SmoothL1Loss,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/76644,2022-05-02T02:09:23Z,`torch.smm` backward fail with strange error message,torch.smm,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/76585,2022-04-29T11:02:16Z,`torch.linalg.cond` has different results for tensor requiring autograd,torch.linalg.cond,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/76522,2022-04-28T04:40:04Z,`torch.clamp` does not distribute gradients as element-wise`min/max` do,torch.clamp,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/76522,2022-04-28T04:40:04Z,`torch.clamp` does not distribute gradients as element-wise`min/max` do,torch.min,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/76522,2022-04-28T04:40:04Z,`torch.clamp` does not distribute gradients as element-wise`min/max` do,torch.max,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/76287,2022-04-24T15:38:37Z,torch.elastic fails to shutdown despite crashed processes,torch.distributed.elastic.multiprocessing.api,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/76012,2022-04-19T02:27:00Z,torch.nn.LayerNorm is very slow on GPU (much slower than a custom LayerNorm version in the ConvNext model),torch.nn.LayerNorm,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/75912,2022-04-15T20:21:57Z,"interaction with psychopy during imports, script exits with: free(): invalid pointer. Aborted (core dumped)",torch,1.11.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/75862,2022-04-15T08:04:15Z,LayerNorm and GroupNorm with num_groups=1 not equivalent,torch.nn.LayerNorm,1.11.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/75862,2022-04-15T08:04:15Z,LayerNorm and GroupNorm with num_groups=1 not equivalent,torch.nn.GroupNorm,1.11.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/75788,2022-04-14T09:50:58Z,`torch.jit.script` Script functions do return `requires_grad = False` if `torch.no_grad()` has been used,torch.jit.script,1.11.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/75788,2022-04-14T09:50:58Z,`torch.jit.script` Script functions do return `requires_grad = False` if `torch.no_grad()` has been used,torch.no_grad(),1.11.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/75733,2022-04-13T14:27:35Z,"`torch.sum, prod, cumsum, cumprod, sparse.sum` INTERNAL ASSERT FAIL",torch.sum,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/75733,2022-04-13T14:27:35Z,"`torch.sum, prod, cumsum, cumprod, sparse.sum` INTERNAL ASSERT FAIL",torch.prod,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/75733,2022-04-13T14:27:35Z,"`torch.sum, prod, cumsum, cumprod, sparse.sum` INTERNAL ASSERT FAIL",torch.cumsum,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/75733,2022-04-13T14:27:35Z,"`torch.sum, prod, cumsum, cumprod, sparse.sum` INTERNAL ASSERT FAIL",torch.cumprod,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/75733,2022-04-13T14:27:35Z,"`torch.sum, prod, cumsum, cumprod, sparse.sum` INTERNAL ASSERT FAIL",torch.sparse.sum,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/75427,2022-04-07T14:37:13Z,`torch.cuda.is_bf16_supported()` seem to not work properly,torch.cuda.is_bf16_supported(),1.11.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/75363,2022-04-06T21:39:33Z,conv3d has numerical issue where same input produces output that are not bit-wise identical,torch.ops.aten._convolution,1.11.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/75240,2022-04-05T00:45:49Z,Large cumulative sums appear to be nondeterministic.,torch.Tensor.cumsum,1.11.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/75198,2022-04-04T16:36:02Z,`torch.cuda.get_device_name` fails to identify RTX 3090 Ti,torch.cuda.get_device_name,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/75147,2022-04-02T18:00:03Z,Dataloader hangs. Potential deadlock with `set_num_threads` in worker processes?,torch.set_num_threads,1.11.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/75031,2022-03-31T18:51:32Z,comm hook error in BWD pass,torch.distributed,1.11.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/74909,2022-03-29T15:47:08Z,torch.package fails to import if dataclass and __future__.annotations present,torch.package,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/74809,2022-03-27T13:53:47Z,Incorrect results for `torch.distributed.gather` for tensor created from permuted NumPy array,torch.distributed.gather,1.11.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/74115,2022-03-11T18:17:58Z,DistributedDataParallel high peak memory usage with find_unused_parameters=True,torch.nn.parallel.DistributedDataParallel,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/74112,2022-03-11T17:26:42Z,torch.jit.script does not custom state_dicts,torch.jit.script,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/74095,2022-03-11T10:12:45Z,torchscript RNN modules cannot move between GPU,torch::jit::load,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/74095,2022-03-11T10:12:45Z,torchscript RNN modules cannot move between GPU,torch::flip,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/74095,2022-03-11T10:12:45Z,torchscript RNN modules cannot move between GPU,torch::lstm,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/74095,2022-03-11T10:12:45Z,torchscript RNN modules cannot move between GPU,torch::nn::modules::rnn,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/74058,2022-03-10T21:27:58Z,torch.fx.symbolic_trace is non-deterministic,torch.fx.symbolic_trace,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/73196,2022-02-21T21:26:48Z,`torch.pow` errors out on specific input,torch.pow,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/71784,2022-01-25T22:21:52Z,[JIT] addmm differs from eager mode,torch.addmm,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/70391,2021-12-24T13:28:38Z,linalg.lstsq INTERNAL ASSERT FAILED,torch.linalg.lstsq,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/70348,2021-12-23T12:33:40Z,nan return by nn.CrossEntropyLoss when all the labels are ignore_index in torch 1.11,torch.nn.CrossEntropyLoss,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/70102,2021-12-17T11:47:32Z,torch jit script segm fault,torch.jit.script,1.11.0,11.1
pytorch,https://github.com/pytorch/pytorch/issues/69972,2021-12-15T13:16:02Z,torchscript does not work with `SyncBatchNorm` layers,torch.nn.SyncBatchNorm,1.11.0,11.1
pytorch,https://github.com/pytorch/pytorch/issues/69972,2021-12-15T13:16:02Z,torchscript does not work with `SyncBatchNorm` layers,torch.jit.script,1.11.0,11.1
pytorch,https://github.com/pytorch/pytorch/issues/69912,2021-12-14T17:50:54Z,"torch.nn.functional.ctc_loss with invalid input produce NaN or infinity gradient, while the batch entries are fine",torch.nn.functional.ctc_loss,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/68337,2021-11-15T12:16:39Z,"`nn.functional.max_unpool{n}d`: shape checks fail with `output_size=(C, ...)`.",torch.nn.functional.max_pool1d,1.11.0,11
pytorch,https://github.com/pytorch/pytorch/issues/68337,2021-11-15T12:16:39Z,"`nn.functional.max_unpool{n}d`: shape checks fail with `output_size=(C, ...)`.",torch.nn.functional.max_unpool1d,1.11.0,11
pytorch,https://github.com/pytorch/pytorch/issues/66197,2021-10-06T12:12:49Z,`torch.fx.replace_pattern` doesn't work with untraceable wrapped functions,torch.fx.replace_pattern,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/52415,2021-02-18T01:35:26Z,torch.autograd.Function doesn't support non-Tensor outputs,torch.autograd.Function,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/50076,2021-01-04T23:29:10Z,resize_ documentation does not match implementation when memory_format is given,torch.resize_,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/44631,2020-09-14T09:14:46Z,Multi-process Dataloader and multi-parameter exceptions,torch.utils.data.DataLoader,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/87276,2022-10-19T09:43:09Z,torch.save throws ValueError: ctypes objects containing pointers cannot be pickled,torch.save,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/78486,2022-05-30T10:59:41Z,Exception in torch.jit.script doesn't indicate where in the code the problem lies.,torch.jit.script,1.11.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/101855,2023-05-19T05:15:11Z,torch.jit.trace() Floating point exception,torch.jit.trace,1.10.2,11.1
pytorch,https://github.com/pytorch/pytorch/issues/90261,2022-12-06T06:14:55Z,Why torch.mode return different value between CPU and GPU,torch.mode,1.10.2,10.2
pytorch,https://github.com/pytorch/pytorch/issues/85227,2022-09-18T11:59:41Z,topk returns different results with the same input in cuda and cpu,torch.topk,1.10.2,11.3
pytorch,https://github.com/pytorch/pytorch/issues/81554,2022-07-15T15:10:49Z,float' object is not callable when using scheduler.step() with MultiplicativeLR,torch.optim.lr_scheduler.MultiplicativeLR,1.10.2,11.3
pytorch,https://github.com/pytorch/pytorch/issues/79888,2022-06-20T19:08:37Z,`CosineAnnealingWarmRestarts` does not update parameters added with `add_param_group`,torch.optim.Optimizer.add_param_group,1.10.2,11.3
pytorch,https://github.com/pytorch/pytorch/issues/79888,2022-06-20T19:08:37Z,`CosineAnnealingWarmRestarts` does not update parameters added with `add_param_group`,torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,1.10.2,11.3
pytorch,https://github.com/pytorch/pytorch/issues/78435,2022-05-27T21:33:23Z,fx.Tracer with param_shapes_constant=True not working for RobertaForMaskedLM,torch.fx.Tracer,1.10.2,CPU
pytorch,https://github.com/pytorch/pytorch/issues/77050,2022-05-08T15:57:54Z,ENORMOUS OVERHEAD from mp.get_context('spawn'),torch.multiprocessing.get_context,1.10.2,CPU
pytorch,https://github.com/pytorch/pytorch/issues/76344,2022-04-25T22:06:43Z,"[ONNX] Use topk to export max(dim,keepdim) to onnx",torch.max,1.10.2,11.3
pytorch,https://github.com/pytorch/pytorch/issues/76344,2022-04-25T22:06:43Z,"[ONNX] Use topk to export max(dim,keepdim) to onnx",torch.topk,1.10.2,11.3
pytorch,https://github.com/pytorch/pytorch/issues/76344,2022-04-25T22:06:43Z,"[ONNX] Use topk to export max(dim,keepdim) to onnx",torch.onnx.export,1.10.2,11.3
pytorch,https://github.com/pytorch/pytorch/issues/75662,2022-04-12T12:43:52Z,"multiprocessing and torch.tensor, Cannot allocate memory error",torch.multiprocessing,1.10.2,CPU
pytorch,https://github.com/pytorch/pytorch/issues/73848,2022-03-07T09:22:50Z,torch.distributions.multinomial.Multinomial (an example mistake of docs)?,torch.distributions.multinomial.Multinomial,1.10.2,11.3
pytorch,https://github.com/pytorch/pytorch/issues/73661,2022-03-02T16:54:55Z,`torch.fx.ProxyableClassMeta` does not work if Proxy objects are not included in constructor arguments,torch.fx.ProxyableClassMeta,1.10.2,CPU
pytorch,https://github.com/pytorch/pytorch/issues/73568,2022-03-01T16:20:12Z,Bug when using `nn.Linear`,torch.nn.Linear,1.10.2,10.2
pytorch,https://github.com/pytorch/pytorch/issues/73515,2022-02-28T16:28:24Z,`torch.distributed.nn.functional.all_gather`: Tensors must be contiguous,torch.distributed.nn.functional.all_gather,1.10.2,11.3
pytorch,https://github.com/pytorch/pytorch/issues/73515,2022-02-28T16:28:24Z,`torch.distributed.nn.functional.all_gather`: Tensors must be contiguous,torch.distributed.nn.functional._AlltoAll,1.10.2,11.3
pytorch,https://github.com/pytorch/pytorch/issues/72835,2022-02-15T00:35:42Z,torch.distributed hangs at barrier(),torch.distributed,1.10.2,11.3
pytorch,https://github.com/pytorch/pytorch/issues/123082,2024-04-01T08:41:05Z,Torch.cdist calculation problem,torch.cdist,1.10.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/104602,2023-07-04T14:16:11Z,"F.adaptive_avg_pool3d(input, 1) returns infinity in half precision",torch.nn.functional.adaptive_avg_pool3d,1.10.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/101402,2023-05-15T12:05:58Z,DataParallel for nested modules,torch.nn.DataParallel,1.10.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/93846,2023-02-01T13:43:01Z,large number of temporary files generated when using dataloader with num_workers>0,torch.utils.data.DataLoader,1.10.1,11.1
pytorch,https://github.com/pytorch/pytorch/issues/75549,2022-04-09T01:52:41Z,`torch.linalg.lstsq` raises `CUBLAS_STATUS_EXECUTION_FAILED` for large `B` in CUDA tensors,torch.linalg.lstsq,1.10.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/75025,2022-03-31T16:57:06Z,torch.cuda.init() unstacks existing CUDA contexts,torch.cuda.init,1.10.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/73870,2022-03-07T20:32:16Z,max_pool1d() returns when given invalid large `kernel_size` inputs,torch.max_pool1d,1.10.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/73841,2022-03-06T22:18:14Z,max_pool1d(): `RuntimeError: [enforce fail at CPUAllocator.cpp:68] . DefaultCPUAllocator: can't allocate memory`,torch.max_pool1d,1.10.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/73840,2022-03-06T21:57:33Z,"max_pool1d(): argument 'dilation' must be tuple of ints, but found element of type int at pos 1",torch.max_pool1d,1.10.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/73504,2022-02-28T12:52:37Z,`ge` and `div` behaves differently when converting an overflow number,torch.div,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/73504,2022-02-28T12:52:37Z,`ge` and `div` behaves differently when converting an overflow number,torch.ge,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/73501,2022-02-28T11:15:12Z,`index_copy` has different index behavior with `index_fill`,torch.index_copy,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/73501,2022-02-28T11:15:12Z,`index_copy` has different index behavior with `index_fill`,torch.index_fill,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/73154,2022-02-20T10:20:12Z,`max_unpool2d` returns a tensor with negative dimension,torch.nn.functional.max_unpool2d,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/72911,2022-02-16T12:13:19Z,Vectorized Jacobian and Hessian errors with ffts,torch.autograd.functional.hessian,1.10.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/72911,2022-02-16T12:13:19Z,Vectorized Jacobian and Hessian errors with ffts,torch.fft.irfft,1.10.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/72911,2022-02-16T12:13:19Z,Vectorized Jacobian and Hessian errors with ffts,torch.fft.rfft,1.10.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/72911,2022-02-16T12:13:19Z,Vectorized Jacobian and Hessian errors with ffts,torch.autograd.functional.jacobian,1.10.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/72911,2022-02-16T12:13:19Z,Vectorized Jacobian and Hessian errors with ffts,torch.fft.irfft,1.10.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/72911,2022-02-16T12:13:19Z,Vectorized Jacobian and Hessian errors with ffts,torch.fft.rfft,1.10.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/72821,2022-02-14T23:43:38Z,"`{Batch,Instance}Norm{1,2,3}d` works when `num_features != C`!",torch.nn.BatchNorm2d,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/72821,2022-02-14T23:43:38Z,"`{Batch,Instance}Norm{1,2,3}d` works when `num_features != C`!",torch.nn.InstanceNorm2d,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/72821,2022-02-14T23:43:38Z,"`{Batch,Instance}Norm{1,2,3}d` works when `num_features != C`!",torch.nn.LazyBatchNorm2d,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/72525,2022-02-08T18:00:16Z,KL divergence between two Continuous Bernoulli is negative,torch.distributions.kl_divergence,1.10.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/72525,2022-02-08T18:00:16Z,KL divergence between two Continuous Bernoulli is negative,torch.distributions.ContinuousBernoulli,1.10.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/72341,2022-02-04T17:55:53Z,InstanceNorm doesn't preserve memory format,torch.nn.InstanceNorm2d,1.10.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/71689,2022-01-24T02:08:52Z,torch.distributions.categorical.Categorical does not work with 0 batch size,torch.distributions.categorical.Categorical,1.10.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/71636,2022-01-21T14:09:14Z,`torch.median` will return -2147483648 when input is an empty tensor,torch.median,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/71635,2022-01-21T14:03:24Z,"`torch.nn.functional.{instance, batch}_norm` trigger INTERNAL ASSERT FAILED when input is empty tensor with `complex32`",torch.nn.functional.instance_norm,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/71635,2022-01-21T14:03:24Z,"`torch.nn.functional.{instance, batch}_norm` trigger INTERNAL ASSERT FAILED when input is empty tensor with `complex32`",torch.nn.functional.batch_norm,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/71629,2022-01-21T06:37:39Z,"`torch.{max,min}` have strange error message when `input.numel()==0`",torch.max,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/71629,2022-01-21T06:37:39Z,"`torch.{max,min}` have strange error message when `input.numel()==0`",torch.min,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/71629,2022-01-21T06:37:39Z,"`torch.{max,min}` have strange error message when `input.numel()==0`",torch.median,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/71541,2022-01-20T06:23:00Z,"`torch.sub` behaves differently with `add`, `mul`, `div`",torch.sub,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/71541,2022-01-20T06:23:00Z,"`torch.sub` behaves differently with `add`, `mul`, `div`",torch.add,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/71541,2022-01-20T06:23:00Z,"`torch.sub` behaves differently with `add`, `mul`, `div`",torch.mul,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/71541,2022-01-20T06:23:00Z,"`torch.sub` behaves differently with `add`, `mul`, `div`",torch.div,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/71477,2022-01-19T15:51:54Z,"`torch.cum{min,max}, torch.sort, argsort` do not check the `dim` when the input is 0-d tensor",torch.cummin,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/71477,2022-01-19T15:51:54Z,"`torch.cum{min,max}, torch.sort, argsort` do not check the `dim` when the input is 0-d tensor",torch.cummax,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/71477,2022-01-19T15:51:54Z,"`torch.cum{min,max}, torch.sort, argsort` do not check the `dim` when the input is 0-d tensor",torch.sort,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/71477,2022-01-19T15:51:54Z,"`torch.cum{min,max}, torch.sort, argsort` do not check the `dim` when the input is 0-d tensor",torch.argsort,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/71204,2022-01-12T08:41:06Z,`torch.diag` unexpectedly fails,torch.diag,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/71204,2022-01-12T08:41:06Z,`torch.diag` unexpectedly fails,torch.diagonal,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/71084,2022-01-10T08:38:16Z,"`torch.{inverse,cholesky}` have wrong shape check of square matrices",torch.inverse,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/71084,2022-01-10T08:38:16Z,"`torch.{inverse,cholesky}` have wrong shape check of square matrices",torch.cholesky,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/71082,2022-01-10T08:28:08Z,`torch.combinations` will allocate large memory when `r` is greater than the length of input,torch.combinations,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/71078,2022-01-10T06:04:25Z,"`torch.nn.{Constant,Zero}Pad` unexpectedly fail",torch.nn.ConstantPad2d,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/71078,2022-01-10T06:04:25Z,"`torch.nn.{Constant,Zero}Pad` unexpectedly fail",torch.nn.ZeroPad2d,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/71059,2022-01-08T15:37:31Z,`torch.scatter` will return random value when `input` is empty tensor,torch.scatter,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/71058,2022-01-08T15:18:11Z,`torch.Tensor.where` cannot work when `y` is float,torch.Tensor.where,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/70511,2021-12-30T02:55:26Z,Strange behavior of torch.jit.trace when moving parameters across device,torch.jit.trace,1.10.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/70398,2021-12-25T07:21:20Z,`torch.broadcast_to` can create tensor with negative dimension.,torch.broadcast_to,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/70398,2021-12-25T07:21:20Z,`torch.broadcast_to` can create tensor with negative dimension.,torch.expand,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/70397,2021-12-25T04:58:50Z,`torch.empty_strided` works when the stride is negative!,torch.empty_strided,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/70352,2021-12-23T16:11:58Z,Rprop Optimizer: UnboundLocalError: local variable 'step_size_min' referenced before assignment,torch.optim.Rprop.step,1.10.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/70347,2021-12-23T11:59:04Z,boolean mask + ellipsis lead to incorrect indexing,torch.BoolTensor,1.10.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/70047,2021-12-16T14:13:29Z,[JIT] Cannot `jit.export` a `@staticmethod`,torch.jit.export,1.10.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/70047,2021-12-16T14:13:29Z,[JIT] Cannot `jit.export` a `@staticmethod`,torch.jit.script,1.10.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/130515,2024-07-11T03:12:10Z,[Bug Report] inconsistent global src and local rank in dist.nn.functional.broadcast,torch.distributed.nn.functional._Broadcast,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/116175,2023-12-20T07:04:00Z,Performance for `TransformerEncoderLayer` and `TransformerDecoderLayer` drops severely if pytorch version changed from `pt1.10` to `pt2.0`,torch.nn.TransformerEncoderLayer,1.10.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/116175,2023-12-20T07:04:00Z,Performance for `TransformerEncoderLayer` and `TransformerDecoderLayer` drops severely if pytorch version changed from `pt1.10` to `pt2.0`,torch.nn.TransformerDecoderLayer,1.10.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/110084,2023-09-26T16:24:02Z,scatter_add: Mixing 0-dim and 1-dim tensors,torch.scatter_add,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/102109,2023-05-23T09:30:31Z,Can't vmap over a slice expression,torch.vmap,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/101666,2023-05-17T03:53:35Z,Unexpected behavior of fmod op in some float32 input,torch.fmod,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/91535,2022-12-30T10:24:23Z,"When dist.broadcast float32 to int64, it will silently generate wrong results",torch.distributed.broadcast,1.10.0,11.3.1
pytorch,https://github.com/pytorch/pytorch/issues/87745,2022-10-26T00:48:00Z,TorchDynamo: there has a accuracy issue for conv+unary(binary) post ops for gpu path,torch._inductor.codegen.triton,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/84630,2022-09-07T12:47:37Z,"torch.nn.functional.interpolate fails on some degenerate shapes, but passes on others",torch.nn.functional.interpolate,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/80022,2022-06-22T09:24:49Z,memory leaking when doing all_to_all_single communication,torch.all_to_all_single,1.10.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/78075,2022-05-23T10:40:18Z,torch.multiprocessing.spawn raise PicklingError inside a decorator,torch.multiprocessing.spawn,1.10.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/77821,2022-05-19T02:45:17Z,cannot convert to channels last format for conv2d conv3d hybrid model,torch.channels_last,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/77821,2022-05-19T02:45:17Z,cannot convert to channels last format for conv2d conv3d hybrid model,torch.channels_last_3d,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/76528,2022-04-28T10:49:49Z,Discrepancy in einsum when done in batch vs non-batch,torch.einsum,1.10.0,11.1
pytorch,https://github.com/pytorch/pytorch/issues/76069,2022-04-19T21:10:02Z,`init_process_group` hanging on HPC multi-node system w GPU,torch.distributed.init_process_group,1.10.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/74842,2022-03-28T17:58:30Z,error in quantization by quantize_fx.prepare_fx,torch.quantization.quantize_fx.prepare_fx,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/74301,2022-03-16T15:11:14Z,Error in lobpcg when using largest=False,torch.lobpcg,1.10.0,11.1
pytorch,https://github.com/pytorch/pytorch/issues/74146,2022-03-13T11:56:36Z,RuntimeError: Connection reset by peer when backened by NCCL,torch.distributed.init_process_group,1.10.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/74034,2022-03-10T14:26:04Z,[torch.onnx] ONNX export failed on adaptive_avg_pool2d because input size not accessible not supported,torch.onnx,1.10.0,11.1
pytorch,https://github.com/pytorch/pytorch/issues/74034,2022-03-10T14:26:04Z,[torch.onnx] ONNX export failed on adaptive_avg_pool2d because input size not accessible not supported,torch.onnx.utils,1.10.0,11.1
pytorch,https://github.com/pytorch/pytorch/issues/74034,2022-03-10T14:26:04Z,[torch.onnx] ONNX export failed on adaptive_avg_pool2d because input size not accessible not supported,torch.onnx.symbolic_helper,1.10.0,11.1
pytorch,https://github.com/pytorch/pytorch/issues/74034,2022-03-10T14:26:04Z,[torch.onnx] ONNX export failed on adaptive_avg_pool2d because input size not accessible not supported,torch.adaptive_avg_pool2d,1.10.0,11.1
pytorch,https://github.com/pytorch/pytorch/issues/73608,2022-03-01T23:09:45Z,`torch.jit.load` fails when function parameters use non-ASCII characters,torch.jit.load,1.10.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/72498,2022-02-08T06:21:36Z,torch.jit.script + torch.split + onnxruntime incompatibility,torch.jit.script,1.10.0,11.2
pytorch,https://github.com/pytorch/pytorch/issues/72498,2022-02-08T06:21:36Z,torch.jit.script + torch.split + onnxruntime incompatibility,torch.split,1.10.0,11.2
pytorch,https://github.com/pytorch/pytorch/issues/72498,2022-02-08T06:21:36Z,torch.jit.script + torch.split + onnxruntime incompatibility,torch.onnx.export,1.10.0,11.2
pytorch,https://github.com/pytorch/pytorch/issues/71288,2022-01-13T23:51:05Z,`memory_format` argument isn't supported in torchscript for tensor.is_contiguous(),torch.is_contiguous,1.10.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/71222,2022-01-12T17:16:23Z,torch.linalg.lstsq is nondeterministic,torch.linalg.lstsq,1.10.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/70954,2022-01-06T23:32:39Z,Training grouped Conv2D is slow,torch.nn.Conv2d,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/70388,2021-12-24T11:44:03Z,RuntimeError: tensor has too many (>25) dims when permuting tensor with GPU backend,torch.permute,1.10.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/70388,2021-12-24T11:44:03Z,RuntimeError: tensor has too many (>25) dims when permuting tensor with GPU backend,torch.tensordot,1.10.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/70386,2021-12-24T07:14:34Z,AT_ASSERT fail with DataLoaderOptions().drop_last(),torch.DataLoaderOptions.drop_last,1.10.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/69931,2021-12-14T21:56:39Z,[JIT] torch.exp roughly 20 times slower in TorchScript vs. PyTorch,torch.exp,1.10.0,11.2
pytorch,https://github.com/pytorch/pytorch/issues/69435,2021-12-05T19:52:10Z,"torch.is_tensor(obj) doesn't work with JIT, despite the fact that isinstance(obj, Tensor) already works with JIT",torch.is_tensor,1.10.0,11.1
pytorch,https://github.com/pytorch/pytorch/issues/69433,2021-12-05T14:34:42Z,`torch.transpose` should raise an error when indexing 0 for 0 dimensional tensor.,torch.transpose,1.10.0,11.1
pytorch,https://github.com/pytorch/pytorch/issues/69408,2021-12-04T02:04:13Z,`torch.hstack` should raise an error when tensor is 0 dimensional,torch.hstack,1.10.0,11.1
pytorch,https://github.com/pytorch/pytorch/issues/69353,2021-12-03T08:04:23Z,Pruning `torch.nn.MultiheadAttention` causes RuntimeError,torch.nn.MultiheadAttention,1.10.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/69353,2021-12-03T08:04:23Z,Pruning `torch.nn.MultiheadAttention` causes RuntimeError,torch.nn.utils.prune.global_unstructured,1.10.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/69348,2021-12-03T06:55:32Z,`torch.sspaddmm` should broadcast the input tensor,torch.sspaddmm,1.10.0,11.1
pytorch,https://github.com/pytorch/pytorch/issues/69203,2021-12-01T09:04:14Z,Error in SVD cusolver on Linux,torch.linalg.svd,1.10.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/69203,2021-12-01T09:04:14Z,Error in SVD cusolver on Linux,torch.linalg.svdvals,1.10.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/68978,2021-11-29T09:02:48Z,SequentialLR cannot be used with ReduceLROnPlateau due to .step() not allowing for optional arguments,torch.SequentialLR,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/68978,2021-11-29T09:02:48Z,SequentialLR cannot be used with ReduceLROnPlateau due to .step() not allowing for optional arguments,torch.ReduceLROnPlateau,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/68595,2021-11-18T16:16:21Z,Inconsistent list indexing behavior,torch.Tensor,1.10.0,11.5
pytorch,https://github.com/pytorch/pytorch/issues/68513,2021-11-17T08:15:05Z,torch.fx cannot trace torch.Size() properly,torch.fx,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/68513,2021-11-17T08:15:05Z,torch.fx cannot trace torch.Size() properly,torch.Tensor.size,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/68513,2021-11-17T08:15:05Z,torch.fx cannot trace torch.Size() properly,torch.zeros,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/68513,2021-11-17T08:15:05Z,torch.fx cannot trace torch.Size() properly,torch.Tensor.view,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/68323,2021-11-14T10:42:09Z,sparse.mm: CUDA error: internal error when calling `cusparseSpGEMM_workEstimation [...]`,torch.sparse.mm,1.10.0,11.4
pytorch,https://github.com/pytorch/pytorch/issues/68301,2021-11-13T00:03:32Z,[FX] [BUG] Tensor.{inplace_method}_(.) is eliminated as dead code,torch.Tensor.mul_,1.10.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/68169,2021-11-11T10:48:40Z,torch.histogram: Sum of PDFs is num of bins instead of 1,torch.histogram,1.10.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/67983,2021-11-08T06:31:16Z,torch.jit.export does not working,torch.jit.export,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/67740,2021-11-03T03:16:50Z,`torch.utils.data.random_split` example broken in 1.10,torch.utils.data.random_split,1.10.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/67462,2021-10-28T19:20:47Z,Jacobian mismatch for `nn.functional.ctc_loss`,torch.nn.functional.ctc_loss,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/67324,2021-10-27T08:20:24Z,torch.stft - fill_cuda not implemented for ComplexHalf,torch.stft,1.10.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/65400,2021-09-21T12:23:02Z,torch.sparse.sum on scalar sparse tensor fails when dim is specified,torch.sparse.sum,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/65392,2021-09-21T07:36:12Z,torch.sparse.sum result has wrong dtype when reducing over all dimensions,torch.sparse.sum,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/64107,2021-08-27T19:48:12Z,as_tensor and negative strided np arrays,torch.as_tensor,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/64079,2021-08-27T03:26:49Z,torch.equal does not support sparse tensors,torch.equal,1.10.0,11.1
pytorch,https://github.com/pytorch/pytorch/issues/63802,2021-08-23T20:46:07Z,[torch.distributed.launch|run] Hangs on SIGINT when using a TCPStore backed rdzv_backend,torch.distributed.launch,1.10.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/63802,2021-08-23T20:46:07Z,[torch.distributed.launch|run] Hangs on SIGINT when using a TCPStore backed rdzv_backend,torch.distributed.run,1.10.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/63034,2021-08-10T18:36:57Z,[bug] nn.functional.pad (circular) ubsan failure,torch.nn.functional.pad,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/62545,2021-08-01T07:31:00Z,Incosistency with args for `nn.functional.max_poolNd` vs `nn.MaxPoolNd` functions,torch.nn.functional.max_pool2d,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/62545,2021-08-01T07:31:00Z,Incosistency with args for `nn.functional.max_poolNd` vs `nn.MaxPoolNd` functions,torch.nn.MaxPool2d,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/62508,2021-07-30T21:41:32Z,torch.prod internal asserts when passed a tensor that requires_grad (and a dtype),torch.prod,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/62034,2021-07-22T16:45:33Z,cudnn_batch_norm_backward is extremely imprecise for some input shapes,torch.nn.BatchNorm2d,1.10.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/59256,2021-06-01T19:04:38Z,ProcessGroupGloo creation crashes when world_size > 150,torch.distributed.init_process_group,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/58841,2021-05-24T08:50:13Z,[bug] torch.topk sometimes supports `float16` and sometimes doesn't,torch.topk,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/56440,2021-04-20T02:03:37Z,Raise exception when torch.clamp min value underflows the input tensor's dtype,torch.clamp,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/54815,2021-03-27T00:19:59Z,TensorExpr `LoopNest.get_loops_for` misbehaved after loop distribution transformation,torch._C._te.LoopNest.get_loops_for,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/53649,2021-03-09T21:14:21Z,vmap gradgradcheck test fails for unfold operation,torch.grad,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/53649,2021-03-09T21:14:21Z,vmap gradgradcheck test fails for unfold operation,torch.vmap,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/53649,2021-03-09T21:14:21Z,vmap gradgradcheck test fails for unfold operation,torch.autograd.gradcheck,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/53649,2021-03-09T21:14:21Z,vmap gradgradcheck test fails for unfold operation,torch.autograd.grad,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/53649,2021-03-09T21:14:21Z,vmap gradgradcheck test fails for unfold operation,torch.autograd.gradgradcheck,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/51622,2021-02-03T07:10:28Z,"`fractional_max_pool{2, 3}d` inconsistent between CUDA and CPU",torch.nn.functional.fractional_max_pool2d,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/51622,2021-02-03T07:10:28Z,"`fractional_max_pool{2, 3}d` inconsistent between CUDA and CPU",torch.nn.functional.fractional_max_pool3d,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/40914,2020-07-02T13:33:28Z,Inconsistent handling of torch.Size.__add__,torch.Size.__add__,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/38640,2020-05-18T09:51:46Z,Maxunpool seems to give a weird error message,torch.MaxUnpool2d,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/31779,2020-01-02T17:25:49Z,torch.poisson returns floating point tensor,torch.poisson,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/29843,2019-11-14T21:44:09Z,torch.distributions.normal.Normal is not JIT supported,torch.distributions.normal.Normal,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/125728,2024-05-08T01:01:04Z,"torch._inductor.config.max_autotune_gemm_backends = ""TRITON"" crashes with Convolution layer",torch._inductor.config.max_autotune_gemm_backends,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/125728,2024-05-08T01:01:04Z,"torch._inductor.config.max_autotune_gemm_backends = ""TRITON"" crashes with Convolution layer",torch._inductor.config.max_autotune,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/125728,2024-05-08T01:01:04Z,"torch._inductor.config.max_autotune_gemm_backends = ""TRITON"" crashes with Convolution layer",torch.compile,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130826,2024-07-16T09:07:00Z,models `.forward` and exported onnx are not the same,torch.nn.Module.forward,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/130826,2024-07-16T09:07:00Z,models `.forward` and exported onnx are not the same,torch.onnx.export,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/125984,2024-05-11T01:07:45Z,`torch.export` code that includes `torch.autograd.grad`,torch.export,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/125984,2024-05-11T01:07:45Z,`torch.export` code that includes `torch.autograd.grad`,torch.autograd.grad,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/125984,2024-05-11T01:07:45Z,`torch.export` code that includes `torch.autograd.grad`,torch._export.aot_compile,2.3.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/125749,2024-05-08T09:22:35Z,MultiHeadAttention.attention' is being compiled since it was called from 'MultiHeadAttention.forward',torch.jit.script,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/125749,2024-05-08T09:22:35Z,MultiHeadAttention.attention' is being compiled since it was called from 'MultiHeadAttention.forward',torch.jit._recursive.create_script_module,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/125749,2024-05-08T09:22:35Z,MultiHeadAttention.attention' is being compiled since it was called from 'MultiHeadAttention.forward',torch.jit.RecursiveScriptModule._construct,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/125749,2024-05-08T09:22:35Z,MultiHeadAttention.attention' is being compiled since it was called from 'MultiHeadAttention.forward',torch.jit._recursive.create_script_module_impl,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/125749,2024-05-08T09:22:35Z,MultiHeadAttention.attention' is being compiled since it was called from 'MultiHeadAttention.forward',torch.jit._recursive.create_methods_and_properties_from_stubs,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/125749,2024-05-08T09:22:35Z,MultiHeadAttention.attention' is being compiled since it was called from 'MultiHeadAttention.forward',torch.jit._recursive.compile_unbound_method,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/125749,2024-05-08T09:22:35Z,MultiHeadAttention.attention' is being compiled since it was called from 'MultiHeadAttention.forward',MultiHeadAttention.attention,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/125749,2024-05-08T09:22:35Z,MultiHeadAttention.attention' is being compiled since it was called from 'MultiHeadAttention.forward',MultiHeadAttention.forward,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128503,2024-06-12T11:27:55Z,crash@sleef_tryVXE2 () while trying to run torch.compile() BERT model,torch.compile,2.2.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/120791,2024-02-28T11:01:01Z,about tensor.new(storage) issue,torch.tensor.new,2.1.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/115640,2023-12-12T16:48:53Z,"Quantisation results in a ""Illegal instruction (core dumped)""",torch.quantization.quantize_dynamic,2.1.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123072,2024-04-01T05:29:55Z,Saving & Loading Model Across [In-tree and out-of-tree] Devices,torch.load,2.1.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/116182,2023-12-20T10:30:00Z,[ONNX] GFPGANv1.pth to onnx conversion error,torch.onnx.export,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/113809,2023-11-15T22:07:42Z,"Error during DDP, torch.compile, and cudagraph_trees",torch.compile,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/113809,2023-11-15T22:07:42Z,"Error during DDP, torch.compile, and cudagraph_trees",torch._dynamo,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/113809,2023-11-15T22:07:42Z,"Error during DDP, torch.compile, and cudagraph_trees",torch._inductor.cudagraph_trees,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/127324,2024-05-28T18:51:32Z,torch.compile reorder_for_compute_comm_overlap sink_waits pass does not work,torch.compile,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/127324,2024-05-28T18:51:32Z,torch.compile reorder_for_compute_comm_overlap sink_waits pass does not work,torch.distributed.init_process_group,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/127324,2024-05-28T18:51:32Z,torch.compile reorder_for_compute_comm_overlap sink_waits pass does not work,torch.distributed._functional_collectives.all_reduce,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/127324,2024-05-28T18:51:32Z,torch.compile reorder_for_compute_comm_overlap sink_waits pass does not work,torch.cuda.set_device,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/127324,2024-05-28T18:51:32Z,torch.compile reorder_for_compute_comm_overlap sink_waits pass does not work,torch.ops._c10d_functional.all_reduce_,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/127324,2024-05-28T18:51:32Z,torch.compile reorder_for_compute_comm_overlap sink_waits pass does not work,torch.ops._c10d_functional.wait_tensor,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/127324,2024-05-28T18:51:32Z,torch.compile reorder_for_compute_comm_overlap sink_waits pass does not work,torch._inductor.config.reorder_for_compute_comm_overlap,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/125956,2024-05-10T20:26:36Z,scalar_tensor call with symbolic bool input does not work in inductor,torch.aten.scalar_tensor.default,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/121386,2024-03-07T06:26:01Z,torch.compile + ring attention,torch.compile,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/121386,2024-03-07T06:26:01Z,torch.compile + ring attention,torch._dynamo,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/121386,2024-03-07T06:26:01Z,torch.compile + ring attention,torch.nn.modules.module,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/121386,2024-03-07T06:26:01Z,torch.compile + ring attention,torch.nn.parallel.distributed,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/121386,2024-03-07T06:26:01Z,torch.compile + ring attention,torch.distributed,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117957,2024-01-22T02:15:44Z,SDPA accuracy issue for CUDA (Disable temporarily),torch._dynamo.utils,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117957,2024-01-22T02:15:44Z,SDPA accuracy issue for CUDA (Disable temporarily),torch.testing._internal.common_distributed,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117957,2024-01-22T02:15:44Z,SDPA accuracy issue for CUDA (Disable temporarily),torch.testing._internal.common_utils,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/117957,2024-01-22T02:15:44Z,SDPA accuracy issue for CUDA (Disable temporarily),torch._inductor.fx_passes.fuse_attention,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/116350,2023-12-23T03:26:49Z,RuntimeError: derivative for aten::_scaled_dot_product_flash_attention_backward is not implemented,torch._scaled_dot_product_flash_attention_backward,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/113707,2023-11-14T22:47:14Z,torch.compile doesnt respect use_determistic_algorithms during the backward(),torch.compile,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/113707,2023-11-14T22:47:14Z,torch.compile doesnt respect use_determistic_algorithms during the backward(),torch.use_deterministic_algorithms,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/113707,2023-11-14T22:47:14Z,torch.compile doesnt respect use_determistic_algorithms during the backward(),torch.nn.ReflectionPad2d,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/113707,2023-11-14T22:47:14Z,torch.compile doesnt respect use_determistic_algorithms during the backward(),aten.replication_pad2d_backward,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/112256,2023-10-27T16:21:01Z,[ONNX] In-place additon not being functionalized by torch.onnx.dynamo_export,torch.onnx.dynamo_export,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/107175,2023-08-14T21:05:05Z,"sdp_kernel causes dynamo error on torch.compile(model, fullgraph=True)",torch.backends.cuda.sdp_kernel,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/106637,2023-08-04T21:44:48Z,Using retain_graph in backward() with FSDP,torch.distributed.fsdp.FullyShardedDataParallel,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/106220,2023-07-28T20:21:10Z,torch compile does not work with torch.nn.functional.softmax ?,torch.nn.functional.softmax,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/101249,2023-05-12T02:59:52Z,`einsum` is about 40x slower on CUDA than manually multiplying and summing,torch.einsum,2.0.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/93515,2023-01-21T01:16:11Z,Dynamo can not trace 'int(a_scalar_tensor.item())',torch._dynamo.export,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/93604,2022-11-02T18:28:13Z,TorchBench - moco - RuntimeError: Tensors must be CUDA and dense,torch.distributed.all_gather,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/54082,2021-03-16T19:13:14Z,Dispatch table for linalg_norm is fishy,torch.linalg.norm,2.0.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/95434,2023-02-24T01:50:57Z,It seems that `torch.Tensor.addmv` and `torch.Tensor.addr` will check some inputs' dtype if and only if in `backward()`,torch.Tensor.addr,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/95434,2023-02-24T01:50:57Z,It seems that `torch.Tensor.addmv` and `torch.Tensor.addr` will check some inputs' dtype if and only if in `backward()`,torch.Tensor.addmv,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/123147,2024-04-02T03:16:53Z,Tracker for Adagrad+SGD sparse tests failing with dynamo,torch.optim.SGD,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/123147,2024-04-02T03:16:53Z,Tracker for Adagrad+SGD sparse tests failing with dynamo,torch.optim.Adagrad,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/122840,2024-03-27T23:00:02Z,a log_softmax kernel get much worse perf with padding,torch.log_softmax,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/121261,2024-03-05T04:05:44Z,Convolution and norm with vmap jacobian failing,torch.func.jacfwd,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/121261,2024-03-05T04:05:44Z,Convolution and norm with vmap jacobian failing,torch.vmap,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/121261,2024-03-05T04:05:44Z,Convolution and norm with vmap jacobian failing,torch.nn.Conv1d,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/121261,2024-03-05T04:05:44Z,Convolution and norm with vmap jacobian failing,torch.nn.LayerNorm,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/121261,2024-03-05T04:05:44Z,Convolution and norm with vmap jacobian failing,torch.nn.InstanceNorm,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/121261,2024-03-05T04:05:44Z,Convolution and norm with vmap jacobian failing,torch.nn.BatchNorm,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/121261,2024-03-05T04:05:44Z,Convolution and norm with vmap jacobian failing,torch.functional.normalise,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/114044,2023-11-19T02:48:24Z,Nan on torch.corrcoef(x.t()),torch.corrcoef,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/104933,2023-07-08T03:37:16Z,torch.func.jvp fails with BERT training,torch.func.jvp,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/98002,2023-03-30T18:53:40Z,DataLoader with collate_fn that returns tensors in GPU memory raises warnings when deleted,torch.utils.data.DataLoader,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/91879,2023-01-09T12:01:48Z,ddp vs fsdp,torch.ddp,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/91879,2023-01-09T12:01:48Z,ddp vs fsdp,torch.fsdp,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/91879,2023-01-09T12:01:48Z,ddp vs fsdp,torch.amp,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/91879,2023-01-09T12:01:48Z,ddp vs fsdp,torch.cuda,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/91879,2023-01-09T12:01:48Z,ddp vs fsdp,torch.bfloat16,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/91879,2023-01-09T12:01:48Z,ddp vs fsdp,torch.cuda.current_device,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/84864,2022-09-12T15:13:00Z,torch.utils.checkpoint (with use_reentrant=False) doesn't work with all PyTorch features that set TLS,torch.utils.checkpoint,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/84864,2022-09-12T15:13:00Z,torch.utils.checkpoint (with use_reentrant=False) doesn't work with all PyTorch features that set TLS,torch.utils._python_dispatch.TorchDispatchMode,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/76806,2022-05-04T10:03:31Z,torch.Tensor.__rdiv__ long x scalar float type promotion is incorrect,torch.Tensor.__rdiv__,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/73638,2022-03-02T08:10:43Z,addcdiv is failing the ASAN test for zero divisors,torch.addcdiv,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/71774,2022-01-25T18:11:30Z,matmul returns uninitialized memory for int64 tensors with inner dimension of zero,torch.matmul,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/71774,2022-01-25T18:11:30Z,matmul returns uninitialized memory for int64 tensors with inner dimension of zero,torch.__rmatmul__,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/71774,2022-01-25T18:11:30Z,matmul returns uninitialized memory for int64 tensors with inner dimension of zero,torch.addmm_decomposed,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/71774,2022-01-25T18:11:30Z,matmul returns uninitialized memory for int64 tensors with inner dimension of zero,torch.linalg_multi_dot,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/69831,2021-12-13T12:26:06Z,Incorrect error for integer `out=` dtypes when a float is expected.,torch.linalg.svd,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/64794,2021-09-10T02:21:26Z,linalg.lstsq out variant fails internal assert because it uses non-inplace view op for some inputs,torch.linalg.lstsq,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/50034,2021-01-04T07:05:29Z,Multinomial without replacement produces samples that have zero probability,torch.multinomial,1.13.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/104776,2023-07-07T15:41:47Z,torch.nn.CrossEntropyLoss: class weighting changes label_smoothing,torch.nn.CrossEntropyLoss,1.12.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/90057,2022-12-02T13:12:54Z,[GradScaler] Inconsistent scale values across different GPUs caused by uneven inputs for AMP DDP training,torch.cuda.amp.GradScaler,1.12.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/89684,2022-11-25T13:47:59Z,`positive_semidefinite` constraint fails on CUDA 11.7,torch.distributions.constraints.positive_semidefinite,1.12.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/89684,2022-11-25T13:47:59Z,`positive_semidefinite` constraint fails on CUDA 11.7,torch.linalg.eigvalsh,1.12.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/87964,2022-10-28T05:45:27Z,`torch.distributed` crash with abort only inside if,torch.distributed.FileStore,1.12.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/87170,2022-10-18T04:51:53Z,torch.linalg.cond gives inconsistent results on CPU/CUDA,torch.linalg.cond,1.12.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/87131,2022-10-17T21:04:20Z,torch.clamp does not clamp out of -0 from 0 when ran on the CPU,torch.clamp,1.12.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/83494,2022-08-16T01:57:34Z,`torch.pinverse` produces wrong output!,torch.pinverse,1.12.1,CPU
pytorch,https://github.com/pytorch/pytorch/issues/114080,2023-11-20T00:23:04Z,torch.matrix_exp(x) get inf and nan,torch.matrix_exp,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/94304,2023-02-07T12:35:54Z,"RuntimeError: p.block != nullptr && p.block->ptr != nullptr INTERNAL ASSERT FAILED at ""../c10/cuda/CUDACachingAllocator.cpp"":1275, please report a bug to PyTorch.",torch.load,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/82610,2022-08-01T21:34:03Z,contiguous() not work for rank 1 length 1 tensor.,torch.tensor,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/82610,2022-08-01T21:34:03Z,contiguous() not work for rank 1 length 1 tensor.,torch.tensor().real,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/82610,2022-08-01T21:34:03Z,contiguous() not work for rank 1 length 1 tensor.,torch.tensor().real.contiguous,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/82610,2022-08-01T21:34:03Z,contiguous() not work for rank 1 length 1 tensor.,torch.tensor().real.contiguous().stride,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/70559,2022-01-02T18:01:59Z,Behavior of torch.nn.functional.interpolate with unchanged output size and recompute_scale_factor=False,torch.nn.functional.interpolate,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/58037,2021-05-11T11:05:55Z,linalg.eigh and linalg.cholesky UPLO flag breaks in backward,torch.linalg.eigh,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/58037,2021-05-11T11:05:55Z,linalg.eigh and linalg.cholesky UPLO flag breaks in backward,torch.linalg.cholesky,1.12.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/97030,2023-03-17T16:21:33Z,DDP static graph fails for static model,torch.nn.parallel.DistributedDataParallel,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/42663,2020-08-06T07:42:11Z,Why Conv3D is slower than Conv2D when its flops is smaller than Conv2D,torch.nn.Conv2d,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/42663,2020-08-06T07:42:11Z,Why Conv3D is slower than Conv2D when its flops is smaller than Conv2D,torch.nn.Conv3d,1.11.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/88474,2022-11-04T03:19:18Z,Mixed precision training fails due to NaN in batch norm running_mean,torch.cuda.amp.autocast,1.10.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/76025,2022-04-19T09:21:30Z,Numerical instability: matrix multiplication got different results on cpu and gpu,torch.tensor,1.10.1,11.3.1
pytorch,https://github.com/pytorch/pytorch/issues/70701,2022-01-05T15:14:11Z,AdaptiveAvgPool1d - RuntimeError: shmem_size <= sharedMemPerBlockINTERNAL ASSERT FAILED,torch.AdaptiveAvgPool1d,1.10.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/114085,2023-11-20T01:05:07Z,inconsistency in torch.Tensor.scatter on GPU and CPU,torch.Tensor.scatter,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/76804,2022-05-04T09:53:27Z,"torch.add bool x bool allows integer alpha, inconsistent with other dtype type checking",torch.add,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/58522,2021-05-19T00:44:00Z,DDP grads dont have parity with local training when grads are undefined,torch._C._functions.UndefinedGrad,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/49727,2020-12-22T07:26:40Z,RandomSampler generator created in every iteration,torch.utils.data.RandomSampler,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/45910,2020-10-06T17:29:48Z,with torch.cuda.amp.autocast() get out of memory error when using with torch.no_grad() during validation,torch.cuda.amp.autocast,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/45910,2020-10-06T17:29:48Z,with torch.cuda.amp.autocast() get out of memory error when using with torch.no_grad() during validation,torch.no_grad,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/44169,2020-09-04T02:23:15Z,Deadlock with RPC and dist.barrier() for TensorPipeAgent and NCCL.,torch.rpc.rpc_sync,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/44169,2020-09-04T02:23:15Z,Deadlock with RPC and dist.barrier() for TensorPipeAgent and NCCL.,torch.distributed.barrier,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/44169,2020-09-04T02:23:15Z,Deadlock with RPC and dist.barrier() for TensorPipeAgent and NCCL.,torch.distributed.TensorPipeAgent,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/41244,2020-07-10T12:00:25Z,torch.tan(complex) on CUDA doesn't handle nonfinite values properly,torch.tan,1.10.0,CPU
pytorch,https://github.com/pytorch/pytorch/issues/127697,2024-06-01T23:48:55Z,[dynamo] Issue with construction nn.Parameter,torch.nn.Parameter,"
2.0.0",CPU
pytorch,https://github.com/pytorch/pytorch/issues/112044,2023-10-25T17:43:00Z,pack_padded_sequence/pad_packed_sequence support in dynamo,torch.nn.utils.rnn.pack_padded_sequence,"
1.13.0",CPU
pytorch,https://github.com/pytorch/pytorch/issues/112044,2023-10-25T17:43:00Z,pack_padded_sequence/pad_packed_sequence support in dynamo,torch.nn.utils.rnn.pad_packed_sequence,"
1.13.0",CPU
pytorch,https://github.com/pytorch/pytorch/issues/62283,2021-07-27T18:51:36Z,c10d and discontiguous tensors with mismatch strides,API,"
1.13.0",CPU
pytorch,https://github.com/pytorch/pytorch/issues/62283,2021-07-27T18:51:36Z,c10d and discontiguous tensors with mismatch strides,Name,"
1.13.0",CPU
pytorch,https://github.com/pytorch/pytorch/issues/62283,2021-07-27T18:51:36Z,c10d and discontiguous tensors with mismatch strides,torch.distributed.send,"
1.13.0",CPU
pytorch,https://github.com/pytorch/pytorch/issues/62283,2021-07-27T18:51:36Z,c10d and discontiguous tensors with mismatch strides,torch.distributed.recv,"
1.13.0",CPU
pytorch,https://github.com/pytorch/pytorch/issues/47260,2020-11-03T04:03:44Z,DDP doesn't work with retain_graph = True,API,"
1.12.0",CPU
pytorch,https://github.com/pytorch/pytorch/issues/47260,2020-11-03T04:03:44Z,DDP doesn't work with retain_graph = True,Name,"
1.12.0",CPU
pytorch,https://github.com/pytorch/pytorch/issues/47260,2020-11-03T04:03:44Z,DDP doesn't work with retain_graph = True,torch.nn.parallel.DistributedDataParallel,"
1.12.0",CPU
