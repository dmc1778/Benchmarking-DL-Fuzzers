pytorch,https://github.com/pytorch/pytorch/issues/134079,2024-08-21T03:06:40Z,operator benchmark diag test cuda problem,YES,torch.diag,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/133976,2024-08-20T06:28:39Z,cuda sdpa meet RuntimeError: CUDA error: invalid configuration argument for some input shape,YES,<torch.backends.cuda.sdp_kernel>,<torch 2.1.0>,12.1
pytorch,https://github.com/pytorch/pytorch/issues/133562,2024-08-15T12:35:18Z,FlexAttention with BLOCK_SIZE!=128,YES,torch.nn.attention.flex_attention.flex_attention,<2.0.0>,12.4
pytorch,https://github.com/pytorch/pytorch/issues/133093,2024-08-09T14:37:43Z,Inconsistent results of Conv2d in pytorch between CPU and CUDA,YES,torch.nn.Conv2d,2.0.1,"CUDA 11.8, CUDA 12.1"
pytorch,https://github.com/pytorch/pytorch/issues/132240,2024-07-31T14:56:35Z,torch.nn.functional.interpolate completely broken with torch.jit.script and torch.fx,YES,"torch.nn.functional.interpolate  
torch.jit.script  
torchvision.models.feature_extraction.create_feature_extractor  
torch.fx  ",2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/132067,2024-07-29T17:50:39Z,Pickle loads torch tensor memory leak,YES,"torch.load
torch.zeros",2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/131755,2024-07-25T08:04:46Z,CUDA error: an illegal memory access was encountered when use TORCH_NCCL_NAN_CHECK.,YES,<torch.distributed>,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/131272,2024-07-20T02:42:02Z,ONNX: wrong operator for ceil_mode Pooling in case of skip the last window,YES,torch.nn.AvgPool2d,"<2.0.0>
<2.1.0>",No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/130108,2024-07-04T08:44:11Z,[ONNX]: Fail to export onnx when GroupNorm input shape rank=2,YES,"torch.onnx.export  
torch.nn.GroupNorm",2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/130078,2024-07-04T00:34:09Z,cuda graph error for torch.isin if the input tensors are too large,YES,torch.isin,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/129823,2024-06-29T14:39:21Z,Output does not match,YES,<torch.compile>,<torch 2.1.0>,12.2
pytorch,https://github.com/pytorch/pytorch/issues/129649,2024-06-27T10:44:18Z,Low GPU precision,YES,torch.nn.functional.conv3d,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/129200,2024-06-21T02:51:49Z,"Export model using onnx.dynamo_export has bug of ""torch._dynamo.exc.Unsupported: call_method TupleVariable() size [ConstantVariable(int)] {}""",YES,torch.onnx.dynamo_export,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128501,2024-06-12T10:56:18Z,torch.onnx.export with dynamic axes fails for torch.nn.InstanceNorm1d with track_running_stats=True,YES,"torch.onnx.export
torch.nn.InstanceNorm1d",2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/128494,2024-06-12T08:22:26Z,torch.onnx.export fails for torch.meshgrid with indexing='xy',YES,"<API Name>
torch.onnx.export
torch.meshgrid",2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/128131,2024-06-06T10:26:28Z,Unable to record Memory consumption with `torch.cuda.memory._record_memory_history()`,YES,<torch.cuda.memory._record_memory_history>,2.1.0,cu118
pytorch,https://github.com/pytorch/pytorch/issues/127223,2024-05-27T11:06:08Z,[BUG] torch.linalg.lstsq returning wrong result.,YES,<torch.linalg.lstsq>,<PyTorch 2.0.1>,11.7
pytorch,https://github.com/pytorch/pytorch/issues/126760,2024-05-21T07:24:37Z,"""torch.orgqr"" performs differently on cpu and gpu",YES,torch.orgqr,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/126748,2024-05-21T03:11:29Z,"""torch.geqrf"" performs differently on cpu and gpu",YES,torch.geqrf,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/126660,2024-05-20T02:35:17Z,UnsupportedOperatorError: aten::scatter_reduce when include_self=False,YES,"<torch.onnx.export>  
<torch.onnx.errors.UnsupportedOperatorError>  
<torch.cat>  
<torch.nn.functional.normalize>  
<torch_geometric.nn.max_pool>",2.0.0,"The CUDA version mentioned in the description is not explicitly stated. However, it can be inferred that the PyTorch version being used is `torch==2.0.0`, which typically supports CUDA versions up to 11.8 or later. Hence, if you need to specify a CUDA version commonly associated with PyTorch 2.0.0, it would be:

11.8"
pytorch,https://github.com/pytorch/pytorch/issues/125564,2024-05-06T05:37:28Z,Issues in loading quantized weights with Version==None,YES,"torch.quantization.prepare  
torch.quantization.convert  
torch.load  
torch.nn.modules.module.load_state_dict  ",2.0.0,"11.7, 12.2.140"
pytorch,https://github.com/pytorch/pytorch/issues/125247,2024-04-30T17:28:57Z,RuntimeError: could not create a primitive descriptor for a matmul primitive,YES,torch.nn.Linear,2.0.0,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/125064,2024-04-26T19:24:35Z,ValueError: weight_norm of 'weight' not found in ParametrizedConvTranspose1d,YES,"torch.nn.utils.weight_norm  
torch.nn.utils.parametrizations.weight_norm  
torch.nn.utils.remove_weight_norm",2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/124788,2024-04-23T21:57:16Z,torch.nn.checkpoint.checkpoint ignores default device in backward() call,YES,torch.utils.checkpoint.checkpoint,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/124595,2024-04-22T06:00:18Z,torch._export can't export resnet50 model,YES,<torch._export.capture_pre_autograd_graph>,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/124380,2024-04-18T10:55:06Z,Error when running DDP examples to train between nodes,YES,"torch.distributed.launcher.api  
torch.distributed.elastic.rendezvous.c10d_rendezvous_backend  
torch.distributed.elastic.rendezvous.api  
torch.distributed.utils  
torch.nn.parallel.distributed  
torch.distributed.elastic.multiprocessing.errors  
torch.distributed.run  
torch.distributed.DistBackendError",2.1.0,12.2
pytorch,https://github.com/pytorch/pytorch/issues/124133,2024-04-16T01:34:07Z,ZeroRedundancyOptimizer + AdamW Fused can't load state_dict back,YES,"torch.optim.ZeroRedundancyOptimizer  
torch.optim.AdamW",2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123604,2024-04-09T00:09:24Z,"Can't initialize NVML, H100 gpus on ubuntu",YES,<torch.cuda>,2.1.0,"12.1, 12.3"
pytorch,https://github.com/pytorch/pytorch/issues/122617,2024-03-25T16:57:02Z,[Regression PyTorch 2.1 & 2.2] `torch._transformer_encoder_layer_fwd` outputs on CUDA/NestedTensorCUDA backends significantly diverge from truth for torch>=2.1 (SDP math)),YES,<torch._transformer_encoder_layer_fwd>,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/122568,2024-03-24T06:19:17Z,How do I use exec in a Pytorch Module and train with multiple GPUs?,YES,"torch.nn.MSELoss  
torch.nn.L1Loss  
torch.nn.PoissonNLLLoss  
torch.nn.GaussianNLLLoss  
torch.nn.KLDivLoss  
torch.nn.HuberLoss  
torch.nn.SmoothL1Loss  
torch.nn.BCELoss  
torch.nn.BCEWithLogitsLoss  
torch.nn.SoftMarginLoss  
torch.nn.MultiLabelSoftMarginLoss  
torch.nn.CrossEntropyLoss  
torch.nn.NLLLoss  
torch.nn.TripletMarginLoss  
torch.nn.KLDivergence  
torch.nn.HingeEmbeddingLoss  
torch.nn.MultiLabelMarginLoss  
torch.nn.TripletMarginWithDistanceLoss  
torch.nn.BatchNorm1d  
torch.nn.BatchNorm2d  
torch.nn.LayerNorm  
torch.nn.Conv2d  
torch.nn.MaxPool2d  
torch.nn.AvgPool2d  
torch.nn.Dropout  
torch.nn.LeakyReLU  
torch.nn.ReLU  
torch.nn.PReLU  
torch.nn.Sigmoid  
torch.nn.Tanh  
torch.nn.ELU  
torch.nn.Softmax  
torch.nn.Linear  
torch.nn.Flatten  
torchmetrics.F1Score  
torchmetrics.Accuracy  
torchmetrics.Precision  
torchmetrics.Recall  ",2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/122020,2024-03-16T08:05:29Z, torch.jit.script(model) and torch.jit.trace(model) performance degradation.,YES,"torch.jit.script  
torch.jit.trace",<torch 2.0.1>,"11.8, 12.1"
pytorch,https://github.com/pytorch/pytorch/issues/121683,2024-03-11T23:20:03Z,linalg.ldl_factor raises an error with zero tensor,YES,torch.linalg.ldl_factor,<PyTorch 2.1.0>,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121595,2024-03-10T12:53:36Z,Different Dropout behavior on macOS and Linux,YES,torch.nn.Dropout,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121588,2024-03-09T20:00:55Z,logistic sigmoid function outputs NaN indeterminate on complex input,YES,"<torch.special.expit>
<torch.nn.functional.sigmoid>",<torch 2.1.0>,cu121
pytorch,https://github.com/pytorch/pytorch/issues/121220,2024-03-05T11:08:24Z,Misleading `log_abs_det_jacobian` shape for `StackTransform`?,YES,"torch.distributions.transforms.StackTransform  
torch.distributions.transforms.AffineTransform  
torch.distributions.transforms.IndependentTransform  
torch.distributions.transforms.log_abs_det_jacobian  ",2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121069,2024-03-02T08:06:45Z,'torch.compile()' causes 'Accuracy failed: uint8 tensor did not match',YES,torch.compile,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/120996,2024-03-01T14:35:23Z,"Unnecessary CUDA synchronization, or missing warnings",YES,"torch.full  
torch.tensor  
torch.cuda.set_sync_debug_mode  
torch.ones  
torch.tensor",2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/120713,2024-02-27T14:39:32Z,Unstable DtoH execution time,YES,"<torch.distributed.all_gather_into_tensor>
<torch.cuda.Stream>
<torch.cuda.synchronize>
<torch.empty>
<torch.rand>
<torch.zeros_like>
<torch.distributed.barrier>
<torch.profiler.profile>
<torch.profiler.record_function>
<torch.profiler.schedule>
<torch.profiler.ProfilerActivity>",<torch 2.1.0>,12.2
pytorch,https://github.com/pytorch/pytorch/issues/120709,2024-02-27T14:04:41Z,Communications slowdown when using multiple hosts,YES,"<torch.distributed.all_gather_into_tensor>
<torch.distributed.reduce_scatter_tensor>
<torch.distributed.barrier>",<torch 2.1.0>,12.2
pytorch,https://github.com/pytorch/pytorch/issues/120705,2024-02-27T11:42:21Z,Trying doing static_quantization with Pytorch 2.1.0 torch.ao.quantization,YES,torch.ao.quantization,2.1.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/120597,2024-02-26T05:41:30Z,floating point exception on `torch.fmod` when x is large negative integer and y is `-1`,YES,<torch.fmod>,2.1.0,cu121
pytorch,https://github.com/pytorch/pytorch/issues/120395,2024-02-22T09:36:30Z,watchdog exception error,YES,"<torch.c10_cuda.check_implementation>  
<torch.c10d.ProcessGroupNCCL>  
<torch.c10d.ProcessGroupNCCL.WorkNCCL>  ",<PyTorch 2.1.0>,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119452,2024-02-08T13:42:51Z,"RuntimeError: !block->expandable_segment_ INTERNAL ASSERT FAILED at ""../c10/cuda/CUDACachingAllocator.cpp"":2814, please report a bug to PyTorch. ",YES,torch.cuda.mem_get_info,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119420,2024-02-07T23:09:01Z,torch.stft sometimes raises RuntimeError: cuFFT error: CUFFT_INTERNAL_ERROR on low free memory,YES,torch.stft,PyTorch 2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119360,2024-02-07T07:18:45Z,`torch.onnx.dynamo_export` provides wrong output shape for `torch.fft.irfft`,YES,"torch.onnx.dynamo_export  
torch.fft.irfft",2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119188,2024-02-05T12:15:59Z,scaled_dot_product_attention: different output between the reference implementation and the one from torch.nn.functional,YES,torch.nn.functional.scaled_dot_product_attention,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/118991,2024-02-02T11:01:15Z,Memory Leak in v2.0.1 ,YES,torch.randn,<PyTorch 2.0.1>,11.8
pytorch,https://github.com/pytorch/pytorch/issues/118829,2024-02-01T01:08:40Z,The inference result is wrong when I apply the torch.quantization.fuse_modules api to resnet50 model.,YES,torch.quantization.fuse_modules,<PyTorch 2.1.0>,12.1
pytorch,https://github.com/pytorch/pytorch/issues/118634,2024-01-30T14:03:53Z,symbolically traced variables cannot be used as inputs to control flow,YES,<torch.quantization.quantize_fx.prepare_fx>,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/118628,2024-01-30T08:53:34Z,TransformerEncoderLayer fast path predicts NaN when provided attention bias,YES,torch.nn.TransformerEncoderLayer,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/118303,2024-01-25T17:13:39Z,Inconsistent outputs of `mish` and `log10` between eagermode and torch.jit.trace,YES,"torch.nn.functional.mish  
torch.log10",<PyTorch 2.0.1>,11.7
pytorch,https://github.com/pytorch/pytorch/issues/118302,2024-01-25T17:04:07Z,Inconsistent outputs of `torch.adaptive_avg_pool1d` and `torch.threshold_` between eagermode and torch.jit.trace,YES,"torch.adaptive_avg_pool1d  
torch.threshold_",<torch 2.0.1>,11.7
pytorch,https://github.com/pytorch/pytorch/issues/118300,2024-01-25T16:56:56Z,Inconsistent outputs in `linalg_ldl_factor_ex` and `special_spheriacal_bessel_j0` between eagermode and torch.jit.trace,YES,"<torch._C._linalg.linalg_ldl_factor_ex>
<torch._C._special.special_spherical_bessel_j0>",<torch 2.0.1>,11.7
pytorch,https://github.com/pytorch/pytorch/issues/118297,2024-01-25T16:47:20Z,Inconsistent outputs of `cosine_similarity` and `linalg_svdvals` between eagermode and torch.jit.trace(),YES,"torch.cosine_similarity  
torch._C._linalg.linalg_svdvals","<Library Version>
2.0.1",11.7
pytorch,https://github.com/pytorch/pytorch/issues/118165,2024-01-24T02:51:36Z,[Quantization-aware training] Can not quantize nn.MultiheadAttention module ,YES,"torch.ao.quantization.get_default_qat_qconfig  
torch.ao.quantization.prepare_qat  
torch.nn.MultiheadAttention  ",2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/118072,2024-01-23T05:24:21Z,"Dynamo: assert ""source"" in options and options[""source""] is not None for default_generator.set_state call",YES,<torch.compile>,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/117961,2024-01-22T04:36:52Z,ReplicatePad2d cuda not support bfloat16,YES,<torch.replication_pad2d_cuda>,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/117634,2024-01-17T11:33:12Z,CUDA error in ../aten/src/ATen/native/cuda/IndexKernel.cu when index tensor contains multiple indices,YES,torch.index_select,2.1.0,cu121
pytorch,https://github.com/pytorch/pytorch/issues/117541,2024-01-16T13:13:27Z,Jit model running on cuda:1 uses memory on cuda:0,YES,torch.jit.script,<torch 2.1.0>,12.1
pytorch,https://github.com/pytorch/pytorch/issues/117421,2024-01-12T23:42:40Z,FSDP silently ignores a state_dict loaded between forward and backward.,YES,"<torch.distributed.fsdp.FullyShardedDataParallel>
<torch.distributed.fsdp.StateDictType>
<torch.distributed.fsdp.ShardingStrategy>",2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/117102,2024-01-10T10:59:29Z,conver_fx report len(device) > 1 error,YES,<torch.convert_fx>,2.0.0,cu117
pytorch,https://github.com/pytorch/pytorch/issues/116923,2024-01-06T11:10:57Z,Memory leak in `all_reduce` on CPU tensors with Gloo,YES,"<API Name>
torch.distributed.all_reduce",<PyTorch 2.1.0>,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116882,2024-01-05T18:37:08Z,RuntimeError: CUDA out of memory.,YES,<torch.zeros>,<Library Version> 2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/116835,2024-01-05T04:47:35Z,torch.compile fullgraph=True is failing for GPTJ model for toy_backend,YES,torch.compile,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116766,2024-01-04T07:12:44Z,[Dynamo][DeepSpeed] torch._dynamo.exc.InternalTorchDynamoError: NestedUserFunctionVariable() has no type,YES,torch._dynamo,2.1.0,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/116707,2024-01-03T18:10:12Z,AttributeError: `Adam` object has no attribute `param_groups` with Python 3.11,YES,torch.load,2.0.1,Is CUDA available: False
pytorch,https://github.com/pytorch/pytorch/issues/116697,2024-01-03T12:37:30Z,Tracing GPU usage of `torch.vmap` vectorized operation using `torch.profiler` cause CUDA OOM,YES,"<torch.vmap>
<torch.profiler.profile>
<torch.cuda.synchronize>
<torch.cuda.empty_cache>
<torch.no_grad>
<torch.argsort>
<torch.randn>
<torch.device>
<torch.Tensor.scatter_>",2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116690,2024-01-03T09:38:49Z,Huggingface Diffusers PNDMScheduler symbolic tracing with make_fx fails,YES,torch.fx.experimental.proxy_tensor.make_fx,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116633,2024-01-02T19:58:34Z,Sparse tensor element-wise multiplication bug when broadcast needed,YES,torch.sparse_tensor.expand,<torch 2.0.1>,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/116612,2024-01-02T15:52:40Z,Loaded torch.jit traced module treats dtypes incorrectly in autograd,YES,"torch.autograd.grad  
torch.jit.trace_module  
torch.jit.load  
torch.matmul  ",2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/116590,2024-01-01T18:49:34Z,P2p operations hang when mix the usage of default and non-default communication groups,YES,"<torch.distributed.isend>
<torch.distributed.irecv>
<torch.distributed.new_group>
<torch.distributed.P2POp>
<torch.distributed.batch_isend_irecv>",2.1.0,12.2
pytorch,https://github.com/pytorch/pytorch/issues/116555,2023-12-30T02:58:35Z,The performance of `torch.index_select` and regular indexing differs dramatically based on the size of the tensor it is indexing,YES,<torch.index_select>,<PyTorch 2.0.1>,11.7
pytorch,https://github.com/pytorch/pytorch/issues/116515,2023-12-28T19:27:04Z,mobile model has very different inference result,YES,"torch.utils.mobile_optimizer.optimize_for_mobile  
torch.jit.mobile._load_for_lite_interpreter  
torchvision.models.mobilenet_v3_small  ",2.0.1,Could not collect
pytorch,https://github.com/pytorch/pytorch/issues/116255,2023-12-21T09:27:46Z,PrivateUseOne backend does't support tensor shollow copy ,YES,"torch.jit.load  
torch.jit.trace  
torch.to  ",2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/116182,2023-12-20T10:30:00Z,[ONNX]  GFPGANv1.pth to onnx conversion error,YES,torch.onnx.export,PyTorch 2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/116175,2023-12-20T07:04:00Z,Performance for `TransformerEncoderLayer` and `TransformerDecoderLayer` drops severely if pytorch version changed from `pt1.10` to `pt2.0`,YES,"<torch.nn.TransformerEncoderLayer>
<torch.nn.TransformerDecoderLayer>",2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/116142,2023-12-19T23:20:17Z,Memory usage appears to increase if dtype is cast after Tensor.expand,YES,torch.Tensor.expand,<torch 2.0.1>,"The issue description does not explicitly mention a specific CUDA version. It only provides versions of relevant libraries. Therefore, no CUDA version can be identified from the given information."
pytorch,https://github.com/pytorch/pytorch/issues/116077,2023-12-19T02:32:23Z,TransformerEncoderLayer and TransformerDecoderLayer do not work,YES,"torch.TransformerEncoderLayer
torch.TransformerDecoderLayer",<torch 2.1.0>,12.1
pytorch,https://github.com/pytorch/pytorch/issues/115977,2023-12-16T06:08:13Z,customized distributed store causes crash during p2p communication,YES,"torch.distributed.new_group  
torch.distributed.barrier  
torch.distributed.init_process_group  ",2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/115914,2023-12-15T07:13:16Z,torch.jit.script fails when parsing torch.SymInt,YES,"torch.jit.script  
torch.SymInt  ",2.1.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/115818,2023-12-14T11:37:46Z,Fx graph recompilation along with partitioner produces different node order even though graph operations are same,YES,"<torch.fx.passes.infra.partitioner.CapabilityBasedPartitioner>
<torch.fx.passes.operator_support.OperatorSupport>
<torch._functorch.aot_autograd.aot_module_simplified>
<torch._functorch.compile_utils.strip_overloads>
<torch._functorch.compilers._disable_jit_autocast>
<torch.jit.script>
<torch.compile>",2.1.0,None
pytorch,https://github.com/pytorch/pytorch/issues/115785,2023-12-13T23:37:58Z,arctan2 fp16 error when optimising,YES,torch.arctan2,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/115733,2023-12-13T09:31:04Z,RuntimeError: thread_predicates_.find(tv_inp) != thread_predicates_.end(),YES,<torch._C._jit_>,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/115626,2023-12-12T11:59:45Z,Alternatives to compiling a for loop,YES,<torch.compile>,<torch 2.1.0>,11.8
pytorch,https://github.com/pytorch/pytorch/issues/115622,2023-12-12T10:36:33Z,Torch jit load model with enums,YES,torch.jit.load,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/115416,2023-12-08T12:22:02Z,torch.nn.functional.embedding segmentation fault on large negative ids,YES,torch.nn.functional.embedding,2.1.0,11.3.1
pytorch,https://github.com/pytorch/pytorch/issues/115388,2023-12-07T23:01:16Z,`torch.distributed.destroy_process_group()` hangs after CUDA graph capture of NCCL operations,YES,<torch.distributed.destroy_process_group>,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/115342,2023-12-07T11:59:20Z,torch.func.grad not working with list comprehension over dictionary,YES,torch.func.grad,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/115077,2023-12-04T16:20:21Z,RuntimeError: cuSPARSELT not supported on your machine. When I was calling: torch._cslt_compress(),YES,<torch._cslt_compress>,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/114943,2023-12-01T10:24:55Z,torch.einsum may choose a strategy for which there is not enough memory,YES,torch.einsum,2.1.0,12.3
pytorch,https://github.com/pytorch/pytorch/issues/114855,2023-11-30T08:19:42Z,SparseTensor index select uses more CUDA memory than Torch index select,YES,"torch.sparse_coo_tensor  
torch.index_select  
torch.to_dense",PyTorch 2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/114672,2023-11-28T07:52:42Z,torch.fft.ifft crashes for empty input,YES,torch.fft.ifft,2.1.0,11.3.1
pytorch,https://github.com/pytorch/pytorch/issues/114650,2023-11-28T00:35:47Z,Error when calculating the Jacobian of torch.conj using forward-mode differentiation,YES,torch.conj,<torch 2.1.0>,12.1
pytorch,https://github.com/pytorch/pytorch/issues/114188,2023-11-20T23:38:35Z,Unsupported operator error: `aten::to_mkldnn` export to ONNX not supported,YES,<torch.aten::to_mkldnn>,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/114112,2023-11-20T10:15:43Z,[JIT] - torch.script - 'Optional[Tensor]' object has no attribute or method 'size',YES,torch.jit.script,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/114090,2023-11-20T02:26:16Z,Nested tensors fail on Conv2D,YES,"torch.nested.nested_tensor  
torch.nn.Conv2d",2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/114040,2023-11-19T00:50:05Z,IndexError: map::at with MPI CUDA collectives,YES,"<torch.distributed.all_gather>
<torch.distributed.all_reduce>
<torch.distributed.init_process_group>",2.1.0,12.2
pytorch,https://github.com/pytorch/pytorch/issues/113956,2023-11-17T16:20:30Z,iSTFT gives wrong results for some batched input,YES,torch.istft,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/113942,2023-11-17T10:26:02Z,Tensorboard list of tensors as input,YES,torch.jit.trace,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/113847,2023-11-16T06:20:20Z,RuntimeError in use torch 2.1.0 cuda 11.8 ,YES,torch.nn.Module.to,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/113817,2023-11-15T23:05:50Z,[ONNX] ONNX export of simple quantized model fails,YES,<torch.onnx.export>,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/113755,2023-11-15T12:35:57Z,Upsample trilinear onnx ,YES,"torch.onnx.dynamo_export  
torch.nn.Upsample  ",2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/113600,2023-11-14T00:43:20Z,Segmentation fault in RPC worker when DataLoader has num_workers > 0,YES,torch.distributed.rpc,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/113449,2023-11-10T15:50:39Z,cuDNN error: CUDNN_STATUS_MAPPING_ERROR on gtx_1080/A10 when conv1d is called,YES,<torch.nn.Conv1d>,"<Library Version>
2.0.1",CUDA version: 11.7
pytorch,https://github.com/pytorch/pytorch/issues/113444,2023-11-10T13:53:18Z,ONNX Export - miscompilation for complex-valued operators,YES,<torch.onnx.dynamo_export>,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/113443,2023-11-10T12:32:32Z,`CompileProfiler` reports graph breaks while `dynamo.explain` reports no graph breaks,YES,"<torch._dynamo.explain>
<torch._dynamo.utils.CompileProfiler>",2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/113245,2023-11-08T06:28:03Z,NCCL error of PyTorch 2.1.0 when using multiple gpus,YES,torch.cuda.nccl.all_gather,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/113180,2023-11-07T18:35:47Z,Higher train loss and worse evaluation metrics when using `torch.compile()`,YES,<torch.compile>,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/113128,2023-11-07T05:37:43Z,NCCL watchdog thread terminated with exception ,YES,<torch.c10d.ProcessGroupNCCL>,<torch 2.1.0>,12.1.1
pytorch,https://github.com/pytorch/pytorch/issues/113024,2023-11-06T13:49:39Z,Upsample bilinear 2d decomposition does not match native implementation for uint8,YES,"torch._C.nn.upsample_bilinear2d  
torch._decomp.decompositions.upsample_bilinear2d_vec",2.1.0,None
pytorch,https://github.com/pytorch/pytorch/issues/112775,2023-11-02T19:35:40Z,Multi-Threaded GraphModule / torch.fx inference raises an exception,YES,torch.fx.symbolic_trace,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/112714,2023-11-02T09:43:06Z,log_softmax() on CPU and GPU has expected numerical error when used with low-precision bfloat16,YES,torch.nn.functional.log_softmax,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/112658,2023-11-01T21:11:34Z,FSDP requires global device context,YES,"<torch.cuda.current_device>  
<torch.cuda.set_device>  
<torch.cuda.device>",2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/112608,2023-11-01T16:48:41Z,`grad_fn` is not defined in `__torch_dispatch__` arguments when lazy device is used,YES,<torch._lazy.ts_backend>,2.1.0,None
pytorch,https://github.com/pytorch/pytorch/issues/112583,2023-11-01T15:12:57Z,Nesting no_grad in autocast causes backwards graph to be (partially) lost outside of no_grad,YES,"torch.cuda.amp.autocast  
torch.no_grad",PyTorch 2.1.0,"The issue description does not explicitly mention a specific CUDA version. However, it references PyTorch version 2.1.0, which typically requires a compatible version of CUDA. Without additional information from the context, I cannot provide a precise CUDA version. If you need to know the CUDA version required for PyTorch 2.1.0, it would be best to check the official PyTorch installation documentation or release notes."
pytorch,https://github.com/pytorch/pytorch/issues/112569,2023-11-01T06:51:05Z,"RuntimeError: NVML_SUCCESS == r INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1695392035891/work/c10/cuda/CUDACachingAllocator.cpp"":1154, please report a bug to PyTorch",YES,"torch.nn.modules.module  
torch.nn.modules.conv  ",2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/112395,2023-10-30T13:51:06Z,"KeyError during model export while using ""newer"" data types",YES,"torch.float8_e5m2
torch.float8_e4m3fn",2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/112377,2023-10-30T08:47:11Z,"RuntimeError: NVML_SUCCESS == r INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1695392020201/work/c10/cuda/CUDACachingAllocator.cpp"":1154, please report a bug to PyTorch.",YES,<torch.cuda.CUDACachingAllocator>,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/112372,2023-10-30T07:00:30Z,[Tracer] RuntimeError: _Map_base::at when tracing using autograd.Function,YES,torch.autograd.Function,"<Library Version>
2.1.0",12.1
pytorch,https://github.com/pytorch/pytorch/issues/112340,2023-10-29T12:42:20Z,Unexpected behaviour with shared modules in multiprocessing on WSL2,YES,<torch.multiprocessing.get_context>,"<Library Version>
2.1.0",12.1
pytorch,https://github.com/pytorch/pytorch/issues/112152,2023-10-26T14:13:20Z,DDP backpropagated gradients not the same across all gpus when forward inference not using all declared modules,YES,<torch.distributed>,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/112095,2023-10-25T23:17:57Z,torch.export fails on a model with optional parameter,YES,torch.export.export,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/112015,2023-10-25T10:23:04Z,op scaled_dot_product_attention case different results,YES,torch.scaled_dot_product_attention,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/112009,2023-10-25T08:00:10Z,_functional_collectives.all_gather_into_tensor cannot compile in aot_module_simplified,YES,"torch.distributed._functional_collectives.all_gather_tensor  
torch.distributed._functional_collectives.all_gather_into_tensor",2.1.0,"The issue description does not explicitly mention any CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/111958,2023-10-24T20:13:41Z,torch 2.1 FSDP only some layers might not be working with training only a couple of layers,YES,torch.FSDP,2.1.0,cu118
pytorch,https://github.com/pytorch/pytorch/issues/111925,2023-10-24T15:37:51Z,Resize warning in two argument torch.logical_* with broadcasting,YES,torch.logical_and,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/111902,2023-10-24T09:00:32Z,torchscript file can not be loaded if its saved form the export model produced by torch.export.export,YES,<torch.export.export>,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/111901,2023-10-24T08:11:27Z,Multiprocess. DataLoader worker  is killed by signal: Segmentation fault.,YES,torch.utils.data.dataloader,<PyTorch 2.1.0>,12.1
pytorch,https://github.com/pytorch/pytorch/issues/111824,2023-10-23T18:22:50Z,GroupNorm & InstanceNorm does not handle channels_last correctly,YES,<torch.nn.GroupNorm>,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/111646,2023-10-20T10:33:28Z,torchrun: elastic training not restarted on missing keep-alive heartbeat/scale-down event,YES,torch.distributed.run,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/111570,2023-10-19T15:24:58Z,Tensor `.cuda()` very slow with specific array sizes ,YES,torch.cuda,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/111551,2023-10-19T06:55:18Z,[inline_inbuilt_nn_modules] Custom `ModuleDict.__getitem__(key: tuple)` produces a graph break,YES,<torch.nn.ModuleDict.__getitem__>,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/111509,2023-10-18T21:16:16Z,Sparse Tensor Sum Still Does Not Work for PyTorch Geometric,YES,torch.sum,2.1.0,12.2
pytorch,https://github.com/pytorch/pytorch/issues/111508,2023-10-18T21:13:04Z,LBFGS accuracy difference between CPU and GPU,YES,<torch.optim.LBFGS>,<PyTorch 2.0.0>,11.7
pytorch,https://github.com/pytorch/pytorch/issues/111480,2023-10-18T16:11:12Z,torch.jit.script persistently changes default from utf-8 to ascii,YES,<torch.jit.script>,2.1.0,<Not specified>
pytorch,https://github.com/pytorch/pytorch/issues/111385,2023-10-16T20:00:41Z,Torch Compile Dynamic fails on sample on diffusers VAE,YES,torch.randn,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/111366,2023-10-16T11:01:17Z,RuntimeError: CUDAPluggableAllocator does not yet support cacheInfo,YES,<torch.cuda.memory.change_current_allocator>,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/111317,2023-10-15T02:20:28Z,Torch 2.1 compile + FSDP (mixed precision) + LlamaForCausalLM: `RuntimeError: attempting to assign a gradient with dtype 'c10::BFloat16' to a tensor with dtype 'float'.`,YES,"torch.compile  
torch.distributed.fsdp  
torch.amp.autocast",2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/111187,2023-10-13T05:23:31Z,"torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1331, unhandled cuda error (run with NCCL_DEBUG=INFO for details)",YES,torch.distributed.distributed_c10d.barrier,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/111182,2023-10-13T03:24:50Z,Training iresnet with torch.compile is slower than eager mode for torch 2.1.0,YES,torch.compile,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/111158,2023-10-12T20:18:05Z,RuntimeError in run_streaming_llama.py When Using Accelerate with Streaming LLMa Model on A4500 GPU,YES,<torch.utils._contextlib>,2.1.0,CUDA version: 11.7
pytorch,https://github.com/pytorch/pytorch/issues/111142,2023-10-12T16:30:09Z,'torch._C.Node' object has no attribute 'cs',YES,torch._C.Node,<PyTorch 2.1.0>,12.1
pytorch,https://github.com/pytorch/pytorch/issues/111135,2023-10-12T13:22:34Z,"No op for aten::where with argument types: Tensor, Tensor, bool.",YES,torch.where,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/111024,2023-10-11T07:19:17Z,ncu python conv2d.py runs indefinitely after activating cudnn.benchmark,YES,<torch.backends.cudnn.benchmark>,<torch 2.1.0>,12.2
pytorch,https://github.com/pytorch/pytorch/issues/110959,2023-10-10T17:37:02Z,`model.named_buffers()` fails if module not hashable.,YES,torch.nn.Module.named_buffers,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/110885,2023-10-09T19:29:22Z,Depthwise conv3d slower than normal conv3d,YES,<torch.nn.Conv3d>,<PyTorch 2.1.0>,12.1
pytorch,https://github.com/pytorch/pytorch/issues/110779,2023-10-07T01:27:45Z,"Repro for non-deterministic ""operation not permitted when stream is capturing"" crash",YES,"torch.distributed.init_process_group  
torch.distributed.new_group  
torch.distributed.all_gather_into_tensor  
torch.cuda.graph","<Library Version>
2.1.0",CUDA version: 12.1
pytorch,https://github.com/pytorch/pytorch/issues/110758,2023-10-06T20:22:03Z,"Fused Adamw RuntimeError: params, grads, exp_avgs, and exp_avg_sqs must have same dtype, device, and layout",YES,torch._fused_adamw_,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/110701,2023-10-06T12:32:03Z,Error with monai SwinUNETR and checkpointing,YES,<torch.utils.checkpoint>,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/110669,2023-10-06T00:32:43Z,Backward pass for Nested Tensors using flash attention in sdpa fails,YES,"torch.backends.cuda.sdp_kernel  
torch.nested.nested_tensor  
aten::_scaled_dot_product_flash_attention_backward",2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/110605,2023-10-05T14:41:21Z,ValueError issued instead of TypeError when tensor is cast to a scalar,YES,torch.math.isfinite,2.0.1,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/110515,2023-10-04T14:35:59Z,DynamicQuantizedLinear shows incorrect qscheme after applying eager mode dynamic quantization,YES,torch.quantized.dynamic_quantized_linear,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/110505,2023-10-04T07:43:21Z,scaled_dot_product returns NaN arrays with eval(),YES,torch.nn.functional.scaled_dot_product,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/110479,2023-10-03T22:11:55Z,[FSDP] [Checkpointing] Loading optimizer state dict with use_orig_params True causes OOM,YES,"<torch.distributed.fsdp.FSDP.optim_state_dict_to_load>
<torch.optim.optimizer.load_state_dict>",2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/110387,2023-10-02T13:34:23Z,Change unsqueeze(0) to preserve memory layout contiguity of original tensor,YES,torch.unsqueeze,PyTorch 2.0.1,None
pytorch,https://github.com/pytorch/pytorch/issues/110366,2023-10-01T20:44:04Z,Categorical Simplex constraint throws error for valid values,YES,torch.distributions.Categorical,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/110356,2023-10-01T13:37:59Z,"Dropout signature inconsistent between `torch.dropout`, `torch.nn.Dropout` and `torch.nn.functional.dropout`",YES,"torch.dropout  
torch.nn.Dropout  
torch.nn.functional.dropout  
torch.nn.functional.alpha_dropout  
torch.nn.functional.feature_alpha_dropout",2.0.1,cu121
pytorch,https://github.com/pytorch/pytorch/issues/110332,2023-09-30T08:29:43Z,`torch.jit.load()` might unresponsive in IBM s390x when loading some certain torchscript saved by x86 machine.,YES,torch.jit.load,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/110249,2023-09-28T21:49:11Z,`torch.func.functional_call` does not work with `__torch_function__ ` Tensor-like objects,YES,torch.func.functional_call,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/110205,2023-09-28T08:02:20Z,RuntimeError: Expected packed scalar Tensor to be of dimension 1. Got 0 instead.,YES,torch.optim.Adam,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/110194,2023-09-28T02:32:05Z,cudaMallocAsync cause too much fragmentation.,YES,<torch.cudaMallocAsync>,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/110056,2023-09-26T04:47:26Z,torch.onnx.export causes floating point exception with core dump for empty slice assignment,YES,<torch.onnx.export>,<PyTorch 2.0.1>,CUDA version: None
pytorch,https://github.com/pytorch/pytorch/issues/110029,2023-09-25T20:23:34Z,Dataloader resetting with num_workers=1 and persistent_workers=True,YES,torch.utils.data.get_worker_info,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/109987,2023-09-25T03:08:08Z,Static quantization for Transformer block : AttributeError 'function' object has no attribute 'is_cuda',YES,"torch.ao.quantization.QuantStub  
torch.ao.quantization.DeQuantStub  
torch.ao.quantization.prepare  
torch.ao.quantization.convert  
nn.Transformer.generate_square_subsequent_mask  
nn.TransformerEncoderLayer",PyTorch 2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/109948,2023-09-23T20:37:43Z,Simple script segfaulting when grad is enabled,YES,<API Name> torch.set_grad_enabled,2.0.1,cu121
pytorch,https://github.com/pytorch/pytorch/issues/109675,2023-09-20T01:51:40Z,[FSDP] UnpicklingError when calling save_state_dict in distributed run,YES,"<torch.distributed.checkpoint.save_state_dict>  
<torch.distributed.checkpoint.all_reduce>  
<torch.distributed.checkpoint.broadcast_object>  
<torch.distributed.broadcast_object_list>  
<torch.distributed._tensor_to_object>  ",<PyTorch 2.0.1>,11.8
pytorch,https://github.com/pytorch/pytorch/issues/109586,2023-09-19T08:21:03Z,Max pool with negative integer inputs and channels_last memory layout gives the wrong values,YES,<torch.max_pool2d>,<PyTorch 2.1.0>,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/109488,2023-09-18T09:24:28Z,[bug] FALLBACK path has been taken inside: runCudaFusionGroup,YES,torch.runCudaFusionGroup,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/109478,2023-09-18T06:29:44Z,ProcessGroup is not automatically destroyed when the process exits,YES,torch.distributed.destroy_process_group,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/109462,2023-09-17T16:45:33Z,Inconsistent behavior for in-place operations on coalesced sparse tensors,YES,"<torch.mul_>
<torch.div_>",2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/109453,2023-09-17T06:11:15Z,Inconsistent Behavior of `ConvTranspose2d` on CPU and CUDA,YES,torch.nn.ConvTranspose2d,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/109445,2023-09-16T20:47:59Z,Memory usage steadily increasing when using back propagation with sparse CSR parameter matrices on CPU,YES,"torch.nn.Parameter  
torch.sparse.mm  ",<torch 2.0.1>,11.7
pytorch,https://github.com/pytorch/pytorch/issues/109385,2023-09-15T16:31:52Z,FSDP crashes when submodule calls method that isn't `forward()`,YES,<torch.distributed.fsdp.FullyShardedDataParallel>,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/109191,2023-09-13T10:27:51Z,Gradients across different ranks are not synchronized when using DDP,YES,"torch.distributed.init_process_group  
torch.nn.parallel.DistributedDataParallel  
torch.nn.functional.relu  
torch.nn.MSELoss  ",2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/109103,2023-09-12T12:14:58Z,"DDP - ""No backend type associated with device type cpu"" with new Model Phi 1.5 despite everything loaded on GPUs",YES,<torch.nn.parallel.distributed>,2.1.0,"The issue description does not explicitly mention a CUDA version. However, it references various library versions and packages. The most relevant library that can imply a CUDA version is the `torch` library, which is currently at version `2.1.0a0+b5021ba`. The specific CUDA version used with PyTorch can sometimes be inferred from the installed version of PyTorch.

For this issue, the CUDA version is not directly stated, but it is typically associated with the PyTorch version you are using. If you need to find the CUDA version compatible with this version of PyTorch, you may refer to the official PyTorch website or documentation.

Without an explicit mention in the issue, I cannot provide a specific CUDA version number. 

If you need to provide a CUDA version based on standard compatibility, you might use:

CUDA version: <not specified>"
pytorch,https://github.com/pytorch/pytorch/issues/109100,2023-09-12T07:29:16Z,FSDP do not support `ignored_parameters` when `auto_wrap_policy` is specified,YES,"<torch.distributed.fsdp.ShardingStrategy._HYBRID_SHARD_ZERO2>
<FSDP>",2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/109002,2023-09-11T04:13:14Z,[FSDP] How can I wrap a model that has both nn.Parameter and nn.Module,YES,"<torch.distributed.fsdp.FullyShardedDataParallel>  
<torch.distributed.fsdp.fully_sharded_data_parallel.CPUOffload>  
<torch.distributed.fsdp.fully_sharded_data_parallel.BackwardPrefetch>  
<torch.distributed.fsdp.wrap.size_based_auto_wrap_policy>  
<torch.distributed.fsdp.wrap.enable_wrap>  
<torch.distributed.fsdp.wrap.wrap>  
<torch.distributed.fsdp.MixedPrecision>  
<torch.distributed.fsdp.ShardingStrategy>  ",2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/108859,2023-09-08T10:36:53Z,Certain torch functions are not handled by torch func wrapper,YES,"torch.mean  
torch.cat  
torch.stack  
torch.isin  ",2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/108745,2023-09-07T03:55:34Z,[PT2.0] [.Compile] [Dynamic] Pytorch FX/JIT graph's inputs/nodes ordering is changed when FX recompiles even though the graph operations are same,YES,<torch.compile>,<PyTorch 2.0.1>,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/108739,2023-09-07T03:14:46Z,"DDP Elastic ""master_addr"" resolution error in environment variables.",YES,torch.distributed.elastic.agent.server.api,2.1.0,None
pytorch,https://github.com/pytorch/pytorch/issues/108676,2023-09-06T17:45:45Z,RuntimeError when calling conv_transpose2d with groups,YES,torch.nn.functional.conv_transpose2d,2.0.0,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/108670,2023-09-06T17:19:07Z,torch.jit.script produces incorrect gradients,YES,<torch.jit.script>,<PyTorch 2.0.1>,11.8
pytorch,https://github.com/pytorch/pytorch/issues/108665,2023-09-06T16:26:59Z,INTERNAL ASSERT FAILED in `shape_type_inference.cpp`,YES,<torch.onnx.export>,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/108645,2023-09-06T09:56:49Z,uninformative OOM error,YES,"torch._tensor.backward  
torch.autograd.backward  ",2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/108642,2023-09-06T09:45:04Z,torch.topk returned values and indices are reordered if sorted=False,YES,torch.topk,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/108627,2023-09-06T06:21:49Z,autocast not consistent across different GPUs (A100 and RTX A6000),YES,torch.amp.autocast,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/108521,2023-09-04T15:54:12Z,"resutl of (torch.mm(a,b) does not match result of (a[:part,:], b)",YES,<torch.mm>,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/108496,2023-09-03T17:42:17Z,The CPU version of `torch.cummax` is slow,YES,torch.cummax,<torch 2.0.1>,11.7
pytorch,https://github.com/pytorch/pytorch/issues/108493,2023-09-03T12:39:25Z,RWKV + Adam exp_avg_sq will change from positive to negative after loss.backward(),YES,torch.optim.Adam,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/108432,2023-09-01T17:53:14Z,Wrong result of first run with torch.compile() when backend is using torch.jit.trace() and model has inplace operators ,YES,"<torch.compile>
<torch.jit.trace>",2.1.0,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/108407,2023-09-01T09:19:57Z,torch.einsum() computes different results on cpu and cuda on A100 GPU.,YES,torch.einsum,2.1.0,cu121
pytorch,https://github.com/pytorch/pytorch/issues/108378,2023-08-31T22:00:54Z,NCCL ISend is not asynchronous ,YES,torch.distributed.isend,2.1.0,12.2
pytorch,https://github.com/pytorch/pytorch/issues/108332,2023-08-31T07:48:19Z,RuntimeError: dims.value().size() == self->getMaybeRFactorDomain().size(),YES,<torch.jit.script>,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/108277,2023-08-30T20:08:29Z,Transformer performance drop due to slow PyTorch GEMMs,YES,<torch.nn.functional.linear>,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/108226,2023-08-30T10:24:22Z,torch.nn.functional.pad() with value type bool,YES,torch.nn.functional.pad,<torch 2.0.1>,11.7
pytorch,https://github.com/pytorch/pytorch/issues/108224,2023-08-30T08:41:40Z,qnnpack quantized model can not be traced,YES,<torch.jit.trace>,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/108211,2023-08-30T03:15:21Z,[Compile] Running Llama2 with torch.compile and FSDP results in Type mismatch assert in LlamaRotaryEmbedding ,YES,"torch.compile  
torch.distributed.fsdp  
torch.nn.modules.module  
torch._dynamo  
torch._subclasses.meta_utils  
torch.nn.modules.module  
torch.nn.modules.module  
torch._dynamo.external_utils  
torch._dynamo.convert_frame  
torch._dynamo.symbolic_convert  
torch._dynamo.variables.builder  ",2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/108210,2023-08-30T03:12:34Z,Using distributed RPC and DDP together triggers error.,YES,"torch.distributed.all_reduce  
torch.distributed.rpc.rpc_sync",<PyTorch 2.0.1>,11.8
pytorch,https://github.com/pytorch/pytorch/issues/108152,2023-08-29T14:43:21Z,`Tensor.uniform_` uses illegal argument name `from`,YES,torch.Tensor.uniform_,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/107854,2023-08-24T07:41:26Z,"""file_descriptor"" multiprocessing sharing strategy works incorrectly in dataloading ",YES,torch.multiprocessing,<PyTorch 2.0.1>,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/107824,2023-08-23T22:08:03Z,torch.compile() fails when an `autograd.Function` gets called and torch.no_grad() is *not* being used,YES,"torch.compile()
torch.no_grad()
torch.autograd.Function
torch.distributed.get_world_size()
torch.distributed._functional_collectives.all_reduce()",2.1.0,12.0
pytorch,https://github.com/pytorch/pytorch/issues/107821,2023-08-23T21:36:32Z,`torch.distributions.Pareto.sample` sometimes gives `inf`,YES,torch.distributions.Pareto.sample,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/107820,2023-08-23T21:32:53Z,`add_image_with_boxes` method from `torch.utils.tensorboard.writer.SummaryWriter` is broken,YES,<torch.utils.tensorboard.writer.SummaryWriter.add_image_with_boxes>,2.0.1,12.2.128
pytorch,https://github.com/pytorch/pytorch/issues/107680,2023-08-22T12:04:15Z,torch.nn.functional.cross_entropy different loss when providing one_hot_target and class weights,YES,torch.nn.functional.cross_entropy,2.0.1,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/107678,2023-08-22T10:29:26Z,[Torch.fx] Torch fx failed to trace torch extension library,YES,<torch.fx.symbolic_trace>,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/107663,2023-08-22T05:46:29Z,`RuntimeError: expected scalar type BFloat16 but found Float` with `torch.nn.TransformerEncoder`,YES,torch.nn.TransformerEncoder,2.1.0,None
pytorch,https://github.com/pytorch/pytorch/issues/107661,2023-08-22T04:12:13Z,A backward bug of dtensor seems to be caused by new_empty_strided,YES,<torch.new_empty_strided>,2.1.0,None
pytorch,https://github.com/pytorch/pytorch/issues/107568,2023-08-21T04:00:42Z,RuntimeError: Unsupported value kind: Tensor while torch.jit.script nn.Module,YES,torch.jit.script,PyTorch 2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/107538,2023-08-20T10:27:30Z,[fx] tracing function with in-place mutation results in unexpected behaviour due to local vars becoming persisted in  `GraphModule(nn.Module)`,YES,<torch.fx.symbolic_trace>,<torch 2.0.1>,None
pytorch,https://github.com/pytorch/pytorch/issues/107514,2023-08-19T15:01:32Z,conv2d wrong results on 3090/3090ti,YES,torch.nn.Conv2d,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/107503,2023-08-19T02:37:47Z,[nightly][jit] bad constant exponent (e+38.f) in default_program fused_mul_div_add,YES,torch.jit.trace,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/107494,2023-08-18T23:38:53Z,Mismatch in type of error raised when reducing along empty slice between eager and primtorch,YES,<torch.reduction>,2.1.0,12.1.105
pytorch,https://github.com/pytorch/pytorch/issues/107436,2023-08-18T03:39:26Z,[FSDP]coding to multi-node save optimizer error,YES,<torch.distributed.fsdp.full_optim_state_dict>,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/107372,2023-08-17T09:57:13Z,torch compile error with SyncBatchNorm,YES,"torch.compile  
torch.nn.SyncBatchNorm  
torch.distributed.new_group  ",2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/107352,2023-08-17T03:02:53Z,dist.scatter is incompatible with transpose/permute operation,YES,<torch.distributed.scatter>,2.0.1,cu117
pytorch,https://github.com/pytorch/pytorch/issues/107298,2023-08-16T15:12:16Z,dist.destroy_process_group did not destroy the process group well,YES,<torch.dist.destroy_process_group>,<torch 2.1.0>,11.8
pytorch,https://github.com/pytorch/pytorch/issues/107276,2023-08-16T03:26:12Z,CUBLAS_STATUS_NOT_SUPPORTED,YES,<torch.nn.parallel.DataParallel>,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/107200,2023-08-15T01:59:42Z,[inductor] [dynamic shape] 5 HF models fails with `Constraints violated` using transformers v4.31.0,YES,torch._dynamo,2.1.0,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/107166,2023-08-14T19:36:55Z,jit compilation returns an int rather than a bool when using math.isnan(),YES,<torch.jit.script>,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/107143,2023-08-14T14:16:25Z,[dynamo] calling __torch_function__ with dynamically created subclass of torch.Tensor fails compilation,YES,<torch.Tensor.__torch_function__>,2.0.1,None
pytorch,https://github.com/pytorch/pytorch/issues/107125,2023-08-14T09:18:31Z,combining `vmap` with NN containing `MaxPool2d' leads to discrepancies in output,YES,"<torch.vmap>
<torch.func.jacrev>
<torch.nn.MaxPool2d>",2.0.1,<CUDA version> 11.7
pytorch,https://github.com/pytorch/pytorch/issues/107102,2023-08-13T18:00:25Z,Error when using sparse_coo tensor with optimizer,YES,"torch.to_sparse_coo  
torch.optim.Adam  
torch.mul  
torch.add  
torch.sum  
torch.relu  
torch.square  
torch.zeros  
torch.autograd.backward  ",2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/107087,2023-08-12T16:21:00Z,RuntimeError with operations on torch.float8_e5m2 and torch.float_e4m3fn data types,YES,"torch.float8_e5m2  
torch.float8_e4m3fn",2.1.0,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/107081,2023-08-12T08:02:07Z,[FSDP] summon_full_params won't change parameters,YES,<torch.FSDP.summon_full_params>,<PyTorch 2.0.0>,11.7
pytorch,https://github.com/pytorch/pytorch/issues/107016,2023-08-11T06:25:40Z,cov onnx error,YES,torch.onnx.dynamo_export,2.1.0,11.8.89
pytorch,https://github.com/pytorch/pytorch/issues/106989,2023-08-10T20:41:13Z,`ray` multiprocessing interference by torch import,YES,<torch>,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/106845,2023-08-09T05:40:03Z,`1/torch.inf` produce inconsistent results,YES,<torch.inf>,2.0.0,11.2
pytorch,https://github.com/pytorch/pytorch/issues/106780,2023-08-08T13:32:31Z,inf and nan are mapped to quant_min in torch.fake_quantize_per_tensor_affine,YES,torch.fake_quantize_per_tensor_affine,<PyTorch 2.0.1>,11.8
pytorch,https://github.com/pytorch/pytorch/issues/106748,2023-08-08T02:34:24Z,[FX][ONNX][exporter] Failed to export traced fx graph to onnx model,YES,torch.onnx.dynamo_export,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/106711,2023-08-07T16:01:14Z,'CUDA out of memory' when using a GPU services for reinforcement learning in Torch rpc tutorial,YES,<torch.distributed.rpc.api>,2.0.1,12.2
pytorch,https://github.com/pytorch/pytorch/issues/106690,2023-08-07T04:34:47Z,DDP grads not synced when static_graph=True and module output is a dict subclass?,YES,torch.nn.parallel.DistributedDataParallel,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/106665,2023-08-05T12:53:17Z,Inconsistency between CPU and GPU for `Linear()` layer with input size 0,YES,torch.nn.Linear,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/106630,2023-08-04T18:47:08Z,no_grad() changes output of TransformerDecoder module during evaluation ,YES,"torch.no_grad  
torch.nn.TransformerDecoder  
torch.nn.TransformerDecoderLayer  ",2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/106579,2023-08-03T21:47:27Z,"Boolean valued images loaded from disk, when converted to torch int/float tensor, the True valued pixels gets converted to 255 instead of 1",YES,"torch.tensor  
torch.int  
torch.float  ",2.0.1,"11.1, 10.2, 11.7"
pytorch,https://github.com/pytorch/pytorch/issues/106533,2023-08-03T07:41:47Z,CUDA device support does not register allocator to c10::GetAllocator(...),YES,"<torch.c10::GetAllocator>  
<torch.c10::cuda::CUDACachingAllocator::get>  
<torch.c10::SetAllocator>",<PyTorch 2.0.1>,11.7
pytorch,https://github.com/pytorch/pytorch/issues/106485,2023-08-02T21:48:59Z,Increasing batch size makes network forward 1000 times slower,YES,"<torch.nn.Embedding>
<torch.nn.Conv2d>
<torch.cuda.Event>
<torch.autocast>",<PyTorch 2.0.0>,11.7
pytorch,https://github.com/pytorch/pytorch/issues/106251,2023-07-29T06:31:31Z, Improve Error Message in MultiMarginLoss for Inconsistent Target Size,YES,torch.nn.MultiMarginLoss,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/106243,2023-07-29T01:03:48Z,OneCycleLR's state_dict includes a full reference to the optimizer,YES,torch.optim.lr_scheduler.OneCycleLR,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/106128,2023-07-27T12:15:41Z,Torchscript optimizer incorrectly applies constant propagation to convert prim::ListConstruct() into prim::Constant,YES,torch.jit.script,<torch 2.0.1>,11.8
pytorch,https://github.com/pytorch/pytorch/issues/106121,2023-07-27T08:09:57Z,Got error when train models with more than one param_group in torch2.0,YES,<torch.optim.AdamW>,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/106110,2023-07-27T04:32:29Z,llama model failed for dynamic shape path,YES,"torch._dynamo.eval_frame  
torch._dynamo.convert_frame  
torch._dynamo.utils  
torch._dynamo.guards  
torch.fx.experimental.symbolic_shapes  ",2.1.0,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/106011,2023-07-26T05:38:37Z,Misleading error message in multilabel_margin_loss when passing incompatible tensor dimensions,YES,torch.nn.MultiLabelMarginLoss,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/106006,2023-07-26T03:43:07Z,[torch.compile] assertion sometimes ignored with inductor backend,YES,"torch.compile  
torch._dynamo.reset",2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/105982,2023-07-25T23:03:30Z,"vmap and rnn/lstm ""accessing '.data' under vmap transform is not allowed""",YES,"<torch.vmap>  
<torch.nn.LSTM>  
<torch.func.functional_call>  
<torch.nn.utils.stateless._functional_call>  
<torch.nn.modules.rnn>  ",2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/105916,2023-07-25T12:41:36Z,Missing coalesced flag from `torch.autograd.Function.backward`,YES,torch.autograd.Function.backward,<torch 2.0.1>,None
pytorch,https://github.com/pytorch/pytorch/issues/105878,2023-07-24T22:55:32Z,FakeTensor detach() gives meta tensor other than FakeTensor under `torch._C._DisableTorchDispatch()`,YES,torch.detach,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/105872,2023-07-24T21:16:19Z,PyTorch 2.0.x `CUDA error: operation not supported` when `Tensor.to` a different device,YES,torch.Tensor.to,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/105846,2023-07-24T15:49:50Z,Strange backward behavior with sparse tensors,YES,torch.sparse_coo_tensor,<torch 2.0.1>,None
pytorch,https://github.com/pytorch/pytorch/issues/105840,2023-07-24T15:00:59Z,[FSDP] FSDP doesn't work (random accuracy performance) when using `param_init_fn` and `sync_module_states=True`,YES,"<torch.distributed.fsdp.param_init_fn>
<torch.distributed.fsdp.sync_module_states>",2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/105802,2023-07-23T15:31:23Z,Errors while trying to finetune compiled transformers model,YES,<torch.compile>,PyTorch 2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/105768,2023-07-21T22:03:11Z,torch.compile uses more memory when using less parameters,YES,<torch.compile>,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/105726,2023-07-21T06:56:04Z,There is a big precision error between A100 and 3090 when using torch.matmul with fp16 precision,YES,<torch.matmul>,2.0.1,"12.1, 12.0, 11.7"
pytorch,https://github.com/pytorch/pytorch/issues/105644,2023-07-20T04:57:33Z,Tensor subclass is not preserved during backward with gradient checkpointing,YES,torch.utils.checkpoint,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/105548,2023-07-19T17:23:18Z,torch.sparse.sampled_addmm doesn't compute gradients for 3D tensors,YES,torch.sparse.sampled_addmm,PyTorch 2.0.1,cu117
pytorch,https://github.com/pytorch/pytorch/issues/105304,2023-07-17T05:03:46Z,"Backward pass with sparse parameters results in error ""Sparse division requires a scalar or zero-dim dense tensor divisor""",YES,<torch.sparse.addmm>,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/105279,2023-07-15T22:19:55Z,[Dynamo][Compile]Torch compile with dynamic shapes not working,YES,"<torch.compile>
<torch._dynamo.optimize>",2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/105068,2023-07-12T16:41:03Z,[linalg] test_ops.py::test_python_ref_meta__refs_linalg_svd_cpu_complex failing,YES,torch.linalg.svd,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/104981,2023-07-11T15:15:08Z,NotImplementedError: Could not run 'aten::_spdiags' with arguments from the 'CUDA' backend.,YES,torch.sparse.spdiags,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/104959,2023-07-11T12:33:15Z,Saving a LightningModule torch.jit.ScriptModule is incompatible with torch.amp.autocast,YES,"torch.amp.autocast  
torch.jit.script",2.0.1,"The issue description does not explicitly mention a specific CUDA version. Therefore, based on the information provided, I cannot identify a CUDA version. If you are looking for a version related to the PyTorch installation, it is generally good practice to check the official PyTorch website or the installation documentation for compatibility with your specific setup."
pytorch,https://github.com/pytorch/pytorch/issues/104875,2023-07-10T14:28:32Z,torch/testing/_comparison.py: If you are a user and see this message during normal operation please file an issue,YES,torch.testing.assert_close,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/104867,2023-07-10T12:19:05Z,rfftn and irfftn operations in pt2 return different results compared to v1.12.1,YES,"<torch.fft.rfftn>
<torch.fft.irfftn>",2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/104857,2023-07-10T02:44:26Z,Torch's `LayerNorm` and Adam optimizer vs those in tensorflow,YES,"torch.LayerNorm  
torch.optim.Adam",<PyTorch 2.0.0>,11.7
pytorch,https://github.com/pytorch/pytorch/issues/104761,2023-07-07T05:36:36Z,"Torch Filename Storage hangs on ""file_system"" sharing strategy after in-place fill",YES,torch.multiprocessing.set_sharing_strategy,"2.0.0
2.0.1",11.8
pytorch,https://github.com/pytorch/pytorch/issues/104732,2023-07-06T20:08:35Z,"torch.jit.trace says ""Arguments for call are invalid"" on torch.ops.aten.sub(3, x, alpha=3)",YES,torch.ops.aten.sub,2.1.0,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/104701,2023-07-06T12:17:30Z,System memory leak when using different input size of torch.nn.Conv3d,YES,torch.nn.Conv3d,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/104698,2023-07-06T10:31:43Z,Incorrect Error Message Ordering for nn.AdaptiveAvgPool2d with Incorrect output_size ,YES,torch.nn.AdaptiveAvgPool2d,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/104697,2023-07-06T10:23:34Z,LSTM built-in dropout not reproducible on GPU,YES,torch.nn.LSTM,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/104620,2023-07-05T08:36:54Z,`torch.distributed.rpc.backend_registry.register_backend` fails to update `BackendType` enum,YES,torch.distributed.rpc.backend_registry.register_backend,2.0.0,None
pytorch,https://github.com/pytorch/pytorch/issues/104589,2023-07-04T08:27:51Z,Issue with FSDP does not reduce memory footprint  when scaling up GPUs,YES,"<torch.distributed.fsdp.FullyShardedDataParallel>  
<torch.distributed.fsdp.wrap.size_based_auto_wrap_policy>  
<torch.distributed.fsdp.ShardingStrategy>  
<torch.distributed.fsdp.fully_sharded_data_parallel.CPUOffload>  
<torch.distributed.fsdp.fully_sharded_data_parallel.BackwardPrefetch>  
",2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/104516,2023-07-03T05:41:28Z,vec_test_all_types_xxx with dtype c10::complex<float> and c10::complex<double> has failures on division,YES,"<torch.vec_test_all_types_AVX2>
<torch.vec_test_all_types_AVX512>",2.1.0,None
pytorch,https://github.com/pytorch/pytorch/issues/104506,2023-07-02T07:33:57Z,TImeout in NCCL doesn't work,YES,<torch.distributed.init_process_group>,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/104411,2023-06-29T13:44:13Z,"RuntimeError: t == DeviceType::CUDA INTERNAL ASSERT FAILED at HIPGuardImplMasqueradingAsCUDA.h:60, please report a bug to PyTorch",YES,torch.nn.modules.module,2.0.1,11.5.119
pytorch,https://github.com/pytorch/pytorch/issues/104322,2023-06-28T06:13:44Z,Illegal Memory Access on H100 `TestSparseCompressedTritonKernelsCUDA.test_triton_sampled_addmm_block_size_16_cuda_bfloat16`,YES,"torch.manual_seed  
torch.cuda.manual_seed_all  ",2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/104301,2023-06-27T21:58:34Z,Attempt to use minifier on sam model fails,YES,<torch.compile>,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/104297,2023-06-27T21:01:43Z,"torch.distributed.all_to_all_single & alltoall_base, size limit INT_MAX",YES,"torch.distributed.all_to_all_single  
torch.distributed.alltoall_base  ",<PyTorch 2.0.1>,11.7
pytorch,https://github.com/pytorch/pytorch/issues/104194,2023-06-26T16:37:04Z,View ops on fake tensors can dispatch `detach`es to backend kernels,YES,<torch.detach>,2.1.0,None
pytorch,https://github.com/pytorch/pytorch/issues/104174,2023-06-26T07:44:44Z,distributed.scatter memory leak in source rank,YES,torch.distributed.scatter,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/104168,2023-06-26T00:46:38Z,Incorrect Reduce collective result with `_coalescing_manager` ,YES,"<torch.distributed.reduce>  
<torch.distributed.distributed_c10d._coalescing_manager>  ",<PyTorch Version> 2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/104152,2023-06-25T04:06:16Z,Error 101: invalid device ordinal (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.),YES,torch.cuda.is_available,<torch 2.0.1>,11.8
pytorch,https://github.com/pytorch/pytorch/issues/104147,2023-06-24T18:55:11Z,PyTorch2.0 ROCM LayerNorm HIP error: invalid configuration,YES,<torch.layer_norm>,2.0.1,N/A
pytorch,https://github.com/pytorch/pytorch/issues/104102,2023-06-23T13:29:40Z,Inductor does not check input SymInt invariant on GraphModules passed in,YES,"<torch.ops.aten.sym_size>
<torch._inductor.config>
<torch._dynamo.config>
<torch._inductor.inductor_prims>
<torch._dynamo.repro.after_aot>
<torch._inductor.codecache>
<torch._inductor.cudagraph_trees>
<torch._inductor.utils>
<torch._inductor.hooks>",2.1.0,12.0
pytorch,https://github.com/pytorch/pytorch/issues/104041,2023-06-22T13:47:39Z,Scripted model is loaded on GPU but the inference seems to utilize the CPU with zero GPU utilization,YES,"<torch.jit.script>  
<torch.jit.save>  ",<torch 2.0.1>,11.8
pytorch,https://github.com/pytorch/pytorch/issues/104026,2023-06-22T06:37:44Z,Is memory-efficient FSDP initialization intended to be possible with torch.device('meta')?,YES,<torch.distributed.fsdp.FullyShardedDataParallel>,PyTorch 2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/103879,2023-06-20T11:57:06Z,Can ``torch.vmap`` add ``grad_fn``= SelectBackward when maping over some dimension of the inputs?,YES,<torch.vmap>,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/103848,2023-06-19T14:21:31Z,torch compile aten::floor_divide error,YES,<torch.aten::floor_divide>,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/103831,2023-06-19T08:52:02Z,[dynamo] AssertionError for custom iterable nn.Module,YES,torch.compile,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/103802,2023-06-17T15:25:30Z,Eager PTDQ Performs Worse Than Non-Quantized Linear Layer on CPU(in Terms of Speed),YES,torch.ao.quantization.quantize_dynamic,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/103752,2023-06-16T15:32:09Z,Pytorch not calling to C code from a docker container,YES,<torch._has_torch_function_variadic>,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/103737,2023-06-16T04:41:36Z,[FSDP] train throughput become slow down when loaded shard optimizer dict,YES,<torch.optim>,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/103730,2023-06-16T02:47:30Z,[FSDP] save model checkpoint with StateDictType.LOCAL_STATE_DICT and LocalStateDictConfig(offload_to_cpu=True) fail,YES,<torch.distributed.fsdp.state_dict_type>,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/103589,2023-06-14T12:57:48Z,`interpolate` with `antialias=True` on CUDA doesn't work if the difference of spatial size is large,YES,<torch.nn.functional.interpolate>,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/103581,2023-06-14T06:34:55Z,Passing dict in datapipe/dataset will have memory leak problem,YES,"torch.utils.data.DataLoader  
torchdata.datapipes.iter.IterableWrapper  
torchdata.dataloader2.DataLoader2  
torch.utils.data.Dataset",2.0.0,"The description does not explicitly mention a CUDA version. However, it does provide versions for PyTorch and TorchData, which are:

- torch version: 2.0.0
- torchdata version: 0.6.0

To provide an answer based on the context, it is common for PyTorch 2.0.0 to be compatible with CUDA versions like 11.6 or 11.7, but these specifics are not provided in the text.

Therefore, since no CUDA version is explicitly mentioned, the answer would be:

<no CUDA version specified>"
pytorch,https://github.com/pytorch/pytorch/issues/103422,2023-06-12T10:06:36Z,Possible memory leak when using Torch and Torchvision in conjunction with XGBoost ,YES,torchvision.models.vit_b_16,PyTorch 2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/103417,2023-06-12T08:08:53Z,"Torch  model compile error  ""/usr/bin/ld: cannot find -lcuda""  though cuda is installed via run file",YES,<torch.compile>,2.0.0,12.0
pytorch,https://github.com/pytorch/pytorch/issues/103397,2023-06-11T20:55:04Z,LayerNorm freeze processes using torch multiprocessing,YES,<torch.nn.LayerNorm>,2.0.1,"The issue description does not explicitly mention a CUDA version. Therefore, it is not possible to identify a specific CUDA version from the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/103372,2023-06-10T18:28:56Z,ImportError: cannot import name 'Store' from 'torch.distributed',YES,torch.distributed.Store,2.0.0,11.4
pytorch,https://github.com/pytorch/pytorch/issues/103254,2023-06-08T17:05:25Z,Unexpected High PCIe traffic in Distributed Training since PT 2,YES,<torch.distributed>,<Library Version> 2.1.0,CUDA 11.8
pytorch,https://github.com/pytorch/pytorch/issues/103189,2023-06-07T19:34:51Z,(fsdp - maybe a bug) SHARDED_STATE_DICT returns tensor with no data,YES,torch.distributed.fsdp.StateDictType,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/103161,2023-06-07T17:18:19Z,Calling jacrev with LSTM and functional_call  gives error,YES,"torch.func.functional_call
torch.func.jacrev
torch.func.jacfwd
torch.nn.LSTM",2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/103093,2023-06-06T17:15:47Z,Inconsistent memory allocation using FSDP between PT 2.0 and Nightlies,YES,<torch.distributed.fsdp>,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/103089,2023-06-06T15:39:17Z,"[OOM] Unable to convert 30B model to ONNX, using 4x A100's",YES,torch.onnx,<torch==2.0.0>,cu118
pytorch,https://github.com/pytorch/pytorch/issues/102948,2023-06-05T03:16:15Z,[onnx] aten::cumprod cannot be exported to ONNX,YES,torch.cumprod,2.0.1,cu117
pytorch,https://github.com/pytorch/pytorch/issues/102936,2023-06-04T16:17:09Z,torch.cuda.is_available() returns False on GTX 1650 with cuda 11.7 and torch==2.0.0+cpu,YES,<torch.cuda.is_available>,"2.0.0, 2.0.1",11.7
pytorch,https://github.com/pytorch/pytorch/issues/102830,2023-06-02T07:43:17Z,Unknow error when using `make_graphed_callables`,YES,<torch.cuda.make_graphed_callables>,2.0.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/102821,2023-06-02T06:17:56Z,"Unable to resume job using FSDP with 64 nodes, errors appeared during loading sharded optimizer state dict ",YES,<torch.distributed.checkpoint.optimizer.load_sharded_optimizer_state_dict>,<torch 2.0.1>,11.7
pytorch,https://github.com/pytorch/pytorch/issues/102812,2023-06-02T03:21:40Z,[DTensor] Error in distribute_module with module._apply,YES,"<torch.distributed._tensor.api.distribute_module>
<torch.nn.modules.module._apply>
<torch.distributed._tensor.api.distribute_tensor>",2.1.0,11.5
pytorch,https://github.com/pytorch/pytorch/issues/102731,2023-06-01T14:01:57Z,"[FSDP]  When amp is enabled, there is a noticeable difference during training between `FSDP `and `DDP`",YES,"<torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel>  
<torch.nn.parallel.DistributedDataParallel>  
<torch.cuda.amp.GradScaler>  
<torch.cuda.amp.autocast>  ",2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/102667,2023-05-31T23:02:44Z,Duplicate parameters (_flat_params and original params) in the state_dict when using `use_orig_params=True` and `StateDictType.LOCAL_STATE_DICT`,YES,<torch.distributed.fsdp.StateDictType.LOCAL_STATE_DICT>,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/102610,2023-05-31T11:06:47Z,Cannot invoke prims.sum with output_dtype,YES,torch.ops.prims.sum,2.1.0,None
pytorch,https://github.com/pytorch/pytorch/issues/102609,2023-05-31T10:58:54Z,[prims] torch.ops.aten.le decomposition confuses scalars and tensors,YES,"torch.ops.aten.le  
torch.ops.prims.le",2.1.0,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/102459,2023-05-28T22:16:48Z,Matrix multiplication performance regression in case of an additional dimension of size 1,YES,"torch.unsqueeze  
torch.einsum  ",PyTorch 2.0.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/102438,2023-05-27T15:25:05Z,DDP multi node multi gpu inconsistent params,YES,<torch.distributed>,<PyTorch 2.0.1>,11.8
pytorch,https://github.com/pytorch/pytorch/issues/102269,2023-05-25T13:47:05Z,Import of torch breaks standard multiprocessing,YES,torch,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/102197,2023-05-24T19:55:43Z,torch.func.jvp fails when acting on a DistributedDataParallel model,YES,torch.func.jvp,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/102085,2023-05-23T13:50:21Z,BatchNorm can't be symbolically traced with torch.fx as a standalone module ,YES,"torch.fx.symbolic_trace  
torch.nn.BatchNorm2d",<PyTorch 2.0.0>,11.7
pytorch,https://github.com/pytorch/pytorch/issues/102078,2023-05-23T12:33:00Z,AddressSanitizer: heap-buffer-overflow in test_comprehensive_nn_functional_embedding_bag_cpu_bfloat16 ,YES,<torch._embedding_bag_dense_backward>,2.1.0,None
pytorch,https://github.com/pytorch/pytorch/issues/101890,2023-05-19T19:07:17Z,Can't reproduce/non-deterministic results with CUDA,YES,"torch.use_deterministic_algorithms  
torch.backends.cudnn.benchmark  
torch.manual_seed  
torch.cuda.manual_seed_all  ",2.0.1,CUDA 11.8
pytorch,https://github.com/pytorch/pytorch/issues/101850,2023-05-19T03:12:11Z,Unexpected modification to CPU affinity of Dataloader workers,YES,torch.utils.data.DataLoader,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/101716,2023-05-17T18:55:40Z,"[TorchScript] aten::__and__ with argument types: Tensor, bool not supported",YES,<torch.jit.trace>,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/101653,2023-05-17T01:17:16Z,Unexpected behavior comparing uint8 tensor to value greater than 255,YES,<torch.tensor>,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/101632,2023-05-16T23:58:51Z,torch.profiler.profile has an empty python replay stack under certain circumstances,YES,<torch.profiler.profile>,"2.0.0, 2.0.1",11.7
pytorch,https://github.com/pytorch/pytorch/issues/101529,2023-05-16T07:34:32Z,Inconsistent performance degradation of 3x3 convolution (torch 2.0.1+cu118),YES,<torch.nn.Conv2d>,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/101428,2023-05-15T17:57:33Z,2D inputs to linear layers run up to 25% slower than 4D ones on some Nvidia GPUs,YES,torch.nn.Linear,2.0.1,"11.7, 11.8"
pytorch,https://github.com/pytorch/pytorch/issues/101385,2023-05-15T06:28:47Z,torch.Tensor.is_sparse returns false for non-COO sparse tensors,YES,torch.Tensor.is_sparse,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/101359,2023-05-14T12:01:53Z,RuntimeError in Scaled Dot Product Attention Tutorial Code,YES,<torch.functional.scaled_dot_product_attention>,2.0.0,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/101356,2023-05-14T08:31:07Z,inductor: inductor conv2d get a different size and stride with eager mod when input channel is zero,YES,torch.nn.Conv2d,2.1.0,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/101265,2023-05-12T06:49:16Z,"Noisy warning - torch.fx.experimental.symbolic_shapes: [WARNING] Ignored guard (...), this could result in accuracy problems",YES,torch.fx.experimental.symbolic_shapes,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/101249,2023-05-12T02:59:52Z,`einsum` is about 40x slower on CUDA than manually multiplying and summing,YES,torch.einsum,<PyTorch 2.0.0>,12.1
pytorch,https://github.com/pytorch/pytorch/issues/101241,2023-05-12T01:28:50Z,Different results with vmap when using torch.jit.script,YES,"<torch.jit.script>
<torch.jit.fork>
<torch.func.vmap>
<torch.autograd.grad>",2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/101188,2023-05-11T14:01:14Z,Weird dataloader performance degradation caused by torch and numpy import order,YES,<torch.utils.data.DataLoader>,<PyTorch 2.0.0>,11.8
pytorch,https://github.com/pytorch/pytorch/issues/101185,2023-05-11T13:16:02Z,Pure virtual function call exception on Python interpreter exit when using debug wheel,YES,torch.impl.dispatch.PythonKernelHolder,2.0.1,None
pytorch,https://github.com/pytorch/pytorch/issues/101160,2023-05-11T06:33:41Z,Fine-tuning HuggingFace wav2vec 2.0 with `torch.compile`,YES,"<torch.compile>
<torch.autograd.grad>
<torch.tensor>
<torch._dynamo>
<torch._functorch.aot_autograd>
<torch._dynamo.config>",2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/101159,2023-05-11T06:15:29Z,Inconsistency between GPU memory usage in torch.cuda.memory_summary and nvidia-smi,YES,torch.cuda.memory_summary,2.0.0,"10.1, 11.0"
pytorch,https://github.com/pytorch/pytorch/issues/101031,2023-05-10T00:05:06Z,[PT2] torch.compile doesn't perform horizontal fusion,YES,torch.compile,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/100989,2023-05-09T17:27:03Z,"RuntimeError: nonzero is not supported for tensors with more than INT_MAX  elements, file a support request",YES,<torch.nn.utils.prune.global_unstructured>,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/100985,2023-05-09T16:44:54Z,"native_batch_norm has different size results on ""CPU"" vs ""META"" device",YES,torch.ops.aten.native_batch_norm,2.1.0,None
pytorch,https://github.com/pytorch/pytorch/issues/100932,2023-05-09T01:26:39Z,torch.concat fails with float16 input in autocast(device_type=cpu) context,YES,torch.concat,2.0.0,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/100838,2023-05-07T22:18:48Z,Dynamo infers different return type vs. eager for `torch.ops.aten`,YES,"<torch.ops.aten.baddbmm>
<torch.ops.aten.full>
<torch.ops.aten.threshold_backward>",2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/100807,2023-05-06T18:19:52Z,[torch.compile] returns output with WRONG SHAPE after `cat_slice_cat`,YES,<torch.compile>,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/100796,2023-05-06T08:28:03Z,[Quant][pt2e] Failed to run pt2e flow on LLaMA,YES,torch._dynamo.export,2.1.0,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/100784,2023-05-06T02:54:35Z,[CUDA RPC] Incorrect results of GPU Tensor transferring using RPC when parallelized with other GPU programs,YES,<torch.distributed.rpc>,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/100730,2023-05-05T17:40:03Z,`torch::jit::EliminateExceptions` lowering pass never completes on specific model,YES,<torch::jit::EliminateExceptions>,2.1.0,cu117
pytorch,https://github.com/pytorch/pytorch/issues/100725,2023-05-05T16:31:24Z,[CUDA RPC] Incorrect messages in CUDA Support RPC when parallelized with other GPU programs,YES,<torch.distributed.rpc>,"<Library Version>
2.0.0",11.7
pytorch,https://github.com/pytorch/pytorch/issues/100636,2023-05-04T17:27:22Z,tracing does not work when torch.distributions is involved,YES,torch.distributions.MultivariateNormal,<PyTorch 2.0.0>,11.7
pytorch,https://github.com/pytorch/pytorch/issues/100582,2023-05-03T21:29:21Z,"Synchronization issue when combining DPP and RPC - ""Parameter marked twice""",YES,"torch.distributed.rpc  
torch.nn.parallel.DistributedDataParallel  
torch.distributed",PyTorch 2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/100253,2023-04-28T16:43:13Z,profiler.export_stacks doesn't return stack trace unless experimental_config is provided,YES,<torch.profiler.export_stacks>,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/100158,2023-04-27T09:27:21Z,`torch.sparse_csc_tensor` matrix multiplication produces MKL error SPARSE_STATUS_ALLOC_FAILED when density is too high,YES,<torch.sparse_csc_tensor>,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/100087,2023-04-26T12:44:11Z,TransformerEncoderLayer behavior inconsistent between training and evaluation mode,YES,<torch.nn.TransformerEncoderLayer>,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/100069,2023-04-26T05:53:42Z,Issue with FSDP + HuggingFace generate,YES,"<torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel>  
<torch.nn.functional.embedding>  
<torch.nn.Module.forward>  
<torch.nn.modules.module.Module._call_impl>  
<torch.nn.modules.module.Module.forward>  
<torch.nn.modules.module.Module._call_impl>  
<torch.nn.modules.module.Module.forward>  
<torch.nn.modules.module.Module._call_impl>  
<torch.nn.modules.sparse.SparseEmbedding.forward>  
<torch.nn.functional.embedding>  
<torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel.summon_full_params>  
<torch.nn.Module.generate>  ",2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/100051,2023-04-26T01:53:22Z,Issue of HistogramObserver to handle abnormal value,YES,<torch.ao.quantization.observer.HistogramObserver>,2.1.0,None
pytorch,https://github.com/pytorch/pytorch/issues/99932,2023-04-24T22:17:50Z,FSDP + gradient clipping raises an odd warning with the simplest model on torch 2.0,YES,"torch.distributed.fsdp.FullyShardedDataParallel  
torch.distributed.fsdp.api.ShardingStrategy",2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/99652,2023-04-20T19:17:12Z,DistributedDataParallel doesn't work with complex buffers,YES,torch.nn.parallel.DistributedDataParallel,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/99556,2023-04-19T19:09:46Z,torch.func.jacrev fails if model contains full_backward_hook,YES,torch.func.jacrev,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/99432,2023-04-18T17:39:17Z,[DTensor] parallelize_module failed with nn.Transformer and the PairwiseParallel plan,YES,"<torch.distributed.tensor.parallel.parallelize_module>
<torch.nn.Transformer>
<torch.distributed.tensor.parallel.PairwiseParallel>",2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/99410,2023-04-18T11:41:22Z,"torch.nn.functional.multilabel_margin_loss cuda lacks checking of ""out of bound""",YES,"torch.nn.functional.multilabel_margin_loss  
torch.nn.MultiLabelMarginLoss",2.0.0,cu118
pytorch,https://github.com/pytorch/pytorch/issues/99407,2023-04-18T09:43:44Z,Torch.fx.symbolic_trace removes some of the keys from module state_dict,YES,torch.fx.symbolic_trace,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/99372,2023-04-17T21:51:17Z,PyTorch 2.0.0 encountered CUDA error: an illegal memory access was encountered,YES,<torch>,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/99265,2023-04-16T13:44:32Z,the error message of torch.addcmul is wrong,YES,torch.addcmul,2.0.0,cu118
pytorch,https://github.com/pytorch/pytorch/issues/99149,2023-04-14T13:59:29Z,"Spectral Normalization can not be applied to Conv{1,2,3}d",YES,torch.nn.utils.parametrizations.spectral_norm,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/99023,2023-04-13T07:49:55Z,Dtype changes while going from FX graph -> Torchscript,YES,"torch.aten.randint  
torch.aten.zeros  
torch.aten.ones_like  
torch.aten.neg  
torch.aten.scatter_","<Library Version>
2.1.0",11.8
pytorch,https://github.com/pytorch/pytorch/issues/99012,2023-04-13T03:40:26Z,"[BUG]Float32 attention mask not working with torch.autocast(""cpu"")",YES,"torch.autocast  
torch.nn.MultiheadAttention",<torch 2.0.0>,11.7
pytorch,https://github.com/pytorch/pytorch/issues/98948,2023-04-12T16:53:22Z,behaviour of `torch.tensor()` changes after editing `Tensor.__getitem__`,YES,<torch.tensor>,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/98939,2023-04-12T15:19:18Z,"torch.dist with minus norm returns tensor(0.), while with -inf can return result",YES,torch.dist,PyTorch 2.0.0,118
pytorch,https://github.com/pytorch/pytorch/issues/98929,2023-04-12T09:54:46Z,torch.sparse_csr_tensor() stops gradients,YES,torch.sparse_csr_tensor(),2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/98861,2023-04-11T19:00:20Z,Sparse Tensor not working for `torch.cat`,YES,torch.cat,<torch 2.0.0>,12.1
pytorch,https://github.com/pytorch/pytorch/issues/98814,2023-04-11T04:35:21Z,autocast does not work properly on embedding module,YES,<torch.autocast>,<PyTorch 2.0.0>,11.8
pytorch,https://github.com/pytorch/pytorch/issues/98695,2023-04-09T16:49:20Z,Torchscript: Name Mangling prevents Type Refinement,YES,torch.jit.script,PyTorch 2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/98675,2023-04-08T16:29:08Z,torch.matmul with batched CSR matrix,YES,torch.matmul,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/98668,2023-04-08T06:01:09Z,Cannot use `checkpoint_sequential` with `torch.compile`,YES,torch.checkpoint_sequential,"2.0.0, 2.1.0","11.7, 11.8"
pytorch,https://github.com/pytorch/pytorch/issues/98533,2023-04-06T20:07:03Z,Dynamo compiled graph gets overwritten by eager in a data dependent branch when False branch is empty,YES,<torch._dynamo>,2.1.0,cu117
pytorch,https://github.com/pytorch/pytorch/issues/98498,2023-04-06T12:36:57Z,"Higher order derivatives not working when setting compute device to `torch.device(""mps"")`",YES,"<API Name>
torch.func.functional_call
torch.autograd.grad
aten::linear_backward",2.0.0,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/98422,2023-04-05T16:34:29Z,[FX] Symbolic trace over `torch.Tensor.${fn}` APIs,YES,"torch.Tensor.flatten  
torch.Tensor.flip  ",2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/98210,2023-04-03T13:27:17Z,torch.jit.script + legacy executor mode has diff in some pattern,YES,torch.jit.script,torch==2.0.0,"The issue description does not explicitly mention a CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/98142,2023-04-01T13:35:10Z,double free or corruption (fasttop),YES,"torch::TensorOptions  
torch::normal  
torch::rand  
value_tensor.to  ",2.0.0,11.8.89
pytorch,https://github.com/pytorch/pytorch/issues/98136,2023-04-01T08:58:55Z,A Segment Fault can be triggered in torch._grid_sampler_2d_cpu_fallback,YES,torch._grid_sampler_2d_cpu_fallback,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/97992,2023-03-30T16:45:08Z,torch.compile not compatible with multiprocessing pool,YES,torch.compile,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97913,2023-03-29T21:40:31Z,BUG torch.jit.annotate on List + torch.stack give wrong DTYPE,YES,"torch.jit.annotate  
torch.stack  ",2.0.0,cu117
pytorch,https://github.com/pytorch/pytorch/issues/97909,2023-03-29T20:54:51Z,[dynamo][inline inbuilt nn modules]`torch.func.functional_call` doesn't work with compiled models,YES,torch.func.functional_call,<PyTorch 2.0.0>,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97876,2023-03-29T11:21:17Z,Missing torch import in _contextlib.py when using torch.jit._recursive,YES,"torch.jit.script  
torch.jit._recursive.create_script_module  
torch.jit.RecursiveScriptModule._construct  
torch.nn.functional.pad",<PyTorch 2.0.0>,11.8
pytorch,https://github.com/pytorch/pytorch/issues/97852,2023-03-29T04:18:46Z,Functionalize crashes on train_step GraphModule,YES,"<torch._enable_functionalization>
<torch._functorch.eager_transforms.functionalize>",2.1.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/97823,2023-03-28T20:39:52Z,Using `param in param_list` can trigger `non-singleton dimension` error?,YES,torch.nn.Parameter,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97784,2023-03-28T15:15:01Z,torch.compile fails with torch._dynamo.exc.TorchRuntimeError on  a function that contains a torch script module,YES,"torch.compile  
torch._dynamo.exc.TorchRuntimeError  
torch.jit.trace  ",2.0.0,"The issue description does not explicitly mention a CUDA version. Therefore, no CUDA version can be identified from the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/97783,2023-03-28T14:49:54Z,The first epoch is very slow when using torch.compile,YES,<torch.compile>,<PyTorch 2.0.0>,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97749,2023-03-28T06:15:05Z,Bug on Minified repro example ,YES,"<torch._dynamo>  
<torch._inductor>  
<torch._functorch>  
<torch._C._nn>  
<torch>  
<torch.nn>  ",2.0.0,"CUDA version: 7, 11.8"
pytorch,https://github.com/pytorch/pytorch/issues/97668,2023-03-27T13:54:17Z,sparse_csr_tensor matmul wrong output in bfloat16,YES,<torch.sparse_csr_tensor>,<PyTorch 2.0.0>,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97623,2023-03-26T08:17:50Z,Compile dynamic does not support GroupNorm in module,YES,"torch.compile  
torch.autograd.grad  
torch._dynamo.config  
torch._dynamo.utils  
torch._dynamo.eval_frame  
torch._dynamo.output_graph  
torch._dynamo.convert_frame  
torch._functorch.aot_autograd  
torch.nn.GroupNorm  ",2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97499,2023-03-24T05:44:00Z,`onnxrt` fails with compilations,YES,torch._dynamo.backends.onnxrt,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97395,2023-03-23T00:07:08Z,Expanded weights tests broken,YES,<torch.testing._internal.common_device_type>,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/97343,2023-03-22T12:27:52Z,Traced module shows non-deterministic behaviour on CUDA,YES,torch.jit.trace,<PyTorch 2.0.0>,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97329,2023-03-22T08:33:52Z,"torch.ops.aten.pow(2.0, 3) return unexpected value with complex type",YES,torch.ops.aten.pow,2.1.0,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/97286,2023-03-21T21:39:08Z,`jacrev` and `jacfwd` raise an error that `Sparse CSR tensors do not have strides`,YES,"torch.func.jacrev  
torch.func.jacfwd",2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97283,2023-03-21T20:52:00Z,`jacfwd` fails when computing the gradient for `channels_last` tensor,YES,<torch.func.jacfwd>,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97189,2023-03-20T21:57:52Z,torch.randint range for torch.int64 dtype seems wrong,YES,torch.randint,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97155,2023-03-20T15:28:06Z,Custom recurrent network takes very long to compile for long sequences,YES,<torch.compile>,<PyTorch 2.0.0>,11.8
pytorch,https://github.com/pytorch/pytorch/issues/97128,2023-03-20T06:01:36Z,"MultiHeadAttention, fast path broken with `bias=False` or uneven number of heads",YES,torch.nn.MultiheadAttention,<PyTorch 2.0.0>,11.8
pytorch,https://github.com/pytorch/pytorch/issues/97097,2023-03-18T20:42:07Z,torch.cuda.FloatTensor().normal_() generate (partially) different sample on different gpu machines,YES,torch.cuda.FloatTensor.normal_(),<PyTorch 2.0.0>,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97088,2023-03-18T11:01:11Z,A Segment Fault can be triggered in torch.embedding,YES,<torch.embedding>,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97087,2023-03-18T10:58:24Z,A Segment Fault can be triggered in torch.adjoint,YES,<torch.adjoint>,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97086,2023-03-18T10:53:40Z,A crash due to Floating Point Exception can be triggered in torch.index_select,YES,<torch.index_select>,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97016,2023-03-17T12:45:36Z,[Inductor] atomic_add does not support bf16,YES,torch.ops.aten.atomic_add,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/96908,2023-03-16T00:46:31Z,Doing inplace on a inplace view of tensor that retains_grad triggers internal assert,YES,<torch.autograd.gradcheck>,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96789,2023-03-15T00:30:22Z,`cumprod` triggers INTERNAL ASSERT FAILED when `out` is a tensor on cuda but input is on cpu,YES,<torch.cumprod>,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96779,2023-03-14T21:36:12Z, Segmentation fault (core dumped) during Torch finetuning (at random step),YES,<torch.enable_grad>,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96769,2023-03-14T20:15:43Z,`sparse.mm` triggers INTERNAL ASSERT FAILED when backwarding,YES,<torch.sparse.mm>,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96726,2023-03-14T05:01:38Z,Memory not release after jit.trace/freeze,YES,torch.jit,2.1.0,None
pytorch,https://github.com/pytorch/pytorch/issues/96704,2023-03-14T00:31:19Z,`logical_xx` operations trigger INTERNAL ASSERT FAIL when `input` is complex tensor on cuda and `other` is on cpu,YES,"torch.logical_and  
torch.logical_or  
torch.logical_xor",2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96693,2023-03-13T21:46:59Z,"torch.compile mode=""max-autotune"" precision appears to be lower",YES,torch.compile,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/96692,2023-03-13T21:42:15Z,[H100] `test_ops.py::TestFakeTensorCUDA.test_fake_crossref_backward_amp_nn_functional_scaled_dot_product_attention_cuda_float32` failed,YES,torch.nn.functional.scaled_dot_product_attention,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/96409,2023-03-09T09:07:17Z,"Initialization on `meta` device failing for models containing `nn.utils.weight_norm`, with `NotImplementedError: Could not run 'aten::_weight_norm_interface' with arguments from the 'Meta' backend.`",YES,"torch.nn.utils.weight_norm  
aten::_weight_norm_interface",2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96316,2023-03-08T18:29:32Z,`FractionalMaxPool3d` INTERNAL ASSERT FAILED when computing `jacrev`,YES,"torch.nn.FractionalMaxPool3d  
torch.func.jacrev",2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96277,2023-03-08T06:10:16Z,A Segment Fault can be triggered in torch.adaptive_max_pool1d with an edge case,YES,<torch.adaptive_max_pool1d>,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96276,2023-03-08T06:06:12Z,A Segment Fault can be triggered in torch.geqrf with an edge case,YES,<torch.geqrf>,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96275,2023-03-08T05:59:47Z,A Segment Fault can be triggered in torch.pinverse,YES,<torch.pinverse>,2.1.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96198,2023-03-07T15:25:32Z,dynamo + dict subclass + tensor instance check: NotImplementedError,YES,torch.Tensor,2.1.0,"The issue description does not mention any specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/95487,2023-02-24T17:23:46Z,Torch RPC on multiple nodes with GPU returns a EOF error,YES,"torch.distributed.rpc.init_rpc  
torch.distributed.rpc.shutdown  
torch.distributed.rpc.rpc_sync  
torch.distributed.rpc.TensorPipeRpcBackendOptions  ",2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/95309,2023-02-22T19:34:45Z,`torch.distributed.Store` triggers INTERNAL ASSER FAILED when seting,YES,torch.distributed.Store,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/95207,2023-02-21T13:52:13Z,"new backend privateuseone with ""to"" op",YES,<torch.to>,2.0.0,None
pytorch,https://github.com/pytorch/pytorch/issues/95161,2023-02-20T12:15:09Z,AOTAutograd based torch.compile doesn't capture manual seed setting in the graph,YES,"<torch.compile>
<torch.manual_seed>
<torch.rand>
<torch.allclose>",2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94966,2023-02-16T05:14:28Z,CSR matrix add_ error with RuntimeError: CUDA error: kernel launch failure when calling cusparseXcsrgeam2Nnz,YES,"torch.optim.sgd._single_tensor_sgd  
torch.sparse_coo_tensor.add_  ",2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94855,2023-02-14T21:47:26Z,torch.compile breaks reproducibility,YES,torch.compile,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94808,2023-02-14T09:09:10Z,Memory Corruption in torch.lstm caused by edge cases,YES,<torch.lstm>,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/94773,2023-02-13T22:24:56Z,`torch.compile` doesn't consider the alias tensor created by `tensor[:]`,YES,<torch.compile>,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94704,2023-02-12T16:32:30Z,`where` triggers INTERNAL ASSERT FAILED when `out` is a long tensor due to mixed types,YES,<torch.where>,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94698,2023-02-12T06:25:54Z,A segment fault can be triggered in torch.avg_pool1d,YES,<torch.avg_pool1d>,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/94696,2023-02-12T06:10:39Z,A segment fault can be triggered in torch.max_pool1d_with_indices,YES,<torch.max_pool1d_with_indices>,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/94669,2023-02-11T08:51:26Z,A segment fault can be triggered in torch.svd,YES,<torch.svd>,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/94668,2023-02-11T08:48:00Z,A segment fault can be triggered in torch.lstm with edge cases,YES,<torch.lstm>,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/94594,2023-02-10T13:50:03Z,A segment fault can be triggered in torch.histogramdd,YES,<torch.histogramdd>,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/94593,2023-02-10T13:46:52Z,Memory corruptions can be triggered in torch._remove_batch_dim,YES,<torch._remove_batch_dim>,<torch 2.0.0>,11.8
pytorch,https://github.com/pytorch/pytorch/issues/94591,2023-02-10T13:37:37Z,Issue with `upsample_nearest2d` decomposition,YES,<torch._C._nn.upsample_nearest2d>,2.0.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/94590,2023-02-10T13:28:31Z,A Segment Fault can be triggered in torch.affine_grid_generator,YES,<torch.affine_grid_generator>,2.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/94496,2023-02-09T08:08:58Z,[torch.compile + cuda stream] Dynamo captures only CUDA streams in FX graph,YES,<torch.cuda.stream>,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94451,2023-02-08T23:05:47Z,"`jacrev` raise ""Cannot access storage of TensorWrapper"" error when computing the grad of `storage`",YES,torch.func.jacrev,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94443,2023-02-08T22:12:06Z,A better error msg for `cuda.jiterator` when input is on `cpu`,YES,torch.cuda.jiterator,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94441,2023-02-08T21:59:36Z,`get_debug_state` a script function causes INTERNAL ASSERT FAILED,YES,<torch.jit.script>,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94336,2023-02-07T20:49:15Z,`scatter` fails the gradient computation in reverse mode for `src` when `index` is empty,YES,<torch.scatter>,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94333,2023-02-07T20:29:00Z,cpu log1p for bfloat16 gives wrong result.,YES,<torch.log1p>,2.0.0,12.0
pytorch,https://github.com/pytorch/pytorch/issues/94208,2023-02-06T19:30:11Z,`zeros_like` + `fill_` makes the gradient computation in forward mode fail,YES,<torch.zeros_like>,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94164,2023-02-06T02:33:00Z,Pytorch 2.0: Detection models from torchvision don't work with onnx and tensorrt backends,YES,"torch.compile  
torch.jit.script  
torch.as_tensor  
torch.device  
torchvision.models.detection.generalized_rcnn  
torchvision.models.detection.transform",2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94160,2023-02-05T23:26:29Z,Subclassed Tensors Decrease Training GPU Throughput by ~40% ,YES,<torch.Tensor>,2.0.0,"11.7, 11.8"
pytorch,https://github.com/pytorch/pytorch/issues/94115,2023-02-04T01:58:14Z,`cat` fails the gradient computation in forward mode with empty tensors when used with legacy vmap,YES,torch.cat,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94111,2023-02-04T01:32:54Z,`svd` triggers INTERNAL ASSERT FAILED when computing jacobian in forward mode,YES,<torch.svd>,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94086,2023-02-03T23:06:55Z,`MSELoss` fails to compute the gradients when inputs have different dtype,YES,<torch.nn.MSELoss>,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94085,2023-02-03T22:57:22Z,`unfold` fails in forward mode when unfolding a scalar tensor,YES,<torch.unfold>,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/93982,2023-02-02T19:44:24Z,`linalg.lstsq` fails the gradient computation in forward mode,YES,torch.linalg.lstsq,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/93002,2023-01-25T19:16:35Z,Replace pattern fails on incompatible function arguments,YES,torch.fx.replace_pattern,2.0.0,12.0
pytorch,https://github.com/pytorch/pytorch/issues/92987,2023-01-25T18:07:27Z,Test Failure: TestUpgraders.test_aten_div_scalar_at_3 on a big-endian machine (issue in torch.jit.load()),YES,torch.jit.load(),2.0.0,None
pytorch,https://github.com/pytorch/pytorch/issues/92910,2023-01-24T18:24:24Z,torch.jit.save() generates different contents in a file among different endian machines,YES,"torch.jit.save  
torch.jit.load",2.0.0,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/92804,2023-01-23T06:27:18Z,Cuda streams and torch.compile,YES,torch._dynamo.optimize,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/92742,2023-01-21T03:55:09Z,[JIT] Consecutive use of `addmm` Leads to Exception,YES,torch.addmm,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/92740,2023-01-21T03:41:21Z,[JIT] Applying `conv2d` over Constants Leads to Exception,YES,"torch.nn.functional.relu  
torch.nn.Conv2d  
aten::conv2d  ",2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/92674,2023-01-20T08:15:59Z,[JIT] `Linear` + `BatchNorm2d` Trigger Inconsistency between Eager Mode and JIT,YES,"torch.jit.optimize_for_inference  
torch.nn.Linear  
torch.nn.BatchNorm2d",2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/92594,2023-01-18T23:25:40Z,INTERNAL ASSERT FAILED when mixed dtypes for `addcmul_`,YES,<torch.addcmul_>,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/91863,2023-01-08T23:31:55Z,torch.Categorical samples indexes with 0 probability when given logits as argument,YES,torch.distributions.Categorical,2.0.0,"The issue description does not mention a specific CUDA version. It only provides information about the versions of Python packages related to PyTorch, but does not include any details about CUDA."
pytorch,https://github.com/pytorch/pytorch/issues/91439,2022-12-28T10:15:53Z,Pytorch2.0 doesn't support compiling GRU and RNN model,YES,"torch.compile  
torch.nn.GRU  
aten._cudnn_rnn_flatten_weight.default",2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/91437,2022-12-28T08:28:12Z,using Tensor subclass between vmap layers,YES,"<torch.overrides.is_tensor_like>
<torch.overrides.enable_reentrant_dispatch>
<torch.Tensor._make_wrapper_subclass>
<torch.Tensor>",2.0.0,11.6
