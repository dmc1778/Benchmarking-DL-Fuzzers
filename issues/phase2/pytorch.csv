pytorch,https://github.com/pytorch/pytorch/issues/133267,2024-08-12T20:44:01Z,`torch.ldexp` incorrectly returns infinity for `float64` inputs larger than `float32` max,YES,torch.ldexp,2.3.1+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/133265,2024-08-12T20:36:06Z,`torch.ldexp` incorrectly returns infinity if `exp` is larger than log2 of the max representable number,YES,torch.ldexp,2.3.1+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/133264,2024-08-12T20:26:33Z,`torch.ldexp` upcasts 16-bit inputs to 32 bits.,YES,torch.ldexp,2.3.1+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/133252,2024-08-12T18:45:29Z,"[Torch.Export] [OpenELM] Failed to Export OpenELM: ""Pop from Empty List""",YES,torch.export.export,"2.3.0, 2.4.0","The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a CUDA version based on the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/133227,2024-08-12T14:15:54Z,Issue Exporting PyTorch Model with pytorch3d.ops.ball_query to ONNX: Bug or Compatibility Problem?,YES,torch.pytorch3d.ops.ball_query,2.3.1+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/133220,2024-08-12T11:07:33Z,error occur when I use torch.onnx.export to generate onnx model,YES,torch.onnx.export,<Library Version> 2.3.1,"11.8, 11.6"
pytorch,https://github.com/pytorch/pytorch/issues/133204,2024-08-12T08:44:05Z,Add nondeterministic alert to `.scatter_`,YES,torch.scatter_,torch==2.5.0a0+gitf5e704a,12.5
pytorch,https://github.com/pytorch/pytorch/issues/133176,2024-08-11T00:06:42Z,[torch.fx] _FunctionalizationMetadataProp.propagate method can't use non-tensor args,YES,<torch.fx.passes.reinplace._FunctionalizationMetadataProp>,2.5.0.dev20240810+cpu,None
pytorch,https://github.com/pytorch/pytorch/issues/133174,2024-08-10T21:13:28Z,"Export on to_dense() fails on compressed cases, works for COO",YES,"<torch.export.export>  
<torch.Tensor.to_dense>  ",mainline,"The issue description does not mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the given text."
pytorch,https://github.com/pytorch/pytorch/issues/133172,2024-08-10T20:07:22Z,NameError('math is not defined') - malformed Triton codegen with math.trunc,YES,"<API Name>
torch.ops.aten.topk.default
torch.ops.aten.sort.default
torch.ops.aten.gather.default",<Library Version> main,"The GitHub issue description does not explicitly mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description provided."
pytorch,https://github.com/pytorch/pytorch/issues/133166,2024-08-10T13:47:51Z,[torch.compile] Integers stored on nn.Modules as dynamic causing errors,YES,torch.compile,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/133163,2024-08-10T11:43:57Z,`torch.load` with `weights_only=True` does not allow `bytes` or `bytearray`,YES,torch.load,torch==2.5.0.dev20240810+cpu,"The issue description does not mention a specific CUDA version. Therefore, the answer is:

<CUDA version>"
pytorch,https://github.com/pytorch/pytorch/issues/133138,2024-08-09T22:31:14Z,Layer normalization for Nested Tensors does not return the correct tensor types,YES,torch.nn.functional.layer_norm,fbcode H100 8/9/24,H100 8/9/24
pytorch,https://github.com/pytorch/pytorch/issues/133114,2024-08-09T18:52:59Z,Conv1D: `CUDNN_BACKEND_OPERATION: cudnnFinalize Failed cudnn_status: CUDNN_STATUS_BAD_PARAM`,YES,torch.nn.functional.grad.conv1d_weight,2.5.0.dev20240707+cu121,12.2
pytorch,https://github.com/pytorch/pytorch/issues/133093,2024-08-09T14:37:43Z,Inconsistent results of Conv2d in pytorch between CPU and CUDA,YES,torch.nn.Conv2d,2.3.0,"CUDA 11.8, CUDA 12.1"
pytorch,https://github.com/pytorch/pytorch/issues/133092,2024-08-09T14:11:00Z,Umbrella issue for Dynamo tracing through ProcessGroup related torch.distributed APIs,YES,torch.distributed.get_backend,main,"The issue description does not mention any specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/133063,2024-08-09T01:45:37Z,torch.compile Parsing error results in error,YES,torch.compile,2.4.0+cpu,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/133062,2024-08-09T01:04:23Z,[Export]`torch.export.export` broken for input of dtype uint16,YES,<torch.export.export>,2.4.0,"The issue description does not explicitly mention a CUDA version. Therefore, based on the provided information, there is no specific CUDA version referenced."
pytorch,https://github.com/pytorch/pytorch/issues/133057,2024-08-09T00:41:41Z,[export],YES,<torch.ops.aten.channel_shuffle>,2.4.0,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/133010,2024-08-08T16:21:49Z,torch.multiprocessing.start_processes is blocking with large input arguments,YES,torch.multiprocessing.start_processes,2.5.0a0+git21d4c48,12.2
pytorch,https://github.com/pytorch/pytorch/issues/133006,2024-08-08T14:53:43Z,Precision Differences in Using `dropout` and `linear`,YES,"torch.dropout  
torch.linear",2.3.1+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132993,2024-08-08T11:44:56Z,torch.export.unflatten of a submodule used twice creates a strange and maybe incorrect graph,YES,"torch.export.export  
torch.export.unflatten",2.4.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132989,2024-08-08T10:33:37Z,Different random seeds can cause differences in arithmetic precision,YES,"<torch.manual_seed>  
<torch.randn>  
<torch.zeros>  
<torch.mean>  
<torch.tril>  
<torch.ones>  
<torch.sum>  
<torch.allclose>  ",<Library Version> 1.13.0,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/132986,2024-08-08T09:41:51Z,`.eval()` and `.train()` don't set value of `.training` properly on `torch.compile()` module,YES,"torch.compile  
torch.nn.Linear  
torch.eval  
torch.train  ",2.4.0a0+3bcc3cddb5.nv24.07,12.5
pytorch,https://github.com/pytorch/pytorch/issues/132929,2024-08-07T20:59:23Z,Compile + torch.autograd.grad returns no gradients ,YES,torch.autograd.grad,2.4.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132928,2024-08-07T20:40:23Z,SparseSemiStructuredTensor fails on H100,YES,"torch.sparse.SparseSemiStructuredTensor  
torch.sparse.to_sparse_semi_structured  
torch.nn.functional.linear  ",2.4.0+cu121,"12.1, 12.5"
pytorch,https://github.com/pytorch/pytorch/issues/132857,2024-08-07T11:32:59Z,torch.library.opcheck generates gradients with strides of 0,YES,<torch.library.opcheck>,2.5.0.dev20240718,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132756,2024-08-06T14:03:57Z,JITerator: Torch.prod produces RuntimeError: CUDA driver error: invalid argument,YES,<torch.prod>,main,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/132700,2024-08-05T21:26:43Z,Forward Hooks in Transformer Not Working When `model.eval()` and `torch.no_grad()`,YES,"<torch.nn.TransformerEncoder>  
<torch.no_grad>  
<torch.nn.TransformerEncoderLayer>  ",2.3.1+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132695,2024-08-05T20:19:16Z,Backward on nested jagged tensor vector multiplication throws error on 0 dim (but not last dim),YES,torch.nested.as_nested_tensor,2.5.0.dev20240803,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132649,2024-08-05T13:18:55Z,nn.Parallel,YES,<torch.nn.DataParallel>,2.1.2+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/132644,2024-08-05T09:40:07Z,`torch.distributed.pipelining` hang and timeout in CPU gloo backend,YES,torch.distributed.pipelining,2.5.0.dev20240805+cpu,None
pytorch,https://github.com/pytorch/pytorch/issues/132640,2024-08-05T09:13:06Z,CUDA Invalid Memory Access caused by torch.distributed.barrier() with NCCL,YES,torch.distributed.barrier,2.5.0.dev20240718,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132634,2024-08-05T08:14:37Z,`torch.linalg.norm` and `torch.norm` give wrong result with `torch.complex32` type tensor,YES,"torch.linalg.norm  
torch.norm",2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132628,2024-08-05T05:36:24Z,set_optimizer_state_dict does not support partial state dict,YES,"<torch.distributed.checkpoint.state_dict.set_optimizer_state_dict>  
<torch.distributed.checkpoint.state_dict.get_optimizer_state_dict>",torch2.4,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/132618,2024-08-04T22:11:07Z,Reducing jagged tensor using unbind results in error in torch.compile,YES,"torch.compile  
torch.nested.as_nested_tensor  
torch.jagged  
torch.unbind  
torch.sum  ",2.5.0.dev20240803,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132614,2024-08-04T13:30:10Z,BUG: `torch.special.gammainc`: returns finite results with NaN input,YES,"torch.special.gammainc  
torch.special.gammaincc",2.3.1+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132597,2024-08-04T03:27:37Z,storing std of tensor leads to inplace backwards error,YES,torch.std,2.4.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132559,2024-08-02T23:04:39Z,How to fix tensor.numpy() not supported for torch.export with strict=False,YES,torch.export,2.5.0a0+git0b7d6b3,12.0
pytorch,https://github.com/pytorch/pytorch/issues/132471,2024-08-02T02:32:50Z,fully_shard with a gloo group segfault,YES,torch.distributed._composable.fsd fully_shard,2.5.0.dev20240801+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132418,2024-08-01T16:28:12Z,[export] Remove_effect_tokens_pass does not work well with other HOO,YES,"torch._print  
torch._higher_order_ops.wrap.wrap_with_set_grad_enabled  
torch.ops.aten.add.Tensor  
torch.fw.inserting_before  ",main,"The issue description does not mention any specific CUDA version. Therefore, I cannot provide a CUDA version based on the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/132397,2024-08-01T10:53:17Z,tensor.to(device) not copying data correctly between two GPUs,YES,torch.tensor.to,2.4.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/132395,2024-08-01T09:53:38Z,`torch.multinomial` generates incorrect distribution,YES,"<torch.multinomial>
<torch.exponential_>",torch==2.4.0,"The issue description does not mention any specific CUDA version. Therefore, the answer is:

No CUDA version mentioned."
pytorch,https://github.com/pytorch/pytorch/issues/132388,2024-08-01T07:27:13Z,`amp.custom_fwd` interacts poorly with custom `autograd.Function` with `setup_context` separated from `forward`,YES,"<torch.amp.custom_fwd>  
<torch.amp.custom_bwd>  
<torch.autograd.Function>  ",2.4.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132386,2024-08-01T07:02:52Z,torch cpu float16 range is not aligned with scipy on polygamma.,YES,torch.polygamma,torch 2.5.0a0+git27c9262,"The issue description does not mention a specific CUDA version. Therefore, the response would be:

<No CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/132366,2024-08-01T02:09:56Z,PyTorch's Distributed Checkpoint Cannot Save a Parameter of Size 1,YES,"torch.distributed.checkpoint  
torch.distributed.checkpoint.state_dict  
torch.distributed.fsdp  ",2.4.0+cu124,12.4
pytorch,https://github.com/pytorch/pytorch/issues/132360,2024-08-01T00:38:22Z,[AOTI] AOTI doesn't work well with torch.select,YES,<torch.select>,2.5.0a0+git68df24f,12.0
pytorch,https://github.com/pytorch/pytorch/issues/132240,2024-07-31T14:56:35Z,torch.nn.functional.interpolate completely broken with torch.jit.script and torch.fx,YES,"torch.nn.functional.interpolate  
torch.jit.script  
torchvision.models.feature_extraction.create_feature_extractor  
torch.fx",2.1.0+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/132222,2024-07-31T09:59:23Z,aten::bucketize.Scalar doesn't work in compile mode,YES,<torch.bucketize>,2.3.1+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132218,2024-07-31T07:08:42Z,[profiler] CUDA runtime op has wrong device time because of lazy init,YES,torch.profiler.profile,2.5.0a0+gitc549629,12.4
pytorch,https://github.com/pytorch/pytorch/issues/132148,2024-07-30T16:02:14Z,compile: `torch._subclasses.fake_tensor.FakeTensor` does not inherit from user-defined `torch.Tensor` subclass,YES,<torch._subclasses.fake_tensor.FakeTensor>,2.4.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132136,2024-07-30T12:31:15Z,`torch.nn.transformer.forward` returns incorrect value inside `torch.no_grad()` blocks.,YES,<torch.nn.Transformer>,2.3.1+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132067,2024-07-29T17:50:39Z,Pickle loads torch tensor memory leak,YES,<torch.loads>,2.0.0a0+gite9ebda2,11.7
pytorch,https://github.com/pytorch/pytorch/issues/132047,2024-07-29T14:57:56Z,Inconsistent behavior of gumbel_softmax on cpu vs. gpu,YES,<torch.gumbel_softmax>,2.5.0.dev20240708+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132045,2024-07-29T14:41:45Z,`torch.scatter`'s behavior is different on cpu and gpu on torch 2.5.0.dev20240708+cu121,YES,torch.scatter,2.5.0.dev20240708+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132041,2024-07-29T13:47:08Z,`torch.nn.functional.max_unpool2d`'s behavior is different on cpu and gpu on torch 2.5.0.dev20240708+cu121,YES,<torch.nn.functional.max_unpool2d>,2.5.0.dev20240708+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132031,2024-07-29T11:10:31Z,`torch.cross`'s behavior is different on cpu and gpu on torch 2.5.0.dev20240708+cu121,YES,<torch.cross>,2.5.0.dev20240708+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132030,2024-07-29T10:36:08Z,"torch.onnx.is_in_onnx_export(): RuntimeError: 0 INTERNAL ASSERT FAILED at ""../torch/csrc/jit/ir/alias_analysis.cpp"":608, please report a bug to PyTorch. We don't have an op for aten::mul but it isn't a special case.",YES,<torch.onnx.is_in_onnx_export>,2.5.0.dev20240708+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132029,2024-07-29T10:29:29Z,torch.QUInt4x2Storage: RuntimeError cuda_dispatch_ptr INTERNAL ASSERT FAILED on torch 2.5.0.dev20240708+cu121,YES,"torch.quantize_per_tensor  
torch.quint4x2  ",2.5.0.dev20240708+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132028,2024-07-29T10:22:26Z,torch.sspaddmm: self.is_sparse() INTERNAL ASSERT FAILED on torch 2.5.0.dev20240708+cu121,YES,torch.sspaddmm,2.5.0.dev20240708+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/132020,2024-07-29T07:32:34Z,`torch.Tensor.to` ignores `memory_format` kwarg,YES,<torch.Tensor.to>,2.4.0a0+3bcc3cddb5.nv24.07,12.5
pytorch,https://github.com/pytorch/pytorch/issues/132014,2024-07-29T03:41:52Z,[FSDP2 Related]`torch.split_with_sizes_copy` of the GPU does not update the version counter of `out` correctly.,YES,<torch.split_with_sizes_copy>,2.3.1,cu121
pytorch,https://github.com/pytorch/pytorch/pull/131977,2024-07-27T12:40:58Z,Fix bmm_sparse_cuda illegal memory access,YES,<torch.bmm_sparse_cuda>,<Library Version> 1.12.0,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/131976,2024-07-27T08:30:00Z,DistributedCheckpoint's async_save doesn't work with 0-dimensional tensors under FSDP,YES,<torch.distributed.checkpoint.async_save>,2.4.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/131975,2024-07-27T08:19:10Z,torch.compile should not recompiles when `.requires_grad=True` under `torch.no_grad()` context,YES,<torch.compile>,2.5.0.dev20240726,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/131974,2024-07-27T07:52:48Z,Missing grad_fn information while torch.compile with customized gradient function,YES,torch.compile,torch     2.3.0+cpu,<No CUDA version mentioned>
pytorch,https://github.com/pytorch/pytorch/issues/131967,2024-07-26T23:40:47Z,Layer normalization on Nested Tensor ragged dimension fails when `lengths is not None`,YES,"<torch.nested.nested_tensor_from_jagged>
<torch.nn.functional.layer_norm>",fbcode H100 7/26/24,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/131893,2024-07-26T14:05:46Z,Static shape guards are put on tensors even if they never actually get used,YES,<torch.compile>,main,"The issue description does not mention any specific CUDA version. Therefore, the answer is:

<No CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/131883,2024-07-26T09:21:09Z,Iterating dataloader fails on sliced dataset ,YES,torch.utils.data.DataLoader,2.3.1+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/131805,2024-07-25T20:24:02Z,[Dynamo] Bad accuracy detected for torch.flip on CUDA,YES,<torch.flip>,2.5.0a0+gitd526743,12.1
pytorch,https://github.com/pytorch/pytorch/issues/131753,2024-07-25T07:31:48Z,PyTorch complains ```A leaf node was passed to _will_engine_execute_node``` when calculating the gradient of a model warpped by ```FlopCounterMode```.,YES,torch._C._will_engine_execute_node,PyTorch 2.4.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/131740,2024-07-25T04:41:49Z,`._scatter(...)` gives different results in CPU vs GPU environment,YES,torch._scatter,2.3.0+cpu,11.7.64
pytorch,https://github.com/pytorch/pytorch/issues/131679,2024-07-24T17:57:25Z,"torch.export() fails on aten.to(..., copy=True) followed by mutation",YES,torch.ops.aten.to,<Library Version> 1.12.0,There is no CUDA version mentioned in the provided GitHub issue description.
pytorch,https://github.com/pytorch/pytorch/issues/131654,2024-07-24T14:49:04Z,torch.export.save doesn't support torch.float8_e4m3fn dtype,YES,torch.export.save,2.5.0.dev20240625+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/131650,2024-07-24T14:23:19Z,"Pytorch 2.4, Cuda 12.4: RuntimeError: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1720538439675/work/aten/src/ATen/cuda/CUDAContext.cpp"":49, please report a bug to PyTorch. device=, num_gpus=",YES,"torch.cuda.get_device_name  
torch.cuda.device_count  
torch.cuda.init  
torch.device  ",2.4.0,12.4
pytorch,https://github.com/pytorch/pytorch/issues/131635,2024-07-24T06:43:46Z,torch.onnx.errors.UnsupportedOperatorError,YES,"<torch.onnx.errors.UnsupportedOperatorError>  
<torch.aten.randint>",<torch 1.13.1+cu117>,cu117
pytorch,https://github.com/pytorch/pytorch/issues/131631,2024-07-24T05:34:59Z,DataLoader worker exited unexpectedly,YES,<torch.utils.data.DataLoader>,<Library Version> 1.9.0,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/131561,2024-07-23T23:37:00Z,Bug with capture_scalar_outputs item,YES,torch._dynamo.config.capture_scalar_outputs,master,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/131515,2024-07-23T20:43:37Z,torch.export.export failed with Dynamic shape on torch.ops.prims.broadcast_in_dim.default,YES,<torch.ops.prims.broadcast_in_dim.default>,2.5.0.dev20240617+cu124,12.4
pytorch,https://github.com/pytorch/pytorch/issues/131491,2024-07-23T18:07:21Z,`torch._assert_async` ignores `assert_msg`,YES,torch._assert_async,PyTorch 2.3.1,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based solely on the given text."
pytorch,https://github.com/pytorch/pytorch/issues/131349,2024-07-22T18:00:00Z,[ONNX] opset 11 aten::index_put generating invalid add ops with None args,YES,torch.index_put,2.4 (nightly build),"The description does not explicitly mention a CUDA version. However, it refers to `device=cuda:0`, which indicates that CUDA is being used, but it does not specify the version of CUDA. Therefore, based on the provided information, I cannot identify a specific CUDA version mentioned in the description."
pytorch,https://github.com/pytorch/pytorch/issues/131328,2024-07-22T15:16:06Z,torch.device context manager change doesn't show in torch.get_default_device,YES,"torch.device  
torch.get_default_device",main,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/131319,2024-07-22T12:50:25Z,"RuntimeError: false INTERNAL ASSERT FAILED at ""../aten/src/ATen/native/BatchLinearAlgebra.cpp"":498",YES,torch.triangular_solve,2.5.0.dev20240720+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/131318,2024-07-22T12:30:51Z,"RuntimeError: INTERNAL ASSERT FAILED at ""../aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp"":1306",YES,torch.sum,2.5.0.dev20240720+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/131283,2024-07-20T17:59:27Z,"`input` argument of `nan_to_num()` works with `complex` type but `nan`, `posinf` and `neginf` argument don't work with `complex` type",YES,<torch.nan_to_num>,2.3.1+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/131274,2024-07-20T04:06:54Z,The empty 0D or more D tensor with exceeding `dim`(`dim=100` or `dim=-100`) for `cummax()` works,YES,torch.cummax,2.3.1+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/131273,2024-07-20T03:56:57Z,The empty 0D or more D tensor with exceeding `dim`(`dim=100` or `dim=-100`) for `cummin()` works,YES,torch.cummin,2.3.1+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/131265,2024-07-19T23:28:50Z,[torch.compile][HF] torch.compile issue tracker for torch.compile on forward method of Meta Llama model ,YES,torch.compile,<Library Version> 2.0.0,CUDA version(s) referenced in the description is not explicitly mentioned. Please provide additional context or documentation for specific CUDA version details.
pytorch,https://github.com/pytorch/pytorch/issues/131254,2024-07-19T21:32:42Z,Masked Attention has no effect in ``TransformerEncoderLayer``,YES,torch.nn.TransformerEncoderLayer,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/131185,2024-07-19T13:08:32Z,"RuntimeError: Expected grad_output.numel() <= std::numeric_limits<int32_t>::max() to be true, but got false.",YES,torch.cuda.empty_cache,torch==2.3.1,CUDA 12.1
pytorch,https://github.com/pytorch/pytorch/issues/131150,2024-07-19T07:28:19Z,Torch.compile does not recompile when called with different options but the same backend,YES,<torch.compile>,2.4.0+cpu,None
pytorch,https://github.com/pytorch/pytorch/issues/131148,2024-07-19T07:04:44Z,"The empty 1D or more D tensor for `median()/nanmedian()` with the deepest `dim` gets errors, not getting NaNs",YES,torch.median,2.3.1+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/131066,2024-07-18T21:59:01Z,Segmentation fault torch::CudaIPCSentData::~CudaIPCSentData(),YES,torch.CudaIPCSentData,2.3.1+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/131043,2024-07-18T16:42:07Z,Using an empty tensor and `torch.int64` or `torch.bool` for `dtype` of `nanmean()` works while a non-empty tensor doesn't work,YES,torch.nanmean,2.3.1+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/131027,2024-07-18T09:07:37Z,AoTInductor cumsum causes illegal memory access for large tensors,YES,<torch.cumsum>,2.3.1+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/131009,2024-07-18T02:26:44Z,[cudagraph]  an issue related to dynamic shapes  for cudagraph backend with sd2.1,YES,"<torch.compiler.cudagraph_mark_step_begin>  
<torch.compile>",2.3.1+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130985,2024-07-17T21:04:12Z,Runtime error when running huggingface pretrained model with torch-xla,YES,<torch.cosine_similarity>,2.1.2,"The issue description does not explicitly mention a CUDA version. Therefore, based on the provided information, there is no CUDA version referenced."
pytorch,https://github.com/pytorch/pytorch/issues/130975,2024-07-17T19:24:29Z,[torch.export] Torch Export produces incorrect program when python generators are used.,YES,<torch.export.export>,2.5.0.dev20240717+cpu,CUDA used to build PyTorch: Could not collect
pytorch,https://github.com/pytorch/pytorch/issues/130932,2024-07-17T14:21:54Z,Error using torch.func transforms with DDP (broadcast_buffers=True),YES,"<torch.func.jacfwd>  
<torch.func.vmap>  
<torch.distributed.init_process_group>  
<torch.cuda.set_device>  
<torch.nn.parallel.DistributedDataParallel>  
<torch.nn.modules.module.Module.forward>  
<torch.nn.parallel.distributed._sync_buffers>  
<torch.nn.parallel.distributed._sync_module_buffers>  
<torch.nn.parallel.distributed._default_broadcast_coalesced>  
<torch.nn.parallel.distributed._distributed_broadcast_coalesced>  
<dist._broadcast_coalesced>  ",2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130931,2024-07-17T14:09:35Z,torch.export.unflatten fails on sequential models,YES,"<torch.export.unflatten>  
<torch.export.export>",2.4.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/pull/130929,2024-07-17T13:47:28Z,Fix non-torchscriptable `torch.nn.MultiheadAttention` with `bias=False`,YES,torch.nn.MultiheadAttention,1.13.0,The description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/130920,2024-07-17T09:45:14Z,Inconsistent floating point guards with numpy scalars,YES,<torch._dynamo.optimize>,2.5.0a0+git0424c83,12.2
pytorch,https://github.com/pytorch/pytorch/issues/130917,2024-07-17T08:42:38Z,Torch compile can't support Tensor.item,YES,<torch.Tensor.item>,2.3.1+cu121,121
pytorch,https://github.com/pytorch/pytorch/issues/130861,2024-07-16T21:29:56Z,torch.fx.Tracer.record_stack_traces is broken in torch 2.4.0,YES,<torch.fx.Tracer.record_stack_traces>,2.4.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130829,2024-07-16T10:34:24Z,"`unbind_copy` gives unexpected results on 1-dimensional inputs, or 0-dimensional outputs",YES,torch.unbind_copy,2.5.0a0+git6f275ae,12.2
pytorch,https://github.com/pytorch/pytorch/issues/130826,2024-07-16T09:07:00Z,models `.forward` and exported onnx are not the same,YES,"torch.nn.Module.forward  
torch.onnx.export  ",torch 2.3.0,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version to provide."
pytorch,https://github.com/pytorch/pytorch/issues/130825,2024-07-16T08:42:42Z,torch.compile fails for complex nested_tensor code,YES,"torch.autograd.backward  
torch.nn.modules.module  
torch._dynamo.eval_frame  
torch._dynamo.convert_frame  
torch._dynamo.utils  
torch._dynamo.symbolic_convert  
torch._dynamo.variables.constant  
torch._dynamo.variables.tensor  ",2.5.0.dev20240710+cu124,12.4
pytorch,https://github.com/pytorch/pytorch/issues/130792,2024-07-16T00:41:57Z,`with torch.device()` modifies the `as_tensor` intended behavior,YES,torch.as_tensor,2.3.1,"The issue description does not explicitly mention a CUDA version. Therefore, based on the provided information, I cannot identify a specific CUDA version."
pytorch,https://github.com/pytorch/pytorch/issues/130728,2024-07-15T11:07:06Z,torch.cuda.empty_cache() doesn't work correctly for the first instance of nn.Module set to cuda (and a potential method to deal with that).,YES,<torch.cuda.empty_cache>,2.3.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130726,2024-07-15T10:09:31Z,TorchDispatchMode fails on jit.trace,YES,torch.jit.trace,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130701,2024-07-14T22:16:33Z,fx.wrap() doesn't really work for things in torch/*,YES,torch.fx.wrap,"main branch, july 14, 2024","The issue description does not mention any specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/130687,2024-07-14T00:25:27Z,"Compilation Fails with torch.sparse and ""fullgraph=True""",YES,"<torch.sparse>
<torch.compile>",2.3.1.post300,12.0
pytorch,https://github.com/pytorch/pytorch/issues/130622,2024-07-12T14:58:52Z,PyTorch profiler produces a trace that is huge and unreadable by perfetto webui when `torch._dynamo` is imported within the code traced,YES,"torch.profiler  
torch._dynamo  ",2.3.1+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/130610,2024-07-12T07:06:07Z,dataloader hangs with fork and pin memory=True,YES,"torch.Generator  
torch.utils.data.dataloader  
torch.utils.data._utils.pin_memory  
torch.autograd.THPPVariable_pin_memory  
torch.autograd.THPPVariable_set_  
at::set_num_threads  ",<Library Version> 1.13.0,"The issue description does not explicitly mention a CUDA version. Therefore, it is not possible to provide a CUDA version based on the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/130609,2024-07-12T05:32:12Z,Setting `dtype` without `dtype=` to `reshape()` using a tensor works even though the doc doesn't explain it and an `int` tensor is returned instead of a `float` tensor,YES,torch.reshape,2.3.0+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/130608,2024-07-12T05:16:58Z,Setting `shape` and `dtype` without `dtype=` to `view()` works against error message but an `int` tensor is returned instead of a `float` tensor,YES,torch.Tensor.view,2.3.0+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/130538,2024-07-11T14:01:36Z,"nested_tensor LayerNorm fails with shape (*,*,j1,*) (but works with (*,j1,*,*))",YES,"torch.nn.LayerNorm  
torch.nested.nested_tensor  
torch.nn.functional.layer_norm  
torch.layer_norm  ",2.5.0.dev20240710+cu124,12.4
pytorch,https://github.com/pytorch/pytorch/issues/130515,2024-07-11T03:12:10Z,[Bug Report] inconsistent global src and local rank in dist.nn.functional.broadcast,YES,torch.distributed.nn.functional._Broadcast,1.10.0,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/130513,2024-07-11T02:27:02Z,RuntimeError Occurred In getBar1SizeOfGpu when initializing torch RPC,YES,<torch.distributed.rpc.api>,2.3.1+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130448,2024-07-10T16:40:48Z,`n` argument of `randperm()` with a 0D or more D tensor works against the doc,YES,torch.randperm,2.3.0+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/130424,2024-07-10T06:29:43Z,torch.onnx.errors.CheckerError: Unrecognized attribute: axes for operator ReduceMean,YES,torch.onnx.export,2.3.1+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/130394,2024-07-09T23:05:28Z,[Inductor] Incorrect strides for function containing `torch.empty_strided`. Causes output to crash.,YES,<torch.empty_strided>,2.5.0a0+gitaace8ff,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130254,2024-07-08T16:54:11Z,"Many types for `start`, `end` and `base` argument of `logspace()` work againt what the doc says",YES,torch.logspace,2.3.0+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/130251,2024-07-08T16:26:49Z,[funcol] numerical error when funcol.reduce-scatter a non-contiguous tensor,YES,<torch.distributed._functional_collectives.reduce_scatter_tensor>,2.2 or 2.3,"The issue description does not explicitly mention a CUDA version. It only refers to PyTorch versions 2.2 and 2.3. Therefore, there is no specific CUDA version provided in the text."
pytorch,https://github.com/pytorch/pytorch/issues/130152,2024-07-05T16:44:29Z,[export] Running torch._inductor.compile directly on an ExportedProgram fails with non-Tensor inputs,YES,torch._inductor.compile,2.5.0.dev20240624+cu124,12.4
pytorch,https://github.com/pytorch/pytorch/issues/130123,2024-07-04T17:35:35Z,randperm + torch.compile + SAC + CUDA graphs doesn't work,YES,<torch.randperm>,2.5.0.dev20240630,12.1
pytorch,https://github.com/pytorch/pytorch/issues/130108,2024-07-04T08:44:11Z,[ONNX]: Fail to export onnx when GroupNorm input shape rank=2,YES,"torch.onnx.export  
torch.nn.GroupNorm  ",PyTorch 2.0.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/130098,2024-07-04T06:43:53Z,`torch.backends.cuda.sdp_kernel(enable_flash=True)` causes graph breaks,YES,torch.backends.cuda.sdp_kernel,N/A,N/A
pytorch,https://github.com/pytorch/pytorch/issues/130078,2024-07-04T00:34:09Z,cuda graph error for torch.isin if the input tensors are too large,YES,torch.isin,2.0.0a0+gite9ebda2,11.7
pytorch,https://github.com/pytorch/pytorch/issues/130066,2024-07-03T22:38:09Z,[dynamic-shapes][dynamo] Iist index out of range for constraint,YES,"torch.compile  
torch._dynamo  
torch.fx.experimental.symbolic_shapes  
torch._guards  
torch._dynamo.guards  
torch._dynamo.convert_frame  
torch._dynamo.symbolic_convert  
torch._dynamo.output_graph  
torch._dynamo.utils  ",<Library Version> 2.0.0,"The provided issue description does not mention any specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/130051,2024-07-03T19:57:54Z,autograd.function with `setup_context` has a number of issues with `torch.compile`,YES,torch.autograd.Function,N/A,N/A
pytorch,https://github.com/pytorch/pytorch/issues/130000,2024-07-03T00:04:55Z,LayoutLMForMaskedLM generates NaN in Max-autotune mode with cuda-graph enabled,YES,<torch.compile>,1.13.0,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the provided information."
pytorch,https://github.com/pytorch/pytorch/issues/129970,2024-07-02T16:54:16Z,Setting the 0D tensor of only one element to `fill_value` of `full()`/`full_like()` works against error message,YES,torch.full_like,2.3.0+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/129966,2024-07-02T16:34:44Z,"torch.view_copy(x, dtype) diverges from eager when the destiny dtype has less bytes than the origin",YES,torch.view_copy,master,"The issue description does not explicitly mention a specific CUDA version. Therefore, it is not possible to provide the CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/129882,2024-07-01T17:31:15Z,Dynamo incorrectly interpolates symbolic variables into f-strings at runtime,YES,torch.compile,main,"The issue description does not mention a specific CUDA version. Therefore, the answer is:

<No CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/129877,2024-07-01T14:11:55Z,Eager and PT2 inconsistent on whether or not scalar tensor is allowed as input where int is expected,YES,torch.max,main,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/129820,2024-06-29T13:39:37Z,torch.lt with out parameter does not return the correct shape,YES,torch.lt,2.5.0.dev20240626+cu118,cu118
pytorch,https://github.com/pytorch/pytorch/issues/129690,2024-06-27T19:31:23Z,[torch.export] Simple model fails to export with dynamic shapes,YES,<torch.export>,2.5.0.dev20240619+cu124,12.4
pytorch,https://github.com/pytorch/pytorch/issues/129673,2024-06-27T16:50:19Z,torch compile error with `torch.Tensor.unsqueeze_`,YES,torch.Tensor.unsqueeze_,2.5.0.dev20240626+cu118,cu118
pytorch,https://github.com/pytorch/pytorch/issues/129651,2024-06-27T11:01:06Z,`torch.nn.function.one_hot` and `torch.Tensor.as_subclass` API not available under `torch.compile`,YES,"torch.nn.function.one_hot  
torch.Tensor.as_subclass",2.1.0+cu118,12.2
pytorch,https://github.com/pytorch/pytorch/issues/129649,2024-06-27T10:44:18Z,Low GPU precision,YES,torch.nn.functional.conv3d,2.0.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/129648,2024-06-27T10:39:13Z,`torch.compile` mode and eager mode are different in generating tensor API behavior,YES,torch.tensor,2.1.0+cu118,12.2
pytorch,https://github.com/pytorch/pytorch/issues/129637,2024-06-27T07:33:19Z,`torch.compile()` failing with PyTorch 2.3 for tupled inputs,YES,<torch.compile>,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129634,2024-06-27T05:52:03Z,Type Error when using from_numpy,YES,torch.from_numpy,2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129579,2024-06-26T17:13:50Z,scaled_dot_product_attention fails on Ampere arch with head_dim > 128,YES,torch.nn.functional.scaled_dot_product_attention,2.5.0a0+git92be340,"The CUDA version mentioned in the issue description is not explicitly stated. However, the issue references the use of `torch` version `2.5.0a0+git92be340`, which may imply compatibility with a specific version of CUDA. Typically, PyTorch version 2.5.0 is compatible with CUDA 11.3 and above. 

Therefore, I cannot provide a specific CUDA version directly mentioned in the text, but you may consider CUDA 11.3 or later as likely relevant to the referenced PyTorch version. 

If you need a more precise answer regarding the CUDA version used, you may need to check the official PyTorch compatibility documentation or the release notes for version 2.5.0."
pytorch,https://github.com/pytorch/pytorch/issues/129534,2024-06-25T23:53:48Z,Dynamo export: Fake tensor broadcast error,YES,<torch.onnx.dynamo_export>,2.5.0.dev20240625+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129447,2024-06-25T04:03:37Z,"Pytorch of 2.3.0 version loads bool type mask in triton, encountering error when use tl.where.",YES,torch.tl.where,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129442,2024-06-25T03:04:25Z,Volatile GPU-Util keeps 100% when using torch.distributed.isend/irecv,YES,"<torch.distributed.isend>
<torch.distributed.irecv>",2.1.2+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129366,2024-06-24T12:08:57Z,Cannot serialize NJTs using torch.save,YES,"torch.save  
torch.nested.nested_tensor  
torch.jagged",PT nightly,"The issue description does not mention a specific CUDA version. It only references ""PT nightly,"" which refers to a nightly build of PyTorch but does not provide information about the CUDA version. Therefore, there is no CUDA version indicated in the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/129327,2024-06-23T05:42:35Z,[export/dynamo] torch._check fails at compile time when the condition evaluates to False,YES,<torch._check>,<Library Version> 2.0.0,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/129319,2024-06-22T21:09:02Z,`int` type for `dims` of `tile()` without `dims=` works with a tensor against the doc,YES,torch.tile,2.3.0+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/129311,2024-06-22T16:56:09Z,The unexpected behavior of `argsort()`,YES,torch.argsort,2.3.0+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/129303,2024-06-22T08:00:26Z,Incorrect index from torch.mode,YES,torch.mode,2.3.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129298,2024-06-22T04:26:52Z,The unexpected behavior of `sort()`,YES,torch.sort,2.3.0+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/129296,2024-06-22T02:10:38Z,`argsort()`/`msort()`/`sort()` can use the 0D tensor of a `complex` type value against error message,YES,torch.sort,2.3.0+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/129272,2024-06-21T21:13:31Z,RecursionError for MaskedTensor.where,YES,"torch.masked.MaskedTensor  
torch.where  ",main and latest,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/129206,2024-06-21T05:06:44Z,Bug in calling full_tensor() when model is sharded with tensor parallel and FSDP-2,YES,"<torch.distributed._tensor.api.DTensor>  
<torch.distributed.checkpoint.state_dict.get_model_state_dict>  
<torch.distributed.checkpoint.save>  
<torch.distributed._composable.fsdp.fully_shard>  
<torch.distributed.device_mesh.init_device_mesh>  
<torch.nn.Parameter.full_tensor>  
<torch.distributed.device_mesh>  
<torch.distributed.get_rank>  ",2.4.0.dev20240412+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129200,2024-06-21T02:51:49Z,"Export model using onnx.dynamo_export has bug of ""torch._dynamo.exc.Unsupported: call_method TupleVariable() size [ConstantVariable(int)] {}""",YES,<torch.onnx.dynamo_export>,2.1.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129191,2024-06-21T00:46:18Z,RuntimeError when using torch.ops.aten._jagged_to_padded_dense_forward with large jagged tensors,YES,torch.ops.aten._jagged_to_padded_dense_forward,fbcode H100 6/20/24,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a CUDA version based on the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/129093,2024-06-19T22:12:54Z,take_along_dim or gather unstable results on cpu with stride 1,YES,"<torch.take_along_dim>  
<torch.gather>",2.2.1+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/129032,2024-06-19T03:39:07Z,torch parallel Broadcast inconsistency,YES,torch.nn.parallel.Broadcast,<torch 2.2.0+cu121>,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128961,2024-06-18T14:44:02Z,`torch.compile` fails with `fullgraph=True` when accessing `getitem` of a `Tensor` subclass,YES,torch.compile,2.5.0.dev20240617+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128947,2024-06-18T11:42:32Z,Memory consumption of conv3d grows too quickly with certain input shapes.,YES,<torch.nn.Conv3d>,<2.3.0>,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128944,2024-06-18T10:55:31Z,torch.compile graph break due to unsupported builtin filter function,YES,torch.compile,2.5.0.dev20240617+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128942,2024-06-18T09:42:57Z,torch.compile graph break with unsupported LOAD_BUILD_CLASS,YES,<torch._dynamo>,2.5.0.dev20240617+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128936,2024-06-18T07:03:37Z,Performance degradation for certain input using Conv2D,YES,torch.nn.Conv2d,2.3.0+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/128791,2024-06-16T07:44:45Z,CUDA error in torch.cdist with compute_mode=donot_use_mm_for_euclid_dist,YES,torch.cdist,2.3.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128711,2024-06-14T16:19:27Z,vmap fails to call torch.compiled function,YES,"torch.compile  
torch.func.vmap",main,"The issue description does not mention any specific CUDA version. Therefore, the answer is:

<No CUDA version>"
pytorch,https://github.com/pytorch/pytorch/issues/128703,2024-06-14T14:50:38Z,autograd with `is_grads_batched=True` fails on GroupNorm,YES,"<API Name>
torch.autograd.grad",<torch 2.1.2>,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/128695,2024-06-14T08:42:00Z,Segmentation fault (core dumped) in `torch._weight_norm_interface`,YES,<torch._weight_norm_interface>,2.3.0a0,None
pytorch,https://github.com/pytorch/pytorch/issues/128694,2024-06-14T08:39:45Z,Segmentation fault (core dumped) in `torch._fused_moving_avg_obs_fq_helper`,YES,<torch._fused_moving_avg_obs_fq_helper>,2.3.0a0,None
pytorch,https://github.com/pytorch/pytorch/issues/128693,2024-06-14T08:37:52Z,Segmentation fault (core dumped) in `torch.fused_moving_avg_obs_fake_quant`,YES,<torch.fused_moving_avg_obs_fake_quant>,2.3.0a0,None
pytorch,https://github.com/pytorch/pytorch/issues/128692,2024-06-14T08:35:54Z,Segmentation fault (core dumped) in `torch._weight_int4pack_mm`,YES,<torch._weight_int4pack_mm>,2.3.0a0,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/128691,2024-06-14T08:32:51Z,Segmentation fault (core dumped) in `torch._remove_batch_dim`,YES,<torch._remove_batch_dim>,2.3.0a0,None
pytorch,https://github.com/pytorch/pytorch/issues/128690,2024-06-14T08:24:29Z,`torch._transform_bias_rescale_qkv`:FPE,YES,<torch._transform_bias_rescale_qkv>,2.3.0a0,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/128645,2024-06-13T20:43:38Z,Improving expand w/ unbacked symints,YES,"<API Name>
torch.expand",1.12.0,"The issue description does not explicitly mention a specific CUDA version. Therefore, it is not possible to provide a CUDA version based on the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/128641,2024-06-13T20:13:26Z,Backward is not supported by `MaskedTensor`'s `torch.maximum`,YES,torch.maximum,2.3.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128610,2024-06-13T14:03:01Z,torch.onnx.export() 1.16 with pytorch 2.2.1 produces invalid model,YES,torch.onnx.export(),2.2.1,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/128600,2024-06-13T07:59:48Z,jacrev and jacfwd yield different results if one uses torch.no_grad blocks in module,YES,"torch.func.jacrev  
torch.func.jacfwd  
torch.func.functional_call  
torch.Tensor.detach  
torch.func.vjp  
torch.func.jvp",2.2.1+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128557,2024-06-12T21:41:31Z,MaskedTensor do not support _is_any_true`,YES,torch._ops.aten._is_any_true,2.3.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128556,2024-06-12T21:36:19Z,Dynamo graph breaks when passing a constant value to a nn module initialized within a function,YES,torch._dynamo,2.4.0a0+git0fbff48,12.0
pytorch,https://github.com/pytorch/pytorch/issues/128505,2024-06-12T13:02:25Z,torch.onnx.export - `repeat_interleave` produces invalid model,YES,"<torch.onnx.export>
<torch.repeat_interleave>",2.3.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128503,2024-06-12T11:27:55Z,crash@sleef_tryVXE2 () while trying to run torch.compile() BERT model,YES,torch.compile,Pytorch 2.2.0,"The issue description does not explicitly mention a CUDA version. Therefore, based on the provided information, I cannot identify a specific CUDA version."
pytorch,https://github.com/pytorch/pytorch/issues/128501,2024-06-12T10:56:18Z,torch.onnx.export with dynamic axes fails for torch.nn.InstanceNorm1d with track_running_stats=True,YES,"torch.onnx.export  
torch.nn.InstanceNorm1d",2.0.0+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/128496,2024-06-12T08:26:51Z,torch.jit.script not work for ParameterDict.items ,YES,"torch.jit.script  
torch.nn.ParameterDict.items",2.2.2+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128494,2024-06-12T08:22:26Z,torch.onnx.export fails for torch.meshgrid with indexing='xy',YES,"<torch.onnx.export>
<torch.meshgrid>",2.0.0+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/128480,2024-06-12T03:36:24Z,InternalTorchDynamoError on converting llama-2 to onnx using torch.onnx.dynamo_export,YES,<torch.onnx.dynamo_export>,torch==2.3.1+cu118,cu118
pytorch,https://github.com/pytorch/pytorch/issues/128424,2024-06-11T17:23:18Z,`torch.compile` with `reduce-overhead`: very long compile time + GPU memory continuously to grow,YES,torch.compile,2.3.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128396,2024-06-11T06:30:21Z,torch.prod cannot be used with cudagraphs,YES,"torch.prod  
torch.cuda.make_graphed_callables  
torch.cuda.graph",2.3.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128381,2024-06-11T01:55:48Z,FP16/BF16 - FP8 mixed matmul causes core dump,YES,torch._inductor.config.force_mixed_mm,2.4.0.dev20240610+cu124,12.4
pytorch,https://github.com/pytorch/pytorch/issues/128351,2024-06-10T21:50:25Z,`Tensor.is_shared` behavior is incorrect for tensors created with `torch.from_file`,YES,"torch.Tensor.is_shared  
torch.from_file",main,"The issue description provided does not mention a specific CUDA version. Therefore, I cannot provide a CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/128325,2024-06-10T11:46:29Z,Cannot export onnx model with autograd.functional.jacobian,YES,"torch.autograd.functional.jacobian  
torch.onnx.export  
torch.onnx.dynamo_export  ",torch==2.3.1,"The issue description does not explicitly mention the CUDA version. Therefore, based on the provided information, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/128324,2024-06-10T11:10:01Z,ONNX dynamic sized model export with torch.onnx.dynamo_export fails when .copy_() / roll / fftn is used,YES,"<torch.onnx.dynamo_export>
<torch.roll>
<torch.fft.fftn>",2.3.0+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/pull/128317,2024-06-10T02:30:22Z,"In gradient clipping, if DTensors are used, need to first convert them to local tensors",YES,<torch.nn.utils.clip_grad_norm_>,<Library Version> 2.0.0,The description provided does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/128294,2024-06-08T20:53:36Z,torch.cumprod will silently cast the output data type to int64,YES,torch.cumprod,torch==2.3.0,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version to provide."
pytorch,https://github.com/pytorch/pytorch/issues/128261,2024-06-07T22:24:28Z,JIT script can cause unneeded `requires_grad` inside `no_grad` blocks,YES,torch.jit.script,2.3.1+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128165,2024-06-06T21:11:52Z,`swap_tensors` fail when calling `nn.Module.to` on XLA `DDP` wrapped models.,YES,torch.utils.swap_tensors,f5328542b5365741176e71dd8a2954e0f350b9bc,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/128149,2024-06-06T17:20:53Z,torch.compile with Custom tensor subclass doesn't inline the tensor subclass methods ,YES,torch.compile,main,"The issue description does not mention a specific CUDA version. Therefore, the answer is:

<No CUDA version referenced>"
pytorch,https://github.com/pytorch/pytorch/issues/128147,2024-06-06T17:01:00Z,torch.compile Jamba: Scheduler Error in codegen for ComputedBuffer,YES,torch._inductor.scheduler,2.3.0,"The issue description does not explicitly mention a CUDA version. Therefore, it is not possible to identify a CUDA version from the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/128131,2024-06-06T10:26:28Z,Unable to record Memory consumption with `torch.cuda.memory._record_memory_history()`,YES,<torch.cuda.memory._record_memory_history>,2.1.0.dev20230405+cu118,cu118
pytorch,https://github.com/pytorch/pytorch/issues/128125,2024-06-06T07:46:25Z,optional memory leak with torch.jit.script on models with fft ,YES,"<torch.jit.script>
<torch.irfft>
<torch.rfft>",2.1.0+cu12,12.0
pytorch,https://github.com/pytorch/pytorch/issues/128073,2024-06-05T22:07:12Z,inductor error when torch.compile on distrifuser,YES,<torch.compile>,2.3.1+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128066,2024-06-05T21:45:10Z,reduce_scatter_tensor with strided inputs produces corrupted results,YES,<torch.reduce_scatter_tensor>,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/128059,2024-06-05T20:48:02Z,[Dynamo] torch.cuda.device context manager doesn't work,YES,<torch.cuda.device>,N/A,N/A
pytorch,https://github.com/pytorch/pytorch/issues/128039,2024-06-05T17:20:18Z,`torch.compile` fails with customized Triton Operator on Triton 2.2,YES,<torch.compile>,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/127978,2024-06-04T23:22:23Z,torch.addr outputs NaN when computing on inf vectors,YES,torch.addr,torch==2.3.0,"The issue description does not explicitly mention a CUDA version. Therefore, it is not possible to identify a CUDA version from the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/127882,2024-06-04T14:00:54Z,tensor.size does not accept `dim=None` argument despite documentation because of some confusion with named dimensions,YES,torch.Tensor.size,N/A,N/A
pytorch,https://github.com/pytorch/pytorch/issues/127866,2024-06-04T07:28:46Z,[torch.prod] Incorrect cpu calculation results for torch.prod,YES,torch.prod,2.3.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/127804,2024-06-03T20:25:59Z,torch.floor_divide throws floating point exception,YES,torch.floor_divide,2.3.0,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version based on the provided information."
pytorch,https://github.com/pytorch/pytorch/issues/127750,2024-06-03T07:47:42Z,aot_function cannot handle index_add,YES,<torch.ops.aten.index_add>,2.4.0.dev20240531,12.4
pytorch,https://github.com/pytorch/pytorch/issues/127736,2024-06-03T02:53:33Z,torch.nn.BatchNorm1d missing check on values of parameters eps and momentum,YES,torch.nn.BatchNorm1d,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/127734,2024-06-03T02:36:16Z,nn.ConvTranspose2d does not check for parameter combinations resulting in convolution output with dimension 0,YES,<torch.nn.ConvTranspose2d>,2.2.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/127733,2024-06-03T02:09:21Z,torch.nn.Linear does not check the value of the out_features parameter,YES,torch.nn.Linear,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/127732,2024-06-03T01:41:29Z,torch.nn.ConvTranspose2d/torch.nn.ConvTranspose1d  lacks a check for the parameter dilation in the model building process,YES,"<torch.nn.ConvTranspose2d>
<torch.nn.ConvTranspose1d>",2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/127701,2024-06-02T08:40:44Z,The type of exception thrown by torch.nn.Conv2d does not match the actual situation,YES,torch.nn.Conv2d,2.2.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/127697,2024-06-01T23:48:55Z,[dynamo] Issue with construction nn.Parameter,YES,torch.nn.Parameter,"<Library Version>
pytorch 2.0.0","The issue description does not explicitly mention a specific CUDA version. Therefore, the answer is:

NA"
pytorch,https://github.com/pytorch/pytorch/issues/127679,2024-06-01T05:14:59Z,RuntimeError: `jit.freeze` fails to find externally assigned attributes,YES,<torch.jit.freeze>,2.3.0,8.0.61
pytorch,https://github.com/pytorch/pytorch/issues/127605,2024-05-31T07:17:11Z,RuntimeError: CUDA error: an illegal memory access was encountered with specific input shape and Conv1d,YES,torch.nn.Conv1d,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/127571,2024-05-30T21:30:37Z,[export] Cannot mutate tensors with frozen storage,YES,<torch.masked_fill_>,nightly,The description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/127387,2024-05-29T08:04:29Z,"Backwards pass through Beta distribution rsample gives inf for 4 < alpha - 2**16 < 1040, beta = 3/2",YES,torch.distributions.beta.Beta.rsample,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/127375,2024-05-29T04:53:14Z,Different tensor strides can result in surprisingly large discrepancies in Conv2d outputs,YES,torch.nn.Conv2d,2.3.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/127324,2024-05-28T18:51:32Z,torch.compile reorder_for_compute_comm_overlap sink_waits pass does not work,YES,"torch.compile  
torch.distributed.init_process_group  
torch.distributed._functional_collectives.all_reduce  
torch.cuda.set_device  
torch.ops._c10d_functional.all_reduce_  
torch.ops._c10d_functional.wait_tensor  
torch._inductor.config.reorder_for_compute_comm_overlap  ",<Library Version> 2.0.0+nightly,"The issue description does not explicitly mention a specific CUDA version. Therefore, the answer is:

<N/A>"
pytorch,https://github.com/pytorch/pytorch/issues/127282,2024-05-28T10:35:27Z,torch.fx.symbolic_trace doesn't support many Callable types,YES,"torch.fx.symbolic_trace  
torch.fx.Tracer.trace",2.3.0+cpu,Could not collect
pytorch,https://github.com/pytorch/pytorch/issues/127223,2024-05-27T11:06:08Z,[BUG] torch.linalg.lstsq returning wrong result.,YES,torch.linalg.lstsq,2.0.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/127221,2024-05-27T09:54:31Z,lacking checking for ConvTranspose's parameters when running with GPUs,YES,torch.nn.ConvTranspose2d,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/127197,2024-05-26T16:12:00Z,Tensors of the same index must be on the same device and the same dtype except step tensors that can be CPU and float32 notwithstanding.,YES,torch._C._group_tensors_by_device_and_dtype,torch==2.2.2+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/127196,2024-05-26T15:11:58Z,torch.topk results differ on CPU and CUDA,YES,torch.topk,2.4.0a0+07cecf4168.nv24.05,12.4
pytorch,https://github.com/pytorch/pytorch/issues/127164,2024-05-25T07:15:05Z,randn generates different output for 4x4 tensor size sliced to match shape of direct 2x4 or 4x2 and compare output,YES,torch.randn,2.2.0a0+81ea7a4,12.3
pytorch,https://github.com/pytorch/pytorch/issues/127153,2024-05-25T02:03:54Z,"[ONNX] pad_sequence() is not exportable, with neither legacy onnx.export nor with dynamo_export",YES,"torch.nn.utils.rnn.pad_sequence  
torch.onnx.export  
torch.onnx.dynamo_export  ",Pytorch nightly 05/24,"The issue description does not explicitly mention a CUDA version. Therefore, it cannot be identified from the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/127083,2024-05-24T14:41:59Z,"Setting a `float`, `complex` or `bool` type value to `precision` argument of `set_printoptions()` has problem when creating a `float`, `complex` or `bool` type tensor",YES,torch.set_printoptions,2.3.0+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/127076,2024-05-24T10:11:51Z,[PT2E Quantization] `prepare_pt2e` produces inconsistent data types for primitive int,YES,torch.ao.quantization.quantize_pt2e.prepare_pt2e,`torch==2.3.0`,"The description does not explicitly mention a CUDA version. Therefore, there is no CUDA version referenced in the issue description provided."
pytorch,https://github.com/pytorch/pytorch/issues/126975,2024-05-23T12:28:19Z,A huge difference between the results of torch.round() on the GPU compared to its results on the CPU and other DL libraries,YES,torch.round,PyTorch 2.2.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126972,2024-05-23T11:22:05Z,ONNX Exporter Fails with Handling Complex Tensors,YES,<torch.onnx.dynamo_export>,2.3.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126963,2024-05-23T07:03:33Z,Segmentation fault (core dumped) when using pytorch Conv layers,YES,<torch.nn.Conv2d>,2.3.0+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/126949,2024-05-23T03:06:28Z,`tensor_split()` with `indices_or_sections=` doesn't work while `tensor_split()` without `indices_or_sections=` works,YES,torch.tensor_split,2.3.0+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/126947,2024-05-23T02:55:36Z,`dsplit()` with `indices_or_sections=` doesn't work while `dsplit()` without `indices_or_sections=` works,YES,torch.dsplit,2.3.0+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/126921,2024-05-22T22:18:59Z,[ONNX] view(dtype=dtype) is not supported by both onnx.export and onnx.dynamo_export,YES,"<torch.onnx.export>
<torch.onnx.dynamo_export>
<torch.Tensor.view>",nightly 20/05,"The issue description does not explicitly mention a CUDA version. However, it references ""Pytorch nighty 20/05,"" which suggests a nightly version of PyTorch, but does not provide the specific CUDA version used. Therefore, no CUDA version can be identified from the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/126882,2024-05-22T15:42:34Z,Bug: `torch.func.jacrev` fails with backend=`aot_eager`,YES,<torch.func.jacrev>,main,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/126848,2024-05-22T05:15:12Z,torch.compile generates wrong code on CPU and compiled code replaces original function,YES,torch.compile,2.4.0.dev20240521,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126777,2024-05-21T14:37:43Z,One complex number or boolean value of a 1D or more D tensor with `argmax()` and `dim=` works,YES,<torch.argmax>,2.2.1+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/126776,2024-05-21T14:30:32Z,One complex number or boolean value of a 1D or more D tensor with `argmin()` and `dim=` works,YES,<torch.argmin>,2.2.1+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/126775,2024-05-21T14:06:46Z,The significant difference between the outputs of torch.reciprocal running on cpu and gpu,YES,torch.reciprocal,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126766,2024-05-21T09:29:39Z,OOM Message is truncated,YES,torch.zeros,2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/126760,2024-05-21T07:24:37Z,"""torch.orgqr"" performs differently on cpu and gpu",YES,torch.orgqr,2.0.0+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/126759,2024-05-21T07:15:49Z,The unexpected behavior of `max()`,YES,torch.max,2.2.1+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/126758,2024-05-21T06:55:22Z,The unexpected behavior of `min()`,YES,torch.min,2.2.1+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/126748,2024-05-21T03:11:29Z,"""torch.geqrf"" performs differently on cpu and gpu",YES,torch.geqrf,2.0.0+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/126691,2024-05-20T16:11:00Z,TORCH_CHECK used within torch.compile does not throw legible errors,YES,torch.compile,1.13.0,"The issue description does not mention a specific CUDA version. Therefore, the answer is:

<No CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/126685,2024-05-20T15:26:30Z,Dynamo fails to export data-dependent `Tensor.tolist`,YES,"<torch.onnx.dynamo_export>  
<torch.unique>  
<torch.tensor_split>  
<torch._dynamo.export>  
<torch._dynamo.convert_frame._compile>  
<torch._dynamo.variables.tensor.tolist>  ",2.4.0.dev20240513+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126674,2024-05-20T08:56:01Z,"[torch.export] RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",YES,torch.export.load,<Library Version> 2.3.0,CUDA used to build PyTorch: Could not collect
pytorch,https://github.com/pytorch/pytorch/issues/126673,2024-05-20T07:45:38Z,torch.nn.AdaptiveAvgPool2d lacks checking of input dimension,YES,torch.nn.AdaptiveAvgPool2d,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126659,2024-05-20T02:09:31Z,`topk()` cannot be used with `complex` or `bool` type of a 0D(only `cuda`) or more D tensor but `topk()`  can be used with `complex` or `bool` type of a 0D(only `cpu`) tensor,YES,torch.topk,2.2.1+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/126658,2024-05-20T00:00:27Z,`kthvalue()` cannot be used with `complex` or `bool` type of a 1D or more D tensor but `kthvalue()`  can be used with `complex` or `bool` type of a 0D tensor,YES,<torch.kthvalue>,2.2.1+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/126620,2024-05-18T15:06:38Z,dataloader crashes after several epochs if the trained model contains triton-based operators,YES,"torch.utils.data.DistributedSampler  
torch.utils.data.DataLoader  
torch.cuda.amp.GradScaler  
torch.optim.AdamW  
torch.nn.CrossEntropyLoss  
torch.nn.LayerNorm  
torch.cuda.amp.autocast",2.3.0+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/126607,2024-05-18T03:44:31Z,[ONNX] export() with dynamic shapes fails where dynamo_export(dynamic_shapes=True) succeeds,YES,"<torch.export.export>
<torch.onnx.dynamo_export>",PyTorch nightly 05/15/24,"The issue description does not explicitly mention a specific CUDA version. Therefore, it cannot be identified from the provided information."
pytorch,https://github.com/pytorch/pytorch/issues/126585,2024-05-17T23:28:07Z,RuntimeError when using Adam(fused=True) with torch.compile,YES,torch.optim.Adam,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126566,2024-05-17T19:35:51Z,`torch.compiler.allow_in_graph` does not create a `call_module` op in fx.Graph in torch 2.3.0,YES,torch.compiler.allow_in_graph,2.3.0+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/126514,2024-05-17T06:35:31Z,"Weights become NaN with torch.compile optimizer capturable=True, lr=0.0, nn.Embedding",YES,"<torch.compile>
<torch.nn.Embedding>",2.4.0a0+ed76079,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126474,2024-05-16T23:11:07Z,[dynamo] Handle inplace op aliasing errors ,YES,torch.addcmul,<Library Version> 2.0.0,"The issue description does not mention any specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/126381,2024-05-16T05:13:37Z,All processes running torch.distributed.destroy_process_group() create CUDA context on device 0,YES,torch.distributed.destroy_process_group,2.3.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126268,2024-05-15T07:48:53Z,```FlopCounterMode``` returns 0 when inference mode is on during forwardpropagation.,YES,"torch.inference_mode  
torch.no_grad  
torch.utils.flop_counter.FlopCounterMode  ",2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/126242,2024-05-14T23:34:30Z,"torch.export 'inline in skipfiles: Signature.bind | bind /usr/lib/python3.10/inspect.py, skipped according trace_rules.lookup SKIP_DIRS'",YES,torch.export,torch==2.4.0.dev20240428+cpu,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/126167,2024-05-14T12:42:51Z,`aten::transpose.int` is acting errouneously,YES,<torch.aten.transpose.int>,2.3.0,None
pytorch,https://github.com/pytorch/pytorch/issues/126154,2024-05-14T09:28:59Z,Optimizer classes not `dill` picklable after using `torch.compile`,YES,"torch.compile
torch.optim.AdamW
torch._C._dynamo.eval_frame",2.3.0,"The issue description does not explicitly mention a CUDA version. Therefore, based on the provided information, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/126141,2024-05-14T03:19:09Z,Inconsistent gradients in reverse mode and forward mode AD when using `MultiheadAttention`,YES,torch.nn.MultiheadAttention,2.2.0,"The description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version referenced in the issue description."
pytorch,https://github.com/pytorch/pytorch/issues/126131,2024-05-14T01:11:10Z,Memory leak when using SequentialLR and ChainedLR schedulers,YES,"<torch.optim.lr_scheduler.ChainedScheduler>
<torch.optim.lr_scheduler.SequentialLR>",nightly: a5c93a6899c657832944cd2eeb5069449e28dbea,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the given information. If there are specific details or a version number mentioned elsewhere, please provide that context for further assistance."
pytorch,https://github.com/pytorch/pytorch/issues/126127,2024-05-14T00:13:46Z,torch.export.export() fails with dynamic shapes when more than one shape is dynamic,YES,<torch.export.export>,Pytorch nightly 05/13,"The description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/126071,2024-05-13T14:36:52Z,Dynamo Export: Some weights are exported as extra inputs,YES,"<torch.export.export>
<torch.onnx.dynamo_export>
<torch.onnx.export>",2.4.0.dev20240513+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126045,2024-05-13T04:35:02Z,A scalar with `elements` or `test_elements` argument  of `isin()` doesn't work ,YES,torch.isin,<N/A>,N/A
pytorch,https://github.com/pytorch/pytorch/issues/126041,2024-05-13T01:56:07Z,A scalar with `input` argument of `remainder()` doesn't work ,YES,<torch.remainder>,N/A,N/A
pytorch,https://github.com/pytorch/pytorch/issues/126037,2024-05-13T00:27:26Z,A scalar with `input` argument of `pow()` doesn't work,YES,<torch.pow>,<N/A>,N/A
pytorch,https://github.com/pytorch/pytorch/issues/126034,2024-05-12T23:31:14Z,A scalar with `tensor` argument of `result_type()` doesn't work,YES,<torch.result_type>,<Library Version> 1.13.0,N/A
pytorch,https://github.com/pytorch/pytorch/issues/126032,2024-05-12T16:55:57Z,all_gather with gloo backend does not work in inference mode,YES,<torch.distributed.all_gather>,pytorch 2.3.0,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the given text."
pytorch,https://github.com/pytorch/pytorch/issues/126029,2024-05-12T13:35:57Z,Inductor error: AttributeError: 'View' object has no attribute 'freeze_layout',YES,torch.freeze_layout,main,"The issue description does not explicitly mention a specific CUDA version. Therefore, based on the provided text, I cannot identify a CUDA version."
pytorch,https://github.com/pytorch/pytorch/issues/126025,2024-05-12T13:11:57Z,torch.compile does not support strided NestedTensor,YES,<torch.compile>,main,"The issue description does not mention any specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/126024,2024-05-12T11:57:46Z,Can not use torch.compile with dynamic=Ture when using multi-threads,YES,<torch.compile>,torch 2.4.0.dev20240417+cpu,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/126021,2024-05-12T10:17:01Z,Incorrect output from `torch.histc` when min and max are equal.,YES,<torch.histc>,2.2.1+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/126014,2024-05-12T01:14:43Z,The unexpected behavior of `can_cast()`,YES,<torch.can_cast>,N/A,N/A
pytorch,https://github.com/pytorch/pytorch/issues/125984,2024-05-11T01:07:45Z,`torch.export` code that includes `torch.autograd.grad`,YES,"torch.export  
torch.autograd.grad  
torch._export.aot_compile",PyTorch 2.3.0,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/125956,2024-05-10T20:26:36Z,scalar_tensor call with symbolic bool input does not work in inductor,YES,torch.aten.scalar_tensor.default,<Library Version> 2.0.0,"The issue description does not mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the given text."
pytorch,https://github.com/pytorch/pytorch/issues/125835,2024-05-09T10:51:00Z,"When using torch.onnx.export, the result of torch.scatter_add may be wrong for opset11",YES,"torch.onnx.export
torch.scatter_add",2.2.2+rocm5.7,N/A
pytorch,https://github.com/pytorch/pytorch/issues/125749,2024-05-08T09:22:35Z,'MultiHeadAttention.attention' is being compiled since it was called from 'MultiHeadAttention.forward',YES,"torch.jit.script  
torch.jit._recursive.create_script_module  
torch.jit.RecursiveScriptModule._construct  
torch.jit._recursive.create_script_module_impl  
torch.jit._recursive.create_methods_and_properties_from_stubs  
torch.jit._recursive.compile_unbound_method  
MultiHeadAttention.attention  
MultiHeadAttention.forward",<torch 2.2.2+cu121>,cu121
pytorch,https://github.com/pytorch/pytorch/issues/125741,2024-05-08T04:32:51Z,[DTensor][Tensor Parallel] transformer test numerical issue when `dtype=torch.float32`,YES,torch.float32,1.9.0,"The issue description does not mention any specific CUDA version. Therefore, there is no CUDA version to provide."
pytorch,https://github.com/pytorch/pytorch/issues/125728,2024-05-08T01:01:04Z,"torch._inductor.config.max_autotune_gemm_backends = ""TRITON"" crashes with Convolution layer",YES,"torch._inductor.config.max_autotune_gemm_backends
torch._inductor.config.max_autotune
torch.compile",torch 2.4.0.dev20240506+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/125718,2024-05-07T22:34:35Z,`torch.compile` and complex numbers,YES,torch.compile,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/125664,2024-05-07T08:21:25Z, Aborted (core dumped) in torch.fft.irfftn/hfftn/ihfftn with malloc(): corrupted top size,YES,"<torch.fft.irfftn>
<torch.fft.hfftn>
<torch.fft.ihfftn>",2.3.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/125539,2024-05-04T22:27:10Z,`triton` package that ships with torch is not compatible with `torch.ops.matmul`,YES,torch.ops.matmul,2.4.0.dev20240427+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/125534,2024-05-04T15:40:30Z,"❓Different results between normal batching and `vmap` while using lower precision (e.g., bfloat16)",YES,<torch.vmap>,2.3.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/125519,2024-05-04T00:06:18Z,Dynamic shapes: Cannot determine contiguity of split on inner dimension,YES,torch.functional.split,main,"The issue description does not mention a specific CUDA version. Therefore, the response is:

<No CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/125511,2024-05-03T22:46:04Z,Eager/FX backend sometimes uses more memory than eager,YES,"<torch.compile>  
<torch._dynamo>  
<torch.optim.RAdam>  
<torch.optim.NAdam>",Nightly: 55851e5de5afdc3b29f843c0b7de1118e92e9492,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version as it is not referenced in the text."
pytorch,https://github.com/pytorch/pytorch/issues/125493,2024-05-03T19:49:52Z,Error calling to_sparse_coo() on any subclass of Tensor,YES,torch.to_sparse_coo,<Library Version> 1.13.0,The issue description does not explicitly mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/125388,2024-05-02T10:51:24Z,torch.multinomial raises no error when sampling from zero weight and replacement=False.,YES,torch.multinomial,2.2.2+cpu,"The issue description does not explicitly mention a CUDA version. Therefore, based on the provided information, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/125375,2024-05-02T03:51:20Z,torch.onnx.dynamo_export fails to convert torchaudio.transforms.MFCC to onnx,YES,"<torch.onnx.dynamo_export>  
<torchaudio.transforms.MFCC>",2.2.2,11.8.0
pytorch,https://github.com/pytorch/pytorch/issues/125341,2024-05-01T21:46:09Z,Happier cowpath for torch.tensor_split on Tensor split sizes,YES,torch.tensor_split,main,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/125223,2024-04-30T10:29:29Z,torch.uniform_() is single-threaded on CPU,YES,torch.uniform_(),2.2.2+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/125116,2024-04-28T06:08:54Z,Does torch.nn.Linear check the weights shape before assignment?,YES,torch.nn.Linear,torch==2.0.1,"The issue description does not mention any specific CUDA version. Therefore, there are no CUDA versions referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/125064,2024-04-26T19:24:35Z,ValueError: weight_norm of 'weight' not found in ParametrizedConvTranspose1d,YES,"torch.nn.utils.weight_norm  
torch.nn.utils.parametrizations.weight_norm  
torch.nn.utils.remove_weight_norm",2.1.0+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/124937,2024-04-25T13:54:29Z,Conflict between bias=False and why_not_sparsity_fast_path in Transformer Module,YES,<torch.utils.backend_registration._privateuse1_backend_name>,2.1.2,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/124918,2024-04-25T07:45:01Z,Custom Operator Design for torch.compile: Must Output Tensors Always Be Returned?,YES,"<torch.library.define>  
<torch.library.impl>  
<torch.library.impl_abstract>  
<torch.compile>  
<torch.ops.mylib.custom_func.default>  
<torch._check>  
<torch.allclose>  
<torch.empty_like>  
<torch.from_numpy>  
<torch.randn>  ",2.3.0a0+40ec155e58.nv24.03,12.4
pytorch,https://github.com/pytorch/pytorch/issues/124877,2024-04-24T20:05:01Z,SDPA memory efficient kernel returns NaNs when the query and key are different lengths,YES,torch.backends.cuda.sdp_kernel,2.2 and 2.3,CUDA 12.1
pytorch,https://github.com/pytorch/pytorch/issues/124854,2024-04-24T16:32:09Z,[inductor] unexpected cuda:0 device usage when compiling and runing a model on cuda:1,YES,<torch.compile>,<Library Version> 2.0.0,"The issue description does not explicitly mention a specific CUDA version. Therefore, there is no CUDA version to provide based on the given text."
pytorch,https://github.com/pytorch/pytorch/issues/124838,2024-04-24T12:29:22Z,Forwardpropagation of high-dimensional tensors through the nn.Linear module becomes multiple times slower since pytorch 2.1.0,YES,<torch.nn.Linear>,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/124796,2024-04-23T22:37:21Z,Dynamo Export Support for Qwen/Qwen-7B-Chat: Mutating module attribute _ntk_alpha_cached_list during export,YES,<torch._dynamo.export>,torch==2.4.0.dev20240423+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/124793,2024-04-23T22:26:35Z,Dynamo Export Support for Google/Gemma-2B: Mutating module attribute inv_freq during export,YES,torch._dynamo.export,torch==2.4.0.dev20240423+cu121,121
pytorch,https://github.com/pytorch/pytorch/issues/124788,2024-04-23T21:57:16Z,torch.nn.checkpoint.checkpoint ignores default device in backward() call,YES,torch.utils.checkpoint.checkpoint,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/124778,2024-04-23T21:12:12Z,Dense-sparse broadcasted multiplication fails in the backward pass,YES,"torch.sparse.sum  
torch.as_strided  
torch.view  
torch.broadcast_to  ",2.2.2,11.8
pytorch,https://github.com/pytorch/pytorch/issues/124714,2024-04-23T07:47:18Z,`torch.distributed` hangs when using `torch.distributed.barrier` before any other communication primitives.,YES,torch.distributed.barrier,<2.4.0.dev20240421+cu121>,12.1
pytorch,https://github.com/pytorch/pytorch/issues/124711,2024-04-23T07:20:27Z,Output Discrepancy between PyTorch Model and Converted ONNX Model,YES,torch.onnx.export,2.2.2+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/124680,2024-04-22T23:42:10Z,Inconsistent results when training a model containing SyncBatchNorm with multiple GPUs,YES,<torch.nn.SyncBatchNorm>,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/124644,2024-04-22T19:21:25Z,torch.Tensor.remainder raises a floating point exception when divisor is -1,YES,torch.Tensor.remainder,2.2.2,"The issue description does not explicitly mention a CUDA version. Therefore, there is no CUDA version to provide from the given text."
pytorch,https://github.com/pytorch/pytorch/issues/124619,2024-04-22T15:06:36Z,torch.compile + dynamic shapes + tensor subclass graph output is broken,YES,torch.compile,<Library Version> 2.0.0,The description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/124595,2024-04-22T06:00:18Z,torch._export can't export resnet50 model,YES,<torch._export.capture_pre_autograd_graph>,2.1.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/124550,2024-04-20T17:20:08Z,`torch.func.linearize` fails when linearizing functions of gradients,YES,<torch.func.linearize>,2.4.0.dev20240409+cu121,12.1.105
pytorch,https://github.com/pytorch/pytorch/issues/124525,2024-04-19T21:36:33Z,`aot_module` + `torch.onnx.export` bug or user error?,YES,"torch.onnx.export
functorch.compile.aot_module",2.2.0+rocm5.7,N/A
pytorch,https://github.com/pytorch/pytorch/issues/124507,2024-04-19T18:14:58Z,torch.export of ResNet with dynamic height fails due to constraint violation,YES,torch.export,2.4.0.dev20240419+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/124344,2024-04-18T00:01:09Z,Inconsistent argument parsing behavior for torch.ops bindings between Tensor and FakeTensor,YES,"torch.ops.aten.dropout  
torch.ops.quantized_decomposed.quantize_per_tensor",main,"The issue description does not mention a specific CUDA version. Therefore, the response is:

<No CUDA version referenced>"
pytorch,https://github.com/pytorch/pytorch/issues/124291,2024-04-17T15:10:19Z,vmap over getitem indexing raises a RunTime error,YES,torch.vmap,2.4.0a0+git9c4fc5f,"CUDA version(s) referenced in the description: 12.1, 12.4"
pytorch,https://github.com/pytorch/pytorch/issues/124275,2024-04-17T09:31:05Z,Performance Impact of `torch.autograd.profiler.record_function` in DataLoader's `_BaseDataLoaderIter` Iteration,YES,<torch.autograd.profiler.record_function>,1.13.0+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/124256,2024-04-17T03:59:57Z,"[torch.compile] Conv1d failed: AssertionError: expected size 33==33, stride 1==50 at dim=1",YES,torch.compile,2.4.0a0+git19f5033,None
pytorch,https://github.com/pytorch/pytorch/issues/124247,2024-04-17T02:17:21Z,"[torch.compile] torch._dynamo.exc.TorchRuntimeError: Failed running call_function <method 'numpy' of 'torch._C.TensorBase' objects>(*(FakeTensor(..., size=(32, 3, 64, 64)),), **{})",YES,torch.Tensor.numpy,2.4.0a0+git19f5033,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/124216,2024-04-16T21:14:09Z,[NestedTensor] NJT with lengths set fails torch.compile(),YES,<torch.compile>,2.2.1+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/124204,2024-04-16T19:26:50Z,RPC + DDP VRAM overflow to rank 0 GPU. ,YES,<torch.distributed.autograd.backward>,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/124133,2024-04-16T01:34:07Z,ZeroRedundancyOptimizer + AdamW Fused can't load state_dict back,YES,"torch.distributed.optim.ZeroRedundancyOptimizer  
torch.optim.AdamW  ",2.2.1+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/124044,2024-04-15T07:53:08Z,torch.compile error,YES,torch.compile,<2.2.2>,12.1
pytorch,https://github.com/pytorch/pytorch/issues/124019,2024-04-14T05:12:22Z,[FSDP+TP] RuntimeError: 'weight' must be 2-D,YES,<torch.distributed.fsdp.api>,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/124006,2024-04-13T12:29:07Z,"[2.3 dynamic shapes] backend='inductor' raised: LoweringException: AssertionError: indices must be int64, byte or bool. Got [torch.float32]",YES,<torch._dynamo>,2.3.0+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/123996,2024-04-13T02:17:21Z,MSE loss triggers ZeroTensor immutable error inside `torch.func`,YES,torch.nn.functional.mse_loss,2.4.0.dev20240409+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123972,2024-04-12T21:06:00Z,Dynamo export: limited support in Torch.cond,YES,"torch.cond  
torch._dynamo.export",2.4.0.dev20240412+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/123962,2024-04-12T18:31:56Z,FSDP Doesn't Work with model.generate(),YES,"<torch.distributed.fsdp.FullyShardedDataParallel>  
<torch.distributed.fsdp.MixedPrecision>  
<torch.distributed.fsdp.StateDictType>  
<torch.distributed.fsdp.BackwardPrefetch>  
<torch.distributed.fsdp.ShardingStrategy>  
<torch.distributed.fsdp.CPUOffload>  
<torch.distributed.fsdp.wrap.transformer_auto_wrap_policy>  ",2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123911,2024-04-12T05:10:20Z,"`torch.softmax(inp, dtype=torch.float32).to(torch.float16)` is not equivalent to `torch.softmax(inp)` for fp16 input",YES,torch.softmax,2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123824,2024-04-11T07:22:28Z,Precision problem in `quantize_fx.convert_fx` and question about the `example_inputs` for `quantize_fx.prepare_fx`.,YES,"<torch.quantization.quantize_fx.convert_fx>
<torch.quantization.quantize_fx.prepare_fx>",2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123702,2024-04-10T03:45:45Z, Issue with Vulkan Device for nn.Conv2d Operation,YES,torch.nn.Conv2d,2.4.0a0+git239abb2,"The issue description does not mention any specific CUDA version. It only references the PyTorch version (2.4.0a0+git239abb2) and the operating system (Ubuntu 22.04). Therefore, there is no CUDA version to provide from this description."
pytorch,https://github.com/pytorch/pytorch/issues/123642,2024-04-09T14:43:52Z,TransformerDecoderLayer fails when on cuda and batch dimension is zero (on cpu is fine),YES,torch.nn.TransformerDecoderLayer,2.2.2+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123628,2024-04-09T05:59:06Z,`torch.onnx.export` doesn't correctly constfold constants and DequantLinear,YES,<torch.onnx.export>,2.2.2,"The CUDA version mentioned in the issue description is not explicitly stated. However, the version of PyTorch mentioned at the end is ""2.2.2"". If you are looking for the CUDA version that corresponds to this PyTorch version, you would typically need to refer to the PyTorch release notes or installation instructions for that specific version. Unfortunately, the issue description does not provide a direct mention of the CUDA version."
pytorch,https://github.com/pytorch/pytorch/issues/123560,2024-04-08T13:46:04Z,RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.cuda.HalfTensor) should be the same,YES,"<torch.cuda.amp.autocast>  
<aten.convolution_backward.default>",pytorch nightly,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/123548,2024-04-08T07:33:03Z,An error occurs when ‘max_split_size_mb ’and ‘expandable_segments ’ are enabled at the same time.,YES,<torch.zeros>,main branch,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/123533,2024-04-07T23:19:06Z,Memory allocation for Conv2d,YES,torch.nn.Conv2d,2.1.2+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123500,2024-04-06T10:05:33Z,"Cannot add 2 sparse_csc_tensor directly, raising RuntimeError",YES,torch.to_sparse_csc,2.2.2+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/123471,2024-04-05T20:04:36Z,Inductor: expected scalar type Long but found Float for int literal,YES,torch.ops._c10d_functional.all_to_all_single,2.4.0a0+gite95ffde,12.0
pytorch,https://github.com/pytorch/pytorch/issues/123447,2024-04-05T15:04:35Z,dist.barrier() hangs after calling async_save,YES,"torch.distributed.checkpoint.async_save  
torch.distributed.barrier",2.4.0a0+gite0c9764,12.0
pytorch,https://github.com/pytorch/pytorch/issues/123421,2024-04-05T05:08:55Z,nonzero with dtensor hang,YES,torch.nonzero,2.2.1-rc3,11.0
pytorch,https://github.com/pytorch/pytorch/issues/123345,2024-04-04T14:40:20Z,torch.export does not support Sparse CSR tensors,YES,torch.export.export,main,"The issue description does not explicitly mention a specific CUDA version. Therefore, the answer is:

<No CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/123245,2024-04-03T08:19:36Z,torch.overrides.get_overridable_functions list functions that are not overridable via __torch_function__,YES,"torch.overrides.get_overridable_functions  
torch.utils.generate_methods_for_privateuse1_backend",2.4.0a0+git4ba51bb,None
pytorch,https://github.com/pytorch/pytorch/issues/123242,2024-04-03T04:42:47Z,torch.compile Conv1d AssertionError,YES,"torch.compile  
torch.nn.Conv1d",2.2.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123158,2024-04-02T08:51:45Z,onnx export error with nn.InstanceNorm2d,YES,torch.nn.InstanceNorm2d,1.12.1+cu116,11.6
pytorch,https://github.com/pytorch/pytorch/issues/123147,2024-04-02T03:16:53Z,Tracker for Adagrad+SGD sparse tests failing with dynamo,YES,"<torch.optim.SGD>
<torch.optim.Adagrad>",<Library Version> 1.13.0,"The issue description does not explicitly mention a specific CUDA version. Therefore, it can be concluded that there is no CUDA version referenced in the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/123122,2024-04-01T20:32:29Z,[torch.export] Unflattened BERT missing positional argument,YES,"<torch.export.export>
<torch.export.unflatten>",2.4.0.dev20240322+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/123082,2024-04-01T08:41:05Z,Torch.cdist calculation problem,YES,torch.cdist,1.10.1+cu113,11.3
pytorch,https://github.com/pytorch/pytorch/issues/123072,2024-04-01T05:29:55Z,Saving & Loading Model Across [In-tree and out-of-tree] Devices,YES,torch.load,<Library Version> 2.1.0,"The issue description does not mention a specific CUDA version. Therefore, it is not possible to identify a CUDA version from the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/123070,2024-04-01T04:24:05Z,CUDA out of memory still exist after using FSDP,YES,torch.nn.functional._canonical_mask,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/123042,2024-03-30T23:54:19Z,ImportError: cannot import name 'triton_key' from 'triton.compiler.compiler',YES,torch._dynamo.config,2.4.0.dev20240330+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/122991,2024-03-29T21:49:09Z,[AOTI] KeyError: 'torch.complex32,YES,<torch.complex32>,2.3.0a0+git57a9a6,12.0
pytorch,https://github.com/pytorch/pytorch/issues/122886,2024-03-28T12:29:02Z,torch.normal ignores default_device ,YES,"torch.normal  
torch.from_numpy",2.1.1+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/122840,2024-03-27T23:00:02Z,a log_softmax kernel get much worse perf with padding,YES,<torch.log_softmax>,<Library Version> 1.13.0,<Cuda version>
pytorch,https://github.com/pytorch/pytorch/issues/122773,2024-03-27T09:03:46Z,torch.gather - Cannot call numel() on tensor with symbolic sizes/strides,YES,torch.gather,2.4.0.dev20240326+cpu,12.2.140
pytorch,https://github.com/pytorch/pytorch/issues/122729,2024-03-26T21:45:17Z,torch.func transforms don't yet support saved tensor hooks.,YES,torch.func.vmap,1.13.0,The description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/122711,2024-03-26T18:08:22Z,`torch.sparse.sum` does not support boolean and int when summing over dense dimensions,YES,torch.sparse.sum,1.13.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/122552,2024-03-23T10:44:04Z,Could not run 'aten::empty_strided' with arguments from the 'CUDA' backend. ,YES,<torch.aten.empty_strided>,1.13.0,"The issue description does not explicitly mention a specific CUDA version. Therefore, the response is:

<CUDA version>"
pytorch,https://github.com/pytorch/pytorch/issues/122550,2024-03-23T09:50:14Z,Adam optimizer：RuntimeError: t == DeviceType::CUDA INTERNAL ASSERT FAILED ,YES,torch.optim.adam,1.12.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/122548,2024-03-23T05:49:31Z,Unexpected device casting from CPU-pinned tensor to GPU tensor.,YES,torch.add,all,"The issue description does not specifically mention a CUDA version. Therefore, based on the provided information, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/122512,2024-03-22T18:41:05Z,Higher peak memory with torch.compile,YES,<torch.compile>,2.2.0,12.2
pytorch,https://github.com/pytorch/pytorch/issues/122478,2024-03-22T04:37:18Z,6470 segmentation fault (core dumped),YES,<torch>,2.2.1+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/122467,2024-03-22T02:19:31Z,RuntimeError: out_ptr == out_accessor[thread_count_nonzero[tid + 1]].data() INTERNAL ASSERT FAILED,YES,"torch.as_tensor  
torch.unbind  
torch.Tensor.to_sparse",<2.2.0>,12.2
pytorch,https://github.com/pytorch/pytorch/issues/122447,2024-03-21T23:03:11Z,[PT2][DTensor] crash during compiling 1D TP or SP on MLP models,YES,torch.distributed._tensor.api,2ab8b344332c5a49913dc204ac3bb8248090359c,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/122183,2024-03-19T09:45:09Z,[FX Graph Mode Quantization] Prepare model fails with Linear-BN when `affine=False`,YES,<torch.ao.quantization.quantize_fx>,2.2.1,"The issue description does not mention a specific CUDA version. Therefore, I cannot provide a CUDA version as it is not referenced in the text."
pytorch,https://github.com/pytorch/pytorch/issues/122170,2024-03-19T05:03:33Z,torch.cumsum() super slow on 2d input,YES,<torch.cumsum>,2.2.1+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/122093,2024-03-18T14:23:32Z,Bug in `torch.compile` with standard type checking tools beartype,YES,<torch.compile>,2.1.2,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/122042,2024-03-17T03:40:22Z,[Incorrect Result] torch.atan(inf) returns NaN value on dtype complex128.,YES,torch.atan,2.2.1+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/122020,2024-03-16T08:05:29Z, torch.jit.script(model) and torch.jit.trace(model) performance degradation.,YES,"torch.jit.script  
torch.jit.trace",2.0.1,"11.8, 12.1"
pytorch,https://github.com/pytorch/pytorch/issues/122019,2024-03-16T06:24:36Z,`F.max_pool3d`: Segfault due to incorrect `dilation` check in kernel,YES,<torch.nn.functional.max_pool3d>,2.2.0a0+git00412e6,None
pytorch,https://github.com/pytorch/pytorch/issues/121966,2024-03-15T10:41:30Z,torch.compile fails when used together with activation checkpointing,YES,"torch.compile  
torch.utils.checkpoint.checkpoint  
torch.cuda.amp.autocast  
torch.nn.functional.dropout  ",<2.2.1+cu121>,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121960,2024-03-15T08:47:00Z,"RuntimeError: ""max_pool2d_channels_last"" not implemented for 'Half'",YES,<torch.max_pool2d>,1.13.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/121926,2024-03-14T19:49:19Z,"Call `torch.pow(int_tensor, torch.tensor(-1))` does not raise error like NumPy.",YES,torch.pow,4305c64fea154ee1ab566e19bd7568753fc30916,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/121857,2024-03-13T22:09:35Z,AdamW(fused=True) slower than unfused AdamW,YES,<torch.optim.AdamW>,2.2.0,12.2
pytorch,https://github.com/pytorch/pytorch/issues/121842,2024-03-13T20:08:55Z,gradcheck fails when inputs are dependent on each other,YES,<torch.autograd.gradcheck>,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121793,2024-03-13T07:01:21Z,"When I use torch. compile(), the inference speed of the model becomes very slow, even dozens of times slower",YES,<torch.compile>,2.0.0+cpu,"The GitHub issue description does not explicitly mention a CUDA version. Therefore, it cannot be identified from the provided information."
pytorch,https://github.com/pytorch/pytorch/issues/121789,2024-03-13T05:20:59Z,Precision Inconsistent for `half` type compared with NumPy and TensorFlow,YES,torch.nn.functional.leaky_relu,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121761,2024-03-12T22:00:34Z,[export] Casting `ExportedProgram` module to CUDA after `export` on CPU fails to run inference,YES,<torch.export>,torch==2.3.0.dev20240312+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/121725,2024-03-12T13:26:54Z,torch.logsumexp throws RuntimeError instead of reducing all dimensions when dim=None,YES,torch.logsumexp,2.2.1+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121719,2024-03-12T10:58:11Z,index_copy change value when use_deterministic (true),YES,"torch.index_copy  
torch.index_put  ",1.13 to 2.0,"The description does not explicitly mention a CUDA version. However, it references PyTorch versions from 1.13 to 2.0. Since specific CUDA versions are often tied to specific PyTorch releases, one would typically look up the compatible CUDA versions for each PyTorch version mentioned.

PyTorch 1.13 and 2.0 may support CUDA versions like 11.6 or 11.7, but the exact versions are not specified in the issue description.

Therefore, there is no specific CUDA version directly mentioned in the description provided."
pytorch,https://github.com/pytorch/pytorch/issues/121711,2024-03-12T05:29:35Z,torch.func.grad hangs on nested differentiation,YES,<torch.func.grad>,2.1.0+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/121683,2024-03-11T23:20:03Z,linalg.ldl_factor raises an error with zero tensor,YES,torch.linalg.ldl_factor,2.1.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121631,2024-03-11T12:41:41Z,[inductor] Incorrect handle of `autocast` results in type mismatch,YES,<torch.autocast>,2.2.1,12.3.103
pytorch,https://github.com/pytorch/pytorch/issues/121588,2024-03-09T20:00:55Z,logistic sigmoid function outputs NaN indeterminate on complex input,YES,"<torch.special.expit>
<torch.nn.functional.sigmoid>",torch==2.1.0+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/121526,2024-03-08T19:33:05Z,Compiling RMSNorm Triton Kernal gives error,YES,<torch._dynamo>,<pytorch nightly version>,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/121470,2024-03-08T03:05:24Z,The output of `fuse_modules` and `fuse_modules_qat` is inconsistent,YES,"<torch.ao.quantization.fuse_modules>
<torch.ao.quantization.fuse_modules_qat>",2.1.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/121389,2024-03-07T06:58:36Z,`torch.gather` backward performance extremely low on `torch.bfloat16`,YES,torch.gather,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121386,2024-03-07T06:26:01Z,torch.compile + ring attention,YES,"torch.compile  
torch._dynamo  
torch.nn.modules.module  
torch.nn.parallel.distributed  
torch.distributed  ",<Library Version> 2.0.0,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version as requested."
pytorch,https://github.com/pytorch/pytorch/issues/121367,2024-03-07T00:15:59Z,`torch.compile` makes triton kernel slower in some cases,YES,<torch.compile>,2.3.0a0+gitfff9d98,12.3
pytorch,https://github.com/pytorch/pytorch/issues/121343,2024-03-06T21:42:29Z,Floating point exception on torch.lcm,YES,<torch.lcm>,2.2.1,cu121
pytorch,https://github.com/pytorch/pytorch/issues/121271,2024-03-05T21:29:32Z,`ReduceLROnPlateau` does not call `super().__init__()`,YES,<torch.optim.ReduceLROnPlateau>,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121256,2024-03-05T19:15:42Z,`torch.compile()` failing with a decoder-based diffusion pipeline,YES,torch.compile,2.2.1+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121238,2024-03-05T16:43:13Z,"Accuracy mismatch with torch.compile(backend=""eager"") for float16",YES,"<torch.addcdiv_>
<torch._foreach_pow>
<torch._foreach_sub_>
<torch._foreach_mul>
<torch._foreach_addcdiv_>
<torch.compile>",main,"The issue description does not explicitly mention a CUDA version. Therefore, it is not possible to provide a specific CUDA version based on the provided information."
pytorch,https://github.com/pytorch/pytorch/issues/121230,2024-03-05T14:55:00Z,INTERNAL ASSERT FAILED in torch.pow,YES,"torch.pow  
torch.quantize_per_tensor  
torch.quantile",2.2.1+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121228,2024-03-05T14:47:28Z,[torch.compile] `merge_unbind_stack` returns a tensor with WRONG shape,YES,"<torch.compile>  
<torch.unbind>  
<torch.stack>  
<merge_unbind_stack>  ",2.3.0.dev20240301+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/121222,2024-03-05T11:50:33Z,Segmentation Fault in `torch.nn.Conv1d` starting from torch 2.2.0,YES,torch.nn.Conv1d,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121220,2024-03-05T11:08:24Z,Misleading `log_abs_det_jacobian` shape for `StackTransform`?,YES,"torch.distributions.transforms.StackTransform  
torch.distributions.transforms.AffineTransform  
torch.distributions.transforms.IndependentTransform  
torch.distributions.transforms.log_abs_det_jacobian  ",2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121209,2024-03-05T07:16:59Z,torch.distributed.DistBackendError: NCCL error,YES,torch.distributed.DistBackendError,2.2.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/121208,2024-03-05T07:11:45Z,Exception: Could not run 'aten::channel_shuffle' with arguments from the 'CUDA' backend. ,YES,torch.aten::channel_shuffle,1.13.0,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/121261,2024-03-05T04:05:44Z,Convolution and norm with vmap jacobian failing,YES,"<torch.func.jacfwd>
<torch.vmap>
<torch.nn.Conv1d>
<torch.nn.LayerNorm>
<torch.nn.InstanceNorm>
<torch.nn.BatchNorm>
<torch.functional.normalise>",<Library Version> 1.13.0,"The issue description does not mention any specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/121185,2024-03-05T01:36:16Z,[torch.compile] `merge_stack_tahn_unbind` raises list index out of range error,YES,"torch.compile  
torch._dynamo.exc.BackendCompilerFailed  
merge_stack_tahn_unbind",2.3.0.dev20240301+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/121184,2024-03-05T01:33:23Z,[torch.compile] Compiled Assumption Failed for `efficient_conv_bn_eval`,YES,torch.compile,2.3.0.dev20240301+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/121135,2024-03-04T15:32:45Z,[torch.compile] `randperm_index_add_pattern` doesn't check the shape mismatch between self and source tensor ,YES,"torch.randperm  
torch.index_add  
torch.compile",2.3.0.dev20240301+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/pull/121105,2024-03-04T00:52:22Z,Include the bound check and test for slow_conv3d,YES,torch._C._nn.slow_conv3d,1.13.0,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/121098,2024-03-03T18:55:55Z,torch.onnx.dynamo_export,YES,"<torch.onnx.dynamo_export>
<torch.fft.rfft>
<torch._dynamo.disable>
<torch.fx.Interpreter>
<torch.ops.aten._native_batch_norm_legit.default>",2.2.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/121095,2024-03-03T09:38:50Z,Segmentation Fault Due to Empty Stride Argument in `torch._C._nn.slow_conv3d`,YES,torch._C._nn.slow_conv3d,2.3.0.dev20240220+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/121069,2024-03-02T08:06:45Z,'torch.compile()' causes 'Accuracy failed: uint8 tensor did not match',YES,<torch.compile>,2.1.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/120989,2024-03-01T11:20:55Z,input.is_sparse() INTERNAL ASSERT FAILED,YES,"<torch.sparse_coo_tensor>  
<torch.conj_physical>  
<input.is_sparse>  ",2.2.1+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/120905,2024-02-29T13:15:40Z,F.conv2d double precision error,YES,<torch.nn.functional.conv2d>,2.1.2+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/120814,2024-02-28T19:24:18Z,torch.compile() and optimizer UX issues,YES,torch.compile(),1.13.0,"The issue description does not mention a specific CUDA version. Therefore, the answer is:

<Cuda version>"
pytorch,https://github.com/pytorch/pytorch/issues/120791,2024-02-28T11:01:01Z,about tensor.new(storage) issue,YES,torch.tensor.new,pytorch 2.1.1+cu118,cu118
pytorch,https://github.com/pytorch/pytorch/issues/120790,2024-02-28T09:43:19Z,MultiheadAttention returns incorrect results for windowed long causal mask for long input,YES,torch.nn.MultiheadAttention,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/120597,2024-02-26T05:41:30Z,floating point exception on `torch.fmod` when x is large negative integer and y is `-1`,YES,<torch.fmod>,2.1.0+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/120591,2024-02-26T02:20:58Z,torch.distributed.DistBackendError,YES,torch.distributed,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/120585,2024-02-25T14:31:09Z,Functionalization of `as_strided` disregards its base tensor storage.,YES,torch.as_strided,2.3.0a0+gited6660e,"The issue description does not explicitly mention a CUDA version. Therefore, the answer is:

<No CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/120554,2024-02-24T15:55:00Z,`Tensor.new_tensor` fails with `jvp` (problem with duals?),YES,"torch.Tensor.new_tensor  
torch.func.jvp",2.2.1,"The CUDA version mentioned in the description is not explicitly stated. However, the issue references versions of PyTorch (2.2.1 and nightly 20240222) rather than a specific CUDA version. Typically, CUDA versions are not directly mentioned in such issue descriptions. 

Therefore, based on the provided information, there is no specific CUDA version referenced."
pytorch,https://github.com/pytorch/pytorch/issues/120485,2024-02-23T11:47:02Z,Try to make `jit.trace` working with dynamic control flow but it does not work,YES,<torch.jit.trace>,2.2.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/120462,2024-02-23T01:53:21Z,torch.compile() drops transformer/Qwen1.5-7B model output quality from good to unusable,YES,torch.compile,2.2,"The issue description does not explicitly mention a CUDA version. Therefore, based on the provided information, I cannot identify a specific CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/120381,2024-02-22T04:58:47Z,torch.compile has significantly higher peak memory for a simple checkpointed layer of matmul + layernorm + gelu,YES,"torch.compile  
torch.utils.checkpoint.checkpoint",N/A,N/A
pytorch,https://github.com/pytorch/pytorch/issues/120354,2024-02-21T23:00:53Z,Performance degradation for `aten::fill_` and `aten::add_` in backward pass,YES,"<torch.aten::fill_>
<torch.aten::add_>",2.1.2,None
pytorch,https://github.com/pytorch/pytorch/issues/120292,2024-02-21T05:42:21Z,Compilation Failure with torch.nn.functional.rrelu(training=True) in Torch Compile,YES,torch.nn.functional.rrelu,2.3.0.dev20240220+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/120194,2024-02-19T07:12:30Z,model loaded with torch._export.aot_load does not report what file is not found during inference and Cuda driver error.,YES,torch._export.aot_load,2.3.0.dev20240210,12.1
pytorch,https://github.com/pytorch/pytorch/issues/120075,2024-02-16T10:54:07Z,MultiheadAttention can not be exported into onnx with dynamic shapes,YES,<torch.nn.MultiheadAttention>,2.2.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119983,2024-02-15T11:02:45Z,Using masked_scale in a torch.compile graph without meta registration breaks the graph,YES,torch._masked_scale,2.1.1a0+git5f7d2b6,None
pytorch,https://github.com/pytorch/pytorch/issues/119980,2024-02-15T06:42:56Z,Crash on saving FSDP checkpoint of compiled model,YES,"<torch.compile>  
<torch.distributed.fsdp>  ",2.3.0.dev20240213+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119937,2024-02-14T23:10:53Z,FSDP + TP Initialization Error,YES,<torch.distributed.fsdp>,Torch nightly,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/119845,2024-02-13T22:59:26Z,Segmentation fault in dataloader worker sub-process,YES,torch.utils.data.DataLoader,"2.0.1, 2.2.0",N/A
pytorch,https://github.com/pytorch/pytorch/issues/119778,2024-02-13T15:20:07Z,torch.compile doesn't convert all input scalar types to symbolic values,YES,torch.compile,2.2.0+cu118,CUDA version: 11.8
pytorch,https://github.com/pytorch/pytorch/issues/119758,2024-02-13T06:13:24Z,Flight Recorder dump_entries() segfaults when used with coalesced operations,YES,torch.batched_isend_irecv,1.13.0,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/119698,2024-02-12T18:10:14Z,torch._inductor.triton_heuristics.cached_autotune is not thread safe / suited for multiprocessing via DDP/FSDP,YES,torch._inductor.triton_heuristics.cached_autotune,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119665,2024-02-11T23:49:17Z,module.to(device) does not work under FakeTensorMode,YES,<torch.nn.Module>,2.2.0+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/119611,2024-02-10T00:15:40Z,torch.compile slower than eager on simple MLP,YES,torch.compile,nightly,"The issue description does not explicitly mention a specific CUDA version. It only indicates that the version of PyTorch being used is ""nightly."" Therefore, no CUDA version can be identified from the provided text. 

If you need further information, please provide additional context or details."
pytorch,https://github.com/pytorch/pytorch/issues/119460,2024-02-08T16:45:11Z,Torch Dynamo fails to trace `torch.nn.RReLU()`,YES,<torch.nn.RReLU>,2.2.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119453,2024-02-08T14:43:48Z,torch.combinations allocates too much memory,YES,torch.combinations,2.2.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/119420,2024-02-07T23:09:01Z,torch.stft sometimes raises RuntimeError: cuFFT error: CUFFT_INTERNAL_ERROR on low free memory,YES,torch.stft,2.1.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119371,2024-02-07T11:50:10Z,Nan output after masked TransforrmerDecoder ,YES,"torch.nn.TransformerDecoder  
torch.nn.TransformerDecoderLayer  ",torch == 2.1.1,"The issue description does not explicitly mention a CUDA version. Therefore, based on the provided information, I cannot identify a specific CUDA version. If there are additional details or context that mention CUDA, please provide that for further assistance."
pytorch,https://github.com/pytorch/pytorch/issues/119363,2024-02-07T09:18:50Z,[ONNX] None as input to `aten::index_put` unsupported,YES,torch.onnx.dynamo_export,2.2.0+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/119360,2024-02-07T07:18:45Z,`torch.onnx.dynamo_export` provides wrong output shape for `torch.fft.irfft`,YES,"torch.onnx.dynamo_export  
torch.fft.irfft",2.1.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119348,2024-02-07T04:07:26Z,torch.compile + user defined triton kernel falsely reports mutated input,YES,<torch.compile>,<2.3.0.dev20240206+cu118>,11.8
pytorch,https://github.com/pytorch/pytorch/issues/119345,2024-02-07T02:23:21Z,all_reduce misaligned address with bfloat16,YES,torch.distributed.all_reduce,2.2.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119343,2024-02-07T02:19:39Z,Use Dtensor and ColwiseParallel api to do Tensor Parallel for llama2 model but got communication error.,YES,"<torch.distributed.tensor.parallel.ColwiseParallel>
<torch.distributed.tensor.parallel.RowwiseParallel>
<torch.distributed._tensor.DTensor>",2.3.0a0,"The issue description does not explicitly mention a specific CUDA version. Therefore, it is not possible to provide a CUDA version based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/119241,2024-02-05T23:57:06Z,Inductor doesn't move 0-D tensors to enable cudagraphs,YES,<torch.Tensor>,Nightly 6c67f3333a539e8f29515375a87612897214f8f2,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/119188,2024-02-05T12:15:59Z,scaled_dot_product_attention: different output between the reference implementation and the one from torch.nn.functional,YES,<torch.nn.functional.scaled_dot_product_attention>,2.1.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119162,2024-02-05T03:55:34Z,torch.compile changes the dtype of key matrix in attention implementation with torch.set_default_dtype,YES,"torch.compile  
torch.set_default_dtype  
torch.bfloat16  
F.scaled_dot_product_attention",2.3.0.dev20240201+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119152,2024-02-04T22:54:59Z,Dynamo fails on scalar bit shifts when `dynamic=True`,YES,torch.compile,2.3.0.dev20240204+cpu,CUDA version: Not mentioned
pytorch,https://github.com/pytorch/pytorch/issues/119148,2024-02-04T10:01:46Z,onnx torch.jit.script export problem,YES,<torch.jit.script>,torch==1.13.0,"The issue description does not mention any specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/119147,2024-02-04T09:48:52Z,Incorrect Results from cdist_backward Operator with Input Dimensions Exceeding int32_max,YES,torch.cdist_backward,torch2.1,"The issue description does not explicitly mention a CUDA version. Therefore, it is not possible to identify a specific CUDA version from the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/119131,2024-02-03T21:05:34Z,"`scaled_dot_product_attention` produces NaNs for math backend, but not memory efficient backend when used in mixed precision.",YES,torch.nn.functional.scaled_dot_product_attention,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119123,2024-02-03T09:55:56Z,"`DataLoader` generic argument is dropped in `__iter__`, making it non type-safe",YES,"torch.utils.data.DataLoader  
torch.utils.data.Dataset",2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119108,2024-02-03T00:16:35Z,[MPS] Tracking issue for ModuleInfo failures when enabling testing for `torch.float16`,YES,<torch.nn>,main,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/119076,2024-02-02T21:50:49Z,torch.bmm fails when multiplying a batch of sparse matrices by a dense tensor,YES,torch.bmm,2.1.2+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/119000,2024-02-02T13:39:38Z,Performance regression in torch.nn.Linear,YES,"<torch.nn.Linear>  
<torch.functional.linear>",2.1.2,"cu113, cu121"
pytorch,https://github.com/pytorch/pytorch/issues/118922,2024-02-01T22:15:11Z,torch.export(strict=False) fails with tensor constants,YES,torch.export.export,master,"The issue description does not explicitly mention a CUDA version. Therefore, based on the provided information, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/118883,2024-02-01T17:37:33Z,Torch Sparse Sampled AddMM Backwards Memory Usage Blowup,YES,<torch.sparse.sampled_addmm>,2.3.0.dev20240131,11.8
pytorch,https://github.com/pytorch/pytorch/issues/118856,2024-02-01T11:03:26Z,torch._dynamo.exc.TorchRuntimeError: Failed running call_function of torch.sparse_coo_tensor,YES,torch.sparse_coo_tensor,2.3.0a0+git249411e,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/118853,2024-02-01T08:47:00Z,MultiheadAttention returns different results for binary mask and float mask,YES,<torch.nn.MultiheadAttention>,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/118829,2024-02-01T01:08:40Z,The inference result is wrong when I apply the torch.quantization.fuse_modules api to resnet50 model.,YES,<torch.quantization.fuse_modules>,2.1.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/118741,2024-01-31T11:15:12Z,Inconsistent Behavior of `torch.dsplit` with torch.compile,YES,"<torch.dsplit>
<torch.special.logit>",2.3.0.dev20240130+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/118736,2024-01-31T08:34:07Z,[typing] `torch.Tensor` subclass types (eg `nn.Parameter`) broken by 2.2 release,YES,torch.Tensor,2.2.0+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/118723,2024-01-31T03:50:23Z,"Libtorch torch::load(tensor, ""tensor.pt""); error",YES,torch::load,1.13.1,CUDA 11.3/11.6
pytorch,https://github.com/pytorch/pytorch/issues/118642,2024-01-30T16:42:09Z,Different outputs of `div` and `threshold` with a specific input,YES,"torch.div  
torch.nn.functional.threshold",20240130,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot identify a specific CUDA version from the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/118628,2024-01-30T08:53:34Z,TransformerEncoderLayer fast path predicts NaN when provided attention bias,YES,torch.nn.TransformerEncoderLayer,2.1.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/118532,2024-01-29T15:36:51Z,Truth function does not handle tensor inputs ,YES,torch.compile,1.13.0,NA
pytorch,https://github.com/pytorch/pytorch/issues/118521,2024-01-29T13:54:47Z,Improve constant propagation in the dynamic shape case,YES,<torch.compile>,master,"The description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/118520,2024-01-29T13:14:09Z,[FSDP] High CPU RAM usage with FSDP sharded checkpointing,YES,torch.get_optimizer_state_dict,torch==2.3.0.dev20240117,"The issue description does not explicitly mention a CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/118492,2024-01-28T04:50:12Z,`torch.compile(dynamic=True)` compiles forever,YES,torch.compile,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/118473,2024-01-27T20:32:59Z,MultiGPU distributed training Hang,YES,"<torch.distributed.init_process_group>
<torch.distributed.barrier>
<torch.cuda.set_device>
<torch.distributed.new_group>",2.3.0.dev20240127+cu121,CUDA 12.1
pytorch,https://github.com/pytorch/pytorch/issues/118462,2024-01-27T14:40:17Z,Inconsistency in Output for `torch.nn.functional.poisson_nll_loss` in JIT Compiled Model,YES,"<torch.nn.functional.poisson_nll_loss>
<torch.nn.functional.silu>",2.3.0.dev20240126+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/118461,2024-01-27T14:18:49Z,[DTensor] DTensor fails to execute backward of index_put op.,YES,<torch.ops.aten.index_put>,nightly,"The issue description does not mention a specific CUDA version. It only mentions ""nightly"" under the ""Versions"" section. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/118400,2024-01-26T16:37:07Z,Inconsistent output of `torch.lodget` with JIT compilation,YES,"<torch.logdet>
<torch._C._special.special_bessel_j1>
<torch.jit.trace>",2.3.0.dev20240126+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/118334,2024-01-26T00:08:12Z,Using autograd.Functions defined in torch/ cause graph breaks,YES,<torch.nested._internal.ViewNestedFromBuffer>,main branch ~jan 24,"The issue description does not explicitly mention a CUDA version. Therefore, it is not possible to provide a specific CUDA version based on the provided information."
pytorch,https://github.com/pytorch/pytorch/issues/118303,2024-01-25T17:13:39Z,Inconsistent outputs of `mish` and `log10` between eagermode and torch.jit.trace,YES,"torch.nn.functional.mish  
torch.log10  ",2.0.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/118302,2024-01-25T17:04:07Z,Inconsistent outputs of `torch.adaptive_avg_pool1d` and `torch.threshold_` between eagermode and torch.jit.trace,YES,"<torch.adaptive_avg_pool1d>
<torch.threshold_>",2.0.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/118300,2024-01-25T16:56:56Z,Inconsistent outputs in `linalg_ldl_factor_ex` and `special_spheriacal_bessel_j0` between eagermode and torch.jit.trace,YES,"<torch._C._linalg.linalg_ldl_factor_ex>
<torch._C._special.special_spherical_bessel_j0>",2.0.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/118175,2024-01-24T07:03:47Z,Different behaviors in `torch.nn.functional.hinge_embedding_loss` between eagermode and torch.compile,YES,<torch.nn.functional.hinge_embedding_loss>,2.3.0.dev20240110+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/118088,2024-01-23T09:38:06Z,Segmentation fault (SIGSEGV) in c10::detail::getNonDeterministicRandom(bool),YES,"torch.c10::detail::getNonDeterministicRandom  
torch.at::CUDAGeneratorImpl::seed  
torch.at::cuda::detail::getDefaultCUDAGenerator  
torch.THCPModule_initExtension  ",2.3.0.dev20240122+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/118086,2024-01-23T09:10:57Z,Different behaviors with torch.isfinite and torch.tril between eagermode and JIT,YES,"torch.isfinite  
torch.tril",<torch 2.3.0.dev20240110+cu118>,11.8
pytorch,https://github.com/pytorch/pytorch/issues/118058,2024-01-23T02:31:23Z,torch.export doesn't handle functools.partial correctly.,YES,<torch.export.export>,2.2.0.dev20231010+cpu,Could not collect
pytorch,https://github.com/pytorch/pytorch/issues/117961,2024-01-22T04:36:52Z,ReplicatePad2d cuda not support bfloat16,YES,torch.replication_pad2d_cuda,2.1.2+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/117960,2024-01-22T03:30:46Z,"when load data  with Distributed Train, when the data is large, it will be raised errors",YES,"<torch.distributed.elastic.multiprocessing.api>  
<torch.distributed.launch>  
<torch.distributed.run>  
<torch.distributed.launcher.api>  ",pytorch 1.13.1,"The issue description does not explicitly mention a CUDA version. Therefore, based on the provided text, I cannot identify any specific CUDA version."
pytorch,https://github.com/pytorch/pytorch/issues/117957,2024-01-22T02:15:44Z,SDPA accuracy issue for CUDA (Disable temporarily),YES,"torch._dynamo.utils  
torch.testing._internal.common_distributed  
torch.testing._internal.common_utils  
torch._inductor.fx_passes.fuse_attention",<Library Version> 2.0.0,"The issue description does not explicitly mention a specific CUDA version. Therefore, no CUDA version can be identified from the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/117924,2024-01-20T10:25:08Z,[FSDP] `ignore_states` for mate params failed after materialized.,YES,torch.distributed.fsdp.FullyShardedDataParallel,torch==2.3.0a0+gitaaae2d8,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/117921,2024-01-20T09:39:21Z,[FSDP] Ignore meta device module failed with `auto_wrap_policy`,YES,torch.distributed.fsdp.auto_wrap_policy,2.3.0a0+gitaaae2d8,"The issue description does not explicitly mention a CUDA version. Therefore, based on the provided information, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/117920,2024-01-20T08:59:25Z,Could not run 'quantized::embedding_byte' with arguments from the 'QuantizedCPU' backend.,YES,<torch.quantization.embedding_byte>,1.13.1+cu116,11.6
pytorch,https://github.com/pytorch/pytorch/issues/117855,2024-01-19T15:15:26Z,"torch.combinations dies with ""numel: integer multiplication overflow""",YES,torch.combinations,2.1.2,"The issue description does not explicitly mention a CUDA version. Therefore, based on the provided information, there is no specific CUDA version identified."
pytorch,https://github.com/pytorch/pytorch/issues/117844,2024-01-19T11:00:27Z,torch.fft.rfft: Unsupported dtype BFloat16,YES,torch.fft.rfft,2.1.2+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/117736,2024-01-18T07:17:04Z,Illegal memory access for nn.Conv2d when intermediate output has size larger than 2**31,YES,torch.nn.Conv2d,2.1.2+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/117623,2024-01-17T10:06:36Z,different behaviors in cumsum between torch.compile mode and eager mode,YES,<torch.cumsum>,2.3.0.dev20240106+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/117617,2024-01-17T07:10:53Z,"torch.topk, when the input contains +nan and -nan, the result is not sure; sometimes -nan is treated greater than normal number, sometimes -nan is the least; ",YES,torch.topk,torch 1.13 & torch 2.1,The description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/117546,2024-01-16T15:37:31Z,interpolate::trilinear has different result between 1.8.0 and the newer version,YES,torch.nn.functional.interpolate,1.8.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/117540,2024-01-16T12:04:51Z,Unwanted Warning in lr_scheduler.step(),YES,<torch.optim.lr_scheduler.CosineAnnealingLR>,2.1.2+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/117486,2024-01-15T08:19:33Z,"all_to_all_single stuck when using output_split_sizes = [1, 3] and input_split_sizes = [1, 3]",YES,"```
torch.distributed._functional_collectives.all_to_all_single
torch.distributed.all_to_all_single
```",2.3.0.dev20240110+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/117348,2024-01-12T12:41:44Z,The bitwise_XX.Scalar_Tensor should be dispatched to core IR but is currently not,YES,torch.ops.aten.bitwise_and.Scalar_Tensor,2.1.0+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/117275,2024-01-11T16:57:29Z,deepcopy of `nn.Parameter` does not transfer __dict__,YES,torch.nn.Parameter,nightly,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/117259,2024-01-11T13:42:27Z,```torch.Tensor.sum```: Internal assert failed,YES,<torch.Tensor.sum>,2.3.0a0+git03969cb,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/117256,2024-01-11T13:09:20Z,```torch.Tensor.round_```: Internal assert failed,YES,torch.Tensor.round_,2.3.0a0+git03969cb,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/117250,2024-01-11T11:27:57Z,```torch.nn.functional.normalize```: Internal assert failed,YES,torch.nn.functional.normalize,2.3.0a0+git03969cb,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/117249,2024-01-11T11:16:44Z,```torch.Tensor.triangular_solve```: Internal assert failed,YES,torch.Tensor.triangular_solve,2.3.0a0+git03969cb,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/117248,2024-01-11T11:06:34Z,```torch.Tensor.matmul```: Internal assert failed,YES,<torch.Tensor.matmul>,2.3.0a0+git03969cb,None
pytorch,https://github.com/pytorch/pytorch/issues/117247,2024-01-11T10:59:53Z,```torch.nn.Linear``` class: Internal assertion failed,YES,torch.nn.Linear,2.3.0a0+git03969cb,None
pytorch,https://github.com/pytorch/pytorch/issues/117244,2024-01-11T10:33:03Z,```torch.Tensor.index_copy_```: Internal assert failed,YES,<torch.Tensor.index_copy_>,2.3.0a0+git03969cb,None
pytorch,https://github.com/pytorch/pytorch/issues/117243,2024-01-11T10:16:21Z,```torch.Tensor.cov```: Internal assertion failed,YES,"torch.Tensor.cov  
torch.Tensor.to_sparse  ",2.3.0a0+git03969cb,None
pytorch,https://github.com/pytorch/pytorch/issues/117242,2024-01-11T10:10:57Z,```torch.tensordot```: Internal assert failed,YES,torch.tensordot,2.3.0a0+git03969cb,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/117239,2024-01-11T08:45:22Z,```torch.nn.Upsample``` class: Internal assert failed,YES,<torch.nn.Upsample>,<2.3.0a0+git03969cb>,None
pytorch,https://github.com/pytorch/pytorch/issues/117236,2024-01-11T08:24:29Z,```torch.linalg.lstsq```: Internal assert failed,YES,torch.linalg.lstsq,2.3.0a0+git03969cb,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/117233,2024-01-11T08:18:55Z,Submit a bug when trying to assign arg 'out=' in api torch._foreach_add_,YES,<torch._foreach_add_>,2.2.0.dev20231208+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/117232,2024-01-11T08:07:10Z,```torch.Tensor.renorm```: Internal assert failed,YES,<torch.Tensor.renorm>,2.3.0a0+git03969cb,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/117229,2024-01-11T07:21:25Z,```torch.nn.functional.pad```: Internal assert failed,YES,"torch.nn.functional.pad  
torch.quantize_per_tensor  ",2.3.0a0+git03969cb,None
pytorch,https://github.com/pytorch/pytorch/issues/117209,2024-01-11T03:23:36Z,"torch.onnx.export the torch,nn.TransformerEncoderLayer, the dynamic_axes work not right",YES,"torch.onnx.export  
torch.nn.TransformerEncoderLayer  
torch.nn.TransformerEncoder  ",2.1.1+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/117130,2024-01-10T18:35:26Z,`torch.cuda.memory_summary()` can give `KeyError`,YES,"<torch.cuda.memory_summary>
<torch.cuda.memory_stats>",2.1.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116992,2024-01-08T21:14:57Z,torch.fake_quantize_per_tensor_affine with inf input provides wrong output value,YES,torch.fake_quantize_per_tensor_affine,2.1.1+cpu,None
pytorch,https://github.com/pytorch/pytorch/issues/116858,2024-01-05T12:32:30Z,torch.compile(...) does not pass models containing RNN layers to custom compilers,YES,torch.compile,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116835,2024-01-05T04:47:35Z,torch.compile fullgraph=True is failing for GPTJ model for toy_backend,YES,torch.compile,2.1.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116766,2024-01-04T07:12:44Z,[Dynamo][DeepSpeed] torch._dynamo.exc.InternalTorchDynamoError: NestedUserFunctionVariable() has no type,YES,torch._dynamo,2.1.0a0+git44d1ed8,None
pytorch,https://github.com/pytorch/pytorch/issues/116707,2024-01-03T18:10:12Z,AttributeError: `Adam` object has no attribute `param_groups` with Python 3.11,YES,<torch.load>,2.0.1,Is CUDA available: False
pytorch,https://github.com/pytorch/pytorch/issues/116697,2024-01-03T12:37:30Z,Tracing GPU usage of `torch.vmap` vectorized operation using `torch.profiler` cause CUDA OOM,YES,"<torch.vmap>
<torch.profiler.profile>
<torch.cuda.synchronize>
<torch.cuda.empty_cache>",2.1.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116633,2024-01-02T19:58:34Z,Sparse tensor element-wise multiplication bug when broadcast needed,YES,torch.sparse_tensor.expand,torch==2.0.1,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/116612,2024-01-02T15:52:40Z,Loaded torch.jit traced module treats dtypes incorrectly in autograd,YES,"torch.jit
torch.autograd.grad
torch.matmul",2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/116555,2023-12-30T02:58:35Z,The performance of `torch.index_select` and regular indexing differs dramatically based on the size of the tensor it is indexing,YES,<torch.index_select>,2.0.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/116533,2023-12-29T10:08:30Z,`backward` implicit conversion from tuple to `torch.Tensor` results in an indexing error message,YES,<torch.autograd.Function.backward>,2.1.2,"The issue description does not explicitly mention a CUDA version. It mainly discusses the error related to implicit conversion in PyTorch. Therefore, no CUDA version can be identified from the provided information."
pytorch,https://github.com/pytorch/pytorch/issues/116393,2023-12-25T12:50:13Z,Error: IndexError: map::at When using torch.distributed.all_reduce(tensor),YES,<torch.distributed.all_reduce>,torch==2.1.2,"The issue description does not explicitly mention a CUDA version. Therefore, no specific CUDA version can be identified from the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/116351,2023-12-23T07:35:10Z,`torch.quantize_per_channel`: FPE ,YES,<torch.quantize_per_channel>,2.3.0a0+git1ee7950,None
pytorch,https://github.com/pytorch/pytorch/issues/116350,2023-12-23T03:26:49Z,RuntimeError: derivative for aten::_scaled_dot_product_flash_attention_backward is not implemented,YES,<torch._scaled_dot_product_flash_attention_backward>,<Library Version> 2.0.0,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/116336,2023-12-22T21:26:04Z,"ONNX export: Unsupported FX nodes: {'call_function': ['aten._sample_dirichlet.default', ",YES,"torch.export.export  
torch.onnx.dynamo_export  
torch.empty  
torch.distributions.transformed_distribution.rsample  
torch._dynamo.export  
torch._dynamo.utils.wrap_fake_exception  
torch._dynamo.utils.get_fake_value  
torch._dynamo.exc.UserError  
torch._dynamo.utils.time_wrapper  
torch._dynamo.symbolic_convert.run  
torch._dynamo.symbolic_convert.step  
torch._dynamo.symbolic_convert.wrapper  
torch._dynamo.symbolic_convert.inline_user_function_return  
torch._dynamo.symbolic_convert.inline_call  
torch._dynamo.symbolic_convert.inline_call_  ",2.1.2+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116320,2023-12-22T09:16:23Z,RuntimeError: CUDA error: an illegal memory access was encountered using vmap and model ensembling call for cuda system,YES,<torch.autograd.Function>,pytorch2.0,CUDA 11.7
pytorch,https://github.com/pytorch/pytorch/issues/116306,2023-12-21T23:43:27Z,ONNX export error when exporting Vision Transformer model from PyTorch to ONNX format,YES,torch.onnx,2.3.0.dev20231221+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/116300,2023-12-21T22:14:33Z,DDPOptimizer lazy compile causes shape mismatch error,YES,torch._dynamo.config.optimize_ddp_lazy_compile,PyTorch nightly,"The issue description does not mention a specific CUDA version. Therefore, the answer is:

<no CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/116253,2023-12-21T08:55:55Z,`torch.Tensor.flipud`: Heap-buffer-overflow,YES,<torch.Tensor.flipud>,2.3.0a0+git1ee7950,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/116252,2023-12-21T08:47:24Z,`torch.Tensor.fliplr`: Heap-buffer-overflow,YES,<torch.Tensor.fliplr>,2.3.0a0+git1ee7950,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/116251,2023-12-21T08:41:51Z,`torch.Tensor.flip`: Heap-buffer-overflow,YES,<torch.Tensor.flip>,2.3.0a0+git1ee7950,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/116250,2023-12-21T08:09:43Z,`torch.Tensor.rot90`:  Heap-buffer-overflow,YES,<torch.Tensor.rot90>,2.3.0a0+git1ee7950,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/116240,2023-12-21T03:06:51Z,[inductor][cpu] RuntimeError: quantized_resize_cpu_ does not have a deterministic implementation,YES,<torch.ops.quantized.linear>,2.3.0a0+git5da0a5d,"The issue description does not mention a specific CUDA version. Therefore, I cannot provide a CUDA version as it is not referenced in the text."
pytorch,https://github.com/pytorch/pytorch/issues/116202,2023-12-20T19:59:44Z,"Dynamo'd `test_mixed_device_dtype` needs higher tolerances for SGD, RAdam and RMSProp ",YES,"<torch.optim.SGD>
<torch.optim.RMSprop>",main,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/116183,2023-12-20T11:36:14Z,Pipeline parallel doesn't seem to work with mixed precision,YES,<torch.distributed.pipeline.sync.Pipe>,2.1.1+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116182,2023-12-20T10:30:00Z,[ONNX]  GFPGANv1.pth to onnx conversion error,YES,torch.onnx.export,<torch 2.1.0+cu118>,11.8
pytorch,https://github.com/pytorch/pytorch/issues/116177,2023-12-20T07:23:50Z,OOM error for collection communication primitive provided by torch.distributed,YES,<torch.distributed.all_reduce>,1.11.0+cu113,11.3
pytorch,https://github.com/pytorch/pytorch/issues/116175,2023-12-20T07:04:00Z,Performance for `TransformerEncoderLayer` and `TransformerDecoderLayer` drops severely if pytorch version changed from `pt1.10` to `pt2.0`,YES,"torch.nn.TransformerEncoderLayer  
torch.nn.TransformerDecoderLayer",1.10.0a0+git36449ea,11.8
pytorch,https://github.com/pytorch/pytorch/issues/116120,2023-12-19T19:48:09Z,bfloat16 on 'grid_sampler_3d_cuda' not implemented,YES,<torch.grid_sampler>,2.1.2,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116117,2023-12-19T18:56:45Z,WeightedRandomSampler doesnt keep expected ratio betwen samples,YES,torch.utils.data.WeightedRandomSampler,2.1.1+cpu,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/116077,2023-12-19T02:32:23Z,TransformerEncoderLayer and TransformerDecoderLayer do not work,YES,"torch.TransformerEncoderLayer  
torch.TransformerDecoderLayer",2.1.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116055,2023-12-18T20:11:55Z,Operating on CUDA tensors on multiprocessing queue hangs,YES,torch.mean,2.1.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/116018,2023-12-18T12:51:01Z,Inconsistent behavior of torch.einsum on float32 and complex128,YES,<torch.einsum>,2.1.2+cpu,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/116016,2023-12-18T11:46:55Z,[distibuted] torch.distributed.new_group failed with Segmentation fault,YES,<torch.distributed.new_group>,23.11-py3,cuda11
pytorch,https://github.com/pytorch/pytorch/issues/115914,2023-12-15T07:13:16Z,torch.jit.script fails when parsing torch.SymInt,YES,"torch.jit.script
torch.SymInt",2.1.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/115861,2023-12-14T19:16:41Z,gather_object does not work with ThreadPoolExecutor,YES,<torch.distributed.gather_object>,<Library Version> 1.13.0,"The issue description does not explicitly mention a specific CUDA version. Therefore, there is no CUDA version to provide."
pytorch,https://github.com/pytorch/pytorch/issues/115821,2023-12-14T13:02:12Z,[BUG] missing automatic dtype promotion for out-of-place `torch.scatter_*()` functions,YES,"<torch.scatter_add>
<torch.scatter_reduce>
<torch.Tensor.scatter_add_>",main branch,"The issue description does not mention any specific CUDA version. Therefore, the answer is:

<No CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/115790,2023-12-13T23:56:21Z,ExportedProgram.module(flat=False) doesn't work with torch.cond,YES,<torch.export.export>,main,"The issue description does not mention any specific CUDA version. Therefore, there is no CUDA version to provide."
pytorch,https://github.com/pytorch/pytorch/issues/115785,2023-12-13T23:37:58Z,arctan2 fp16 error when optimising,YES,<torch.arctan2>,2.1.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/115733,2023-12-13T09:31:04Z,RuntimeError: thread_predicates_.find(tv_inp) != thread_predicates_.end(),YES,<torch._C._jit_pass_lower_thread_predicate>,2.0.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/115640,2023-12-12T16:48:53Z,"Quantisation results in a ""Illegal instruction (core dumped)""",YES,<torch.quantization.quantize_dynamic>,<torch 2.1.1>,12.1
pytorch,https://github.com/pytorch/pytorch/issues/115624,2023-12-12T11:36:21Z,`amax` fails on `masked_tensor` with multiple negative dims,YES,<torch.amax>,2.1.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/115610,2023-12-12T02:50:14Z,[Bug] Big difference between the output of Conv float precision and double precision,YES,<torch.nn.functional.conv2d>,1.12+cu113,cu113
pytorch,https://github.com/pytorch/pytorch/issues/115588,2023-12-11T21:03:12Z,`torch.nn.KLDivLoss` fails `test_grad` and `test_gradgrad` ,YES,torch.nn.KLDivLoss,main,"The issue description does not explicitly mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description provided."
pytorch,https://github.com/pytorch/pytorch/issues/115585,2023-12-11T20:51:56Z,`torch.nn.CTCLoss` doesn't seem to properly support noncontiguous inputs for 1d `target` with `target_dtype=torch.long`,YES,torch.nn.CTCLoss,main,"The issue description does not explicitly mention a specific CUDA version. Therefore, no CUDA version can be identified from the provided text.

<Cuda version>"
pytorch,https://github.com/pytorch/pytorch/issues/115534,2023-12-11T09:54:39Z,Torch.export.export fails with SAM and returns a non-informative Error message ,YES,torch.export.export,2.1.1+cpu,None
pytorch,https://github.com/pytorch/pytorch/issues/115516,2023-12-11T01:50:38Z,[FakeTensor] FakeTensorMode dispatch won't change generator state,YES,"<torch.Generator>  
<torch._C.Generator>  
<torch.rand>  ",2.2.0.dev20231210+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/115492,2023-12-09T23:43:36Z,[dynamic shapes] torch.fx.experimental.symbolic_shapes.ConstraintViolationError from demucs with dynamic shapes,YES,torch.fx.experimental.symbolic_shapes,<Library Version> 2.1.0,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version as it is not referenced in the text."
pytorch,https://github.com/pytorch/pytorch/issues/115488,2023-12-09T22:13:37Z,torch.export.export is overriding source_fn inside higher order op submodules,YES,torch.export.export,main,"The issue description does not mention any specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/115484,2023-12-09T19:55:44Z,torch.compile() breaks when using DeepSpeed ZeRO Level 3 sharding,YES,torch.compile(),2.1.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/115472,2023-12-09T01:14:47Z,Potential issue with custom transformer masks when using fast path and batch_first=True,YES,torch.nn.Transformer,2.1,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/115422,2023-12-08T15:31:20Z,string representation method for empty masked tensors fails,YES,torch.masked.masked_tensor,2.1.0+cu118,cu118
pytorch,https://github.com/pytorch/pytorch/issues/115416,2023-12-08T12:22:02Z,torch.nn.functional.embedding segmentation fault on large negative ids,YES,torch.nn.functional.embedding,2.1.0 and 2.1.1,11.3.1
pytorch,https://github.com/pytorch/pytorch/issues/115388,2023-12-07T23:01:16Z,`torch.distributed.destroy_process_group()` hangs after CUDA graph capture of NCCL operations,YES,<torch.distributed.destroy_process_group>,2.1.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/115362,2023-12-07T18:09:09Z,torch._fused_sdp_choice for fake and real tensors returns different results,YES,torch._fused_sdp_choice,2.2.0a0+git4fa1ff8,The description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/115347,2023-12-07T14:52:59Z,[Functorch] vmap over index_select expands the output,YES,"torch.vmap  
torch.index_select",PT '2.2.0.dev20231128',The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/115342,2023-12-07T11:59:20Z,torch.func.grad not working with list comprehension over dictionary,YES,torch.func.grad,2.1.0+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/115307,2023-12-06T23:13:59Z,torch._inductor.config.save_args=True and dynamic shapes do not work together,YES,"torch._inductor.config  
torch.compile  
torch._dynamo.exc.BackendCompilerFailed  
torch._inductor.compile_fx  
torch._functorch.aot_autograd  
torch._dynamo.output_graph  
torch._dynamo.symbolic_convert  
torch._dynamo.eval_frame  ",2.2.0a0+gitf128616,11.8
pytorch,https://github.com/pytorch/pytorch/issues/115261,2023-12-06T13:41:13Z,[Inductor][cpu][miscompile] Outputs of torch.matmul abnormally change with extra outputs,YES,"torch.matmul  
torch.add  
torch.cat  
torch.max  ",2.2.0.dev20231205+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/115249,2023-12-06T07:10:56Z,[AOT Autograd] operator decomposition does not respect training=False flag for torch.nn.functional.dropout,YES,<torch.nn.functional.dropout>,2.2.0.dev20231127,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/115064,2023-12-04T08:48:48Z,torch.nn.PairwiseDistance，The results vary widely from version to version,YES,torch.nn.PairwiseDistance,1.7.0 and 1.11.0,"The issue description does not explicitly mention any CUDA version. Therefore, there are no CUDA versions referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/114943,2023-12-01T10:24:55Z,torch.einsum may choose a strategy for which there is not enough memory,YES,torch.einsum,2.1.0+cu121,"CUDA Version: 12.3, 12.2, 11.8"
pytorch,https://github.com/pytorch/pytorch/issues/114928,2023-12-01T03:45:39Z,`torch.nn.utils.rnn.pack_padded_sequence` heap-buffer-overflow,YES,"<torch.nn.utils.rnn.pack_padded_sequence>  
<torch.quantize_per_tensor>  ",2.2.0a0+git140c54e,None
pytorch,https://github.com/pytorch/pytorch/issues/114672,2023-11-28T07:52:42Z,torch.fft.ifft crashes for empty input,YES,torch.fft.ifft,2.1.0,11.3.1
pytorch,https://github.com/pytorch/pytorch/issues/114571,2023-11-27T02:29:03Z,inconsistency between nan cast to int32 on CPU and GPU,YES,<torch.tensor>,1.12.0,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based solely on the information given in the issue description."
pytorch,https://github.com/pytorch/pytorch/issues/114569,2023-11-27T02:04:29Z,inconsistency on torch.clamp,YES,torch.clamp,1.8.0,"The issue description does not explicitly mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/114534,2023-11-25T13:47:37Z,`PYTORCH_NO_CUDA_MEMORY_CACHING=1` with `torch.multiprocessing` shared tensors seems to perform use-after-free,YES,"torch.multiprocessing.spawn  
torch.multiprocessing.reductions.reduce_tensor  
torch.storage._share_cuda_  
torch.storage._untyped_storage._share_cuda_  ",main,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/114529,2023-11-25T05:43:02Z,Bug in element-wise multiplication of `torch.sparse_csr_tensor`s on GPU - 0's in result considered significant - PyTorch 2.1.1,YES,torch.sparse_csr_tensor,2.1.1,12.1
pytorch,https://github.com/pytorch/pytorch/issues/114483,2023-11-23T20:55:35Z,dynamo supports Tensor.tolist but not Tensor.item,YES,"<torch.Tensor.tolist>  
<torch.Tensor.item>",2.2.0.dev20231122+cpu,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the provided GitHub issue description."
pytorch,https://github.com/pytorch/pytorch/issues/114406,2023-11-22T21:36:22Z,torch.onnx.dynamo_export functionalization does not support aten.add_.Tensor,YES,torch.onnx.dynamo_export,main,"The issue description does not mention a specific CUDA version. Therefore, the answer is:

<No CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/114344,2023-11-22T08:19:37Z,[AOTAutograd] Incorrect CSE aliasing while `requires_grad` meta differs,YES,<torch.compile>,main,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/114216,2023-11-21T08:11:06Z,torch.nn.functional.max_pool2d outputs inf,YES,torch.nn.functional.max_pool2d,torch2.1.0+cu118,cu118
pytorch,https://github.com/pytorch/pytorch/issues/114189,2023-11-20T23:58:25Z,Custom backend not called for compiling backward graph,YES,"torch.compile  
torch._dynamo.symbolic_convert  
torch.fx.experimental.symbolic_shapes  
torch._functorch.aot_autograd  
torch._inductor.compile_fx  
torch.ops.aten.permute.default  
torch.ops.aten.addmm.default  
torch.ops.aten.mm.default  
torch.ops.aten.sum.dim_IntList  
torch.ops.aten.view.default  
torch.ops.aten.reshape.default  ",PyTorch Nightly,"The issue description does not explicitly mention a specific CUDA version. Therefore, based on the information provided, I cannot identify a CUDA version from the provided text. If you have additional context or a different section of the text that might contain version information, please share!"
pytorch,https://github.com/pytorch/pytorch/issues/114112,2023-11-20T10:15:43Z,[JIT] - torch.script - 'Optional[Tensor]' object has no attribute or method 'size',YES,torch.jit.script,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/114105,2023-11-20T07:52:40Z,Unexpected `None` value for stream with dynamo,YES,<torch.cuda.current_stream>,2.2.0a0+git956114a,CUDA version: 12.3
pytorch,https://github.com/pytorch/pytorch/issues/114093,2023-11-20T02:35:26Z,Found nn.LazyBatchNorm1d(0) has inconsistency bug between GPU and CPU testing,YES,torch.nn.LazyBatchNorm1d,1.13.0,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the given information."
pytorch,https://github.com/pytorch/pytorch/issues/114090,2023-11-20T02:26:16Z,Nested tensors fail on Conv2D,YES,"torch.nested.nested_tensor  
torch.nn.Conv2d",2.0.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/114085,2023-11-20T01:05:07Z,inconsistency in torch.Tensor.scatter on GPU and CPU,YES,torch.Tensor.scatter,<Library Version> 1.10.0,The description does not explicitly mention a CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/114080,2023-11-20T00:23:04Z, torch.matrix_exp(x) get inf and nan,YES,torch.matrix_exp,<Library Version> 1.12.0,"The issue description does not explicitly mention any CUDA version. Therefore, I cannot provide a specific CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/114044,2023-11-19T02:48:24Z,Nan on torch.corrcoef(x.t()),YES,torch.corrcoef,<Library Version> 1.13.0,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the provided information."
pytorch,https://github.com/pytorch/pytorch/issues/114035,2023-11-19T00:13:39Z,Load model from jit script format. Repeating inference several times can lead to errors.,YES,"<torch.jit.trace>
<torch.jit.save>
<torch.jit.load>
<torch._C._jit_override_can_fuse_on_cpu>
<torch._C._jit_override_can_fuse_on_gpu>
<torch._C._jit_set_texpr_fuser_enabled>
<torch._C._jit_set_nvfuser_enabled>",2.1.1+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/113945,2023-11-17T12:25:52Z,torch.jit.trace has incorrect execution for += operation during compilation,YES,"torch.jit.trace  
torch.compile  
torch.onnx.export",2.2.0.dev20231117+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/113942,2023-11-17T10:26:02Z,Tensorboard list of tensors as input,YES,torch.jit.trace,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/113840,2023-11-16T04:06:21Z,torch.einsum is stuck in mp.Process,YES,torch.einsum,2.1,"There is no CUDA version mentioned in the issue description. The version referenced is ""2.1 linux cpu,"" which indicates a version of PyTorch but does not specify a CUDA version."
pytorch,https://github.com/pytorch/pytorch/issues/113809,2023-11-15T22:07:42Z,"Error during DDP, torch.compile, and cudagraph_trees",YES,"torch.compile  
torch._dynamo  
torch._inductor.cudagraph_trees  ",<Library Version> 2.1.0,"The issue description does not explicitly mention a CUDA version. However, it references the use of an H100 GPU, which typically supports CUDA 11.x and above. Since the issue involves PyTorch 2.1.0, it is likely compatible with CUDA 11.8 or higher.

If you need to specify a version based solely on the context provided, it would be reasonable to refer to a version widely compatible with recent PyTorch releases:

<CUDA 11.8>"
pytorch,https://github.com/pytorch/pytorch/issues/113808,2023-11-15T21:53:39Z,[ONNX][dynamo_export] ONNX::Celu Half unsupported but export passed w/ invalid model when opmath disabled,YES,"torch.aten_celu  
torch.onnx.celu",1.13.0,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/113744,2023-11-15T08:49:07Z,Regression: `capture_pre_autograd_graph` does not support empty args and kwargs only anymore,YES,<torch._export.capture_pre_autograd_graph>,2.2.0a0+git8f5fead,None
pytorch,https://github.com/pytorch/pytorch/issues/113737,2023-11-15T04:29:45Z,torch.compile + SAC: mutations in backward are not preserved,YES,torch.compile,PyTorch nightly,"The issue description does not mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/113707,2023-11-14T22:47:14Z,torch.compile doesnt respect use_determistic_algorithms during the backward(),YES,"torch.compile  
torch.use_deterministic_algorithms  
torch.nn.ReflectionPad2d  
aten.replication_pad2d_backward",<Library Version> 2.0.0,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/113612,2023-11-14T01:49:24Z,Graph breaks in APEX `FusedRMSNorm` causes bad interaction between NCCL allreduce and cudagraph tree,YES,torch.apex.fused_rms_norm_affine,PyTorch nightly,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/113600,2023-11-14T00:43:20Z,Segmentation fault in RPC worker when DataLoader has num_workers > 0,YES,torch.distributed.rpc,2.0.1+cu117,11.6
pytorch,https://github.com/pytorch/pytorch/issues/113589,2023-11-13T22:27:17Z,Fake tensor produces incorrect values w/ is_coalesced and sparse_coo,YES,"torch.sparse_coo_tensor  
torch.is_coalesced  ",main,"The issue description does not mention a specific CUDA version. Therefore, the response is:

<No CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/113560,2023-11-13T15:03:00Z,t.contiguous() ~10 slower in eager mode compared to torch.compile,YES,"torch.t  
torch.contiguous  
torch.compile",2.2.0.dev20231025,"The issue description does not explicitly mention a CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/113496,2023-11-10T23:17:35Z,"FSDP.forward() fails ""_is_root should not have been set"" error after saving a distributed checkpoint",YES,"<torch.distributed.checkpoint>  
<torch.distributed.fsdp.FullyShardedDataParallel>  
<torch.distributed.fsdp.api.StateDictType>  
<torch.distributed.fsdp.wrap.ModuleWrapPolicy>  ",2.2.0.dev20230920+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/113444,2023-11-10T13:53:18Z,ONNX Export - miscompilation for complex-valued operators,YES,<torch.onnx.dynamo_export>,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/113443,2023-11-10T12:32:32Z,`CompileProfiler` reports graph breaks while `dynamo.explain` reports no graph breaks,YES,"<torch._dynamo.explain>
<torch._dynamo.utils.CompileProfiler>",2.1.0+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/113415,2023-11-10T02:05:54Z,[inline-inbuilt-nn-modules] Torch compile with DDP errors on parameterized modules,YES,torch.compile,<Library Version> 2.1.0,"The issue description does not explicitly mention a CUDA version. Therefore, based on the provided text, there is no CUDA version referenced."
pytorch,https://github.com/pytorch/pytorch/issues/113370,2023-11-09T17:16:14Z,Incorrect stride when permuting shapes where a zero dimension is present.,YES,"torch.empty  
torch.permute  
torch.contiguous  
torch.view_as_complex  ",1.12.1+cu116,11.6
pytorch,https://github.com/pytorch/pytorch/issues/113326,2023-11-09T02:32:21Z,`torch._C._cuda_getDeviceCount` inflates system memory usage,YES,torch._C._cuda_getDeviceCount,2.2.0.dev20231108+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/113203,2023-11-07T21:00:23Z,`inference_mode` before training results in FSDP AssertionError,YES,"torch.inference_mode  
torch.distributed.fsdp.FullyShardedDataParallel  ",`torch==2.2.0.dev20231102+cu121`,cu121
pytorch,https://github.com/pytorch/pytorch/issues/113180,2023-11-07T18:35:47Z,Higher train loss and worse evaluation metrics when using `torch.compile()`,YES,<torch.compile>,2.2.0a0+git21b6030,12.1
pytorch,https://github.com/pytorch/pytorch/issues/113150,2023-11-07T10:44:24Z,"Aborted (core dumped) after Run Pytorch2.0.0, which I compiled by myself",YES,"torch::jit::createLlgaKernel  
torch::jit::createLlgaGuardKernel  
torch::cuda::shared::initCudartBindings  
cudaProfilerInitialize",2.0.0a0+gitc263bd4,11.7
pytorch,https://github.com/pytorch/pytorch/issues/113129,2023-11-07T05:45:59Z,[torch.compile] Dynamic shape behavior is different between using torch.compile with and without compiled_autograd.enable,YES,"<torch.compile>
<compiled_autograd.enable>",2.2.0.dev20231106+cpu,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/113067,2023-11-06T20:50:10Z,[ONNX] stft export fails with dynamo_export,YES,"<torch.onnx.dynamo_export>  
<torch.stft>",torch==2.2.0.dev20231106+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/113037,2023-11-06T17:40:49Z,"`TensorIteratorBase::is_scalar` return `false` for empty numpy tensors, but true for empty Torch ones",YES,torch.div,2.1,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/113034,2023-11-06T17:28:59Z,make_fx produces incorrect graph when used under FunctionalTensorMode,YES,<torch.fx.experimental.proxy_tensor.make_fx>,Main,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/113024,2023-11-06T13:49:39Z,Upsample bilinear 2d decomposition does not match native implementation for uint8,YES,"torch._C.nn.upsample_bilinear2d  
torch._decomp.decompositions.upsample_bilinear2d_vec",2.1.0+cpu,None
pytorch,https://github.com/pytorch/pytorch/issues/112844,2023-11-03T11:06:39Z,torch.export does not support torchaudio.transforms.Spectrogram,YES,"<torch.onnx.dynamo_export>  
<torch.jit.isinstance>  
<torchaudio.transforms.Spectrogram>  ",2.2.0.dev20231102+cpu,CUDA used to build PyTorch: Could not collect
pytorch,https://github.com/pytorch/pytorch/issues/112831,2023-11-03T07:31:45Z,dynamo_export successfully export model but fails at onnx.checker.check_model,YES,torch.onnx.export,2.2.0.dev20231102+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/112787,2023-11-02T21:00:43Z,"AssertionError: Global state changed while dynamo tracing, please report a bug",YES,"<torch.get_default_dtype>
<torch.set_default_dtype>
<torch.tensor>
<torch.compile>",torch==2.2.0.dev20231030+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/112775,2023-11-02T19:35:40Z,Multi-Threaded GraphModule / torch.fx inference raises an exception,YES,torch.fx.symbolic_trace,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/112714,2023-11-02T09:43:06Z,log_softmax() on CPU and GPU has expected numerical error when used with low-precision bfloat16,YES,torch.nn.functional.log_softmax,2.0.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/112575,2023-11-01T09:51:57Z,"RuntimeError: ""grid_sampler_2d_cuda"" not implemented for 'BFloat16'",YES,<torch.grid_sample>,2.2.0.dev20231006+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/112569,2023-11-01T06:51:05Z,"RuntimeError: NVML_SUCCESS == r INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1695392035891/work/c10/cuda/CUDACachingAllocator.cpp"":1154, please report a bug to PyTorch",YES,"torch.nn.modules.module  
torch.nn.modules.conv  
torch.nn.functional  ",2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/112515,2023-10-31T17:00:49Z,[Bug Report]FSDP: An error raises when loading FSDP distributed checkpoint with ignoring modules.,YES,"torch.distributed.fsdp.FullyShardedDataParallel  
torch.distributed.checkpoint.optimizer.load_sharded_optimizer_state_dict  
torch.distributed.checkpoint.save_state_dict  
torch.distributed.checkpoint.load_state_dict  ",torch==2.2.0.dev20231015+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/112495,2023-10-31T12:22:25Z,Compilation Failure of torch.special.exp2 in torch.compile Optimized Mode,YES,<torch.special.exp2>,2.2.0.dev20230921+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/112492,2023-10-31T11:21:55Z,Compilation Failure of torch.cumsum in torch.compile Optimized Mode,YES,torch.cumsum,2.2.0.dev20230921+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/112443,2023-10-30T21:21:38Z,torch.export emits node outside of Core ATen IR,YES,"torch.ops.aten.nonzero.default
torch.ops.aten._assert_async.msg
torch.ops.aten.sym_size.int
torch.ops.aten.scalar_tensor.default",pytorch main,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version based on the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/112376,2023-10-30T08:43:46Z,Contradictory Error Message for stride Argument in torch.conv_transpose3d(),YES,torch.conv_transpose3d,1.12.0,The issue description provided does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/112347,2023-10-29T17:45:28Z,"[dynamo] `.view([..., -1, ...])` fails on Tensors with unbacked SymInts in the shape",YES,torch.view,2.2.0a0+git8ff0036,12.0
pytorch,https://github.com/pytorch/pytorch/issues/112256,2023-10-27T16:21:01Z,[ONNX] In-place additon not being functionalized by torch.onnx.dynamo_export,YES,torch.onnx.dynamo_export,pytorch 2.0.0,"The description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version referenced in the issue."
pytorch,https://github.com/pytorch/pytorch/issues/112095,2023-10-25T23:17:57Z,torch.export fails on a model with optional parameter,YES,torch.export.export,2.1.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/112044,2023-10-25T17:43:00Z,pack_padded_sequence/pad_packed_sequence support in dynamo,YES,"torch.nn.utils.rnn.pack_padded_sequence
torch.nn.utils.rnn.pad_packed_sequence","<Library Version>
1.13.0","The issue description does not specifically mention any CUDA version. Therefore, there is no CUDA version referenced in the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/112024,2023-10-25T15:12:20Z,torch.inference_mode and tensor subclass: RuntimeError: Cannot set version_counter for inference tensor,YES,"torch.inference_mode  
torch.ops.aten.detach.default",main,"The issue description does not explicitly mention any CUDA version. Therefore, I cannot provide a specific CUDA version based on the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/112015,2023-10-25T10:23:04Z,op scaled_dot_product_attention case different results,YES,<torch.scaled_dot_product_attention>,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/111925,2023-10-24T15:37:51Z,Resize warning in two argument torch.logical_* with broadcasting,YES,torch.logical_and,2.1.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/111908,2023-10-24T11:42:42Z,Inconsistent Keyword Arguments behaviors in torch.triangular_solve(),YES,<torch.triangular_solve>,2.2.0.dev20231023+cu118,cu118
pytorch,https://github.com/pytorch/pytorch/issues/111901,2023-10-24T08:11:27Z,Multiprocess. DataLoader worker  is killed by signal: Segmentation fault.,YES,torch.utils.data.dataloader,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/111900,2023-10-24T07:47:04Z,"OOM when saving model(lora adapter), seems the clause ""FullyShardedDataParallel(model,...)"" will directly cause the OOM.",YES,<torch.distributed.fsdp.FullyShardedDataParallel>,2.2.0.dev20231012+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/111874,2023-10-24T00:37:27Z,Coalescing manager does not work w/device from torch.cuda.current_device(),YES,torch.cuda.current_device,main,"The issue description does not mention any specific CUDA version. Therefore, it is not possible to identify a CUDA version from the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/111834,2023-10-23T19:34:58Z,"Numerical inaccuracies in ""ddp_apply_optim_in_backward"" unit tests for gloo backend",YES,<torch.ddp_apply_optim_in_backward>,1.12.0,"The issue description does not explicitly mention a specific CUDA version. It refers to ""CUDA and ROCm"" environments generally but does not provide version numbers. Therefore, there is no identifiable CUDA version mentioned in the description."
pytorch,https://github.com/pytorch/pytorch/issues/111824,2023-10-23T18:22:50Z,GroupNorm & InstanceNorm does not handle channels_last correctly,YES,<torch.nn.GroupNorm>,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/111570,2023-10-19T15:24:58Z,Tensor `.cuda()` very slow with specific array sizes ,YES,<torch.cuda>,2.1.0+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/111509,2023-10-18T21:16:16Z,Sparse Tensor Sum Still Does Not Work for PyTorch Geometric,YES,torch.sum,2.1.0a0+32f93b1,12.2
pytorch,https://github.com/pytorch/pytorch/issues/111508,2023-10-18T21:13:04Z,LBFGS accuracy difference between CPU and GPU,YES,<torch.optim.LBFGS>,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/111480,2023-10-18T16:11:12Z,torch.jit.script persistently changes default from utf-8 to ascii,YES,<torch.jit.script>,2.1.0,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version based on the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/111187,2023-10-13T05:23:31Z,"torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1331, unhandled cuda error (run with NCCL_DEBUG=INFO for details)",YES,torch.distributed,2.1.0+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/111182,2023-10-13T03:24:50Z,Training iresnet with torch.compile is slower than eager mode for torch 2.1.0,YES,torch.compile,2.1.0+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/111159,2023-10-12T20:41:19Z,Wrong onnx model from `torch.onnx.export` when using `index_add_` function with duplicate `index` values.,YES,"<torch.onnx.export>
<torch.Tensor.index_add_>",1.9.0a0+gitd69c22d,11.1
pytorch,https://github.com/pytorch/pytorch/issues/111142,2023-10-12T16:30:09Z,'torch._C.Node' object has no attribute 'cs',YES,torch._C.Node,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/111138,2023-10-12T14:55:22Z,Module states cannot be fully synchronized due to the DDP broadcast_buffers breaking change,YES,<torch.nn.parallel.DistributedDataParallel>,2.1.0,The issue description does not mention a specific CUDA version. It only references a version of PyTorch (2.1.0) related to the breaking change discussed.
pytorch,https://github.com/pytorch/pytorch/issues/111135,2023-10-12T13:22:34Z,"No op for aten::where with argument types: Tensor, Tensor, bool.",YES,torch.where,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/111131,2023-10-12T10:52:36Z,"""Invalid Scalar type"" when using bf16 allreduce with Gloo backend",YES,<torch.bf16>,main,"The issue description does not mention any specific CUDA version. Therefore, I cannot provide a CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/110959,2023-10-10T17:37:02Z,`model.named_buffers()` fails if module not hashable.,YES,torch.nn.Module.named_buffers,2.1.0,12.1
pytorch,https://github.com/pytorch/pytorch/issues/110946,2023-10-10T12:55:19Z,RuntimeError: out_ptr == out_accessor[thread_count_nonzero[tid + 1]].data() INTERNAL ASSERT FAILED,YES,<torch.TensorAdvancedIndexing>,1.13.0,There is no specific CUDA version mentioned in the provided GitHub issue description.
pytorch,https://github.com/pytorch/pytorch/issues/110937,2023-10-10T09:37:58Z,Issue with torch.distributed.launch,YES,<torch.distributed.launch>,torch>=1.7,"The issue description does not explicitly mention a specific CUDA version. However, it refers to using `torch>=1.7`, which generally implies compatibility with CUDA versions 10.1 and above. Without a specific version mentioned, we cannot provide an exact CUDA version. 

If you are looking for a general guess based on the context, you might consider CUDA 10.1 or newer, but this is not definitive from the provided text. 

Therefore, the answer is:

<Cuda version not explicitly mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/110885,2023-10-09T19:29:22Z,Depthwise conv3d slower than normal conv3d,YES,<torch.nn.Conv3d>,2.1.0+cu121,12.1
pytorch,https://github.com/pytorch/pytorch/issues/110858,2023-10-09T10:03:41Z,Broadcasting matmul is much slower than corresponding einsum,YES,"torch.matmul  
torch.einsum",2.0.1,None
pytorch,https://github.com/pytorch/pytorch/issues/110775,2023-10-07T00:04:43Z,torch.distributed.pipeline skip module throws assert error that portal.grad is not None,YES,torch.distributed.pipeline,2.1.0+cu118,cu118
pytorch,https://github.com/pytorch/pytorch/issues/110505,2023-10-04T07:43:21Z,scaled_dot_product returns NaN arrays with eval(),YES,<torch.nn.functional.scaled_dot_product>,2.0.0+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/110387,2023-10-02T13:34:23Z,Change unsqueeze(0) to preserve memory layout contiguity of original tensor,YES,torch.unsqueeze,2.0.1,None
pytorch,https://github.com/pytorch/pytorch/issues/110363,2023-10-01T19:28:47Z,"nn.BatchNorm2d (track_running_stats = True) causes ""modified by an in-place operation"" error when in torch.nn.parallel.DistributedDataParallel",YES,<torch.nn.BatchNorm2d>,1.13.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/110356,2023-10-01T13:37:59Z,"Dropout signature inconsistent between `torch.dropout`, `torch.nn.Dropout` and `torch.nn.functional.dropout`",YES,"torch.dropout  
torch.nn.Dropout  
torch.nn.functional.dropout  
torch.nn.functional.alpha_dropout  
torch.nn.functional.feature_alpha_dropout",torch==2.0.1 and torch==2.2.0.dev20230912+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/114605,2023-09-29T10:28:46Z,vmap: Transform single-element tensor to integer,YES,<torch.vmap>,1.13.0,"The issue description provided does not mention any specific CUDA version. Therefore, the answer is:

<No CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/110148,2023-09-27T09:33:35Z,Torch.onnx.dynamo_export stuck at reshape,YES,torch.onnx.dynamo_export,2.1.0.dev20230830+cu118,cu118
pytorch,https://github.com/pytorch/pytorch/issues/110084,2023-09-26T16:24:02Z,scatter_add: Mixing 0-dim and 1-dim tensors,YES,torch.scatter_add,<Library Version> 1.10.0,"The description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the issue."
pytorch,https://github.com/pytorch/pytorch/issues/110029,2023-09-25T20:23:34Z,Dataloader resetting with num_workers=1 and persistent_workers=True,YES,torch.utils.data.get_worker_info,2.0.1+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/110014,2023-09-25T15:52:03Z,tan/tanh discrepancies with complex due to jiterator,YES,"torch.tan  
torch._foreach_tan",trunk,"The issue description does not explicitly mention a specific CUDA version. Therefore, based on the provided text, I cannot identify a CUDA version."
pytorch,https://github.com/pytorch/pytorch/issues/109586,2023-09-19T08:21:03Z,Max pool with negative integer inputs and channels_last memory layout gives the wrong values,YES,torch.max_pool2d,2.1.0.dev20230730+cpu,None
pytorch,https://github.com/pytorch/pytorch/issues/109497,2023-09-18T13:41:02Z,Very big differences in output of `torch.lobpcg` (values and run-time) compared to SciPy on a very ill-conditioned Laplacian matrix,YES,torch.lobpcg,2.1.0.dev20230802+cpu,"The issue description does not explicitly mention a CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/109453,2023-09-17T06:11:15Z,Inconsistent Behavior of `ConvTranspose2d` on CPU and CUDA,YES,torch.nn.ConvTranspose2d,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/109193,2023-09-13T11:10:21Z,"F.conv2d(input, weight, bias, self.stride, RuntimeError: cuDNN error: CUDNN_STATUS_MAPPING_ERROR",YES,torch.nn.Conv2d,1.13.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/109191,2023-09-13T10:27:51Z,Gradients across different ranks are not synchronized when using DDP,YES,"<torch.distributed.init_process_group>
<torch.nn.parallel.DistributedDataParallel>",2.0.1+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/109108,2023-09-12T13:27:01Z,Cannot construct `torch.sparse_coo_tensor` (but `scipy.sparse.coo_matrix` works fine): `TypeError: only integer tensors of a single element can be converted to an index`,YES,torch.sparse_coo_tensor,2.1.0.dev20230802+cpu,"The issue description does not explicitly mention the CUDA version. It only includes the PyTorch version, which is ""2.1.0.dev20230802+cpu."" Therefore, based on the provided text, I cannot identify a specific CUDA version."
pytorch,https://github.com/pytorch/pytorch/issues/109009,2023-09-11T08:13:31Z,TypeError: mask must have dtype bool,YES,<torch.flatten>,1.13.0+cu116,11.6
pytorch,https://github.com/pytorch/pytorch/issues/108840,2023-09-08T05:00:14Z,Tensor Parallel doesn't work with torch.compile,YES,torch.compile,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/108676,2023-09-06T17:45:45Z,RuntimeError when calling conv_transpose2d with groups,YES,torch.nn.functional.conv_transpose2d,2.0.0.post101,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/108642,2023-09-06T09:45:04Z,torch.topk returned values and indices are reordered if sorted=False,YES,torch.topk,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/108521,2023-09-04T15:54:12Z,"resutl of (torch.mm(a,b) does not match result of (a[:part,:], b)",YES,<torch.mm>,2.0.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/108493,2023-09-03T12:39:25Z,RWKV + Adam exp_avg_sq will change from positive to negative after loss.backward(),YES,torch.optim.Adam,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/108446,2023-09-01T21:06:26Z,`SymInt` input doesn't get optimized out from `torch.compiled()` graph even if unused,YES,torch.compile,400c4de53bb7b36066aef381313ed71e4a877e95,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/108442,2023-09-01T20:04:58Z,Torch compile generates incorrect graph on Llama model,YES,"<torch.jit.trace>  
<torch.jit.freeze>  
<torch.compile>  
<torch._dynamo.config>  
<torch._dynamo>  
<torch.autograd.profiler>  ",400c4de53bb7b36066aef381313ed71e4a877e95,"The issue description does not explicitly mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/108432,2023-09-01T17:53:14Z,Wrong result of first run with torch.compile() when backend is using torch.jit.trace() and model has inplace operators ,YES,"<torch.compile>
<torch.jit.trace>",2.1.0a0+git6d18fe9.dev,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/108407,2023-09-01T09:19:57Z,torch.einsum() computes different results on cpu and cuda on A100 GPU.,YES,torch.einsum,torch==2.1.0.dev20230812+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/108332,2023-08-31T07:48:19Z,RuntimeError: dims.value().size() == self->getMaybeRFactorDomain().size(),YES,torch.jit.script,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/108226,2023-08-30T10:24:22Z,torch.nn.functional.pad() with value type bool,YES,torch.nn.functional.pad,2.0.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/108210,2023-08-30T03:12:34Z,Using distributed RPC and DDP together triggers error.,YES,"torch.distributed.all_reduce  
torch.distributed.rpc.rpc_sync",2.0.1+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/108190,2023-08-29T22:22:30Z,[FSDP] incorrect backward prefetch order when using BackwardPrefetch.BACKWARD_POST,YES,<torch.BackwardPrefetch>,2.1.0a0+gitd3da6ce,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/108152,2023-08-29T14:43:21Z,`Tensor.uniform_` uses illegal argument name `from`,YES,torch.Tensor.uniform_,2.0.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/107999,2023-08-26T04:30:27Z,`upsample_bilinear2d_backward_out_cuda` is nondeterministic,YES,<torch.upsample_bilinear2d_backward_out_cuda>,1.9.0,The issue description does not specify a particular CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/107824,2023-08-23T22:08:03Z,torch.compile() fails when an `autograd.Function` gets called and torch.no_grad() is *not* being used,YES,"torch.compile()  
torch.no_grad()  
torch.autograd.Function  
torch.distributed.get_world_size()  
torch.distributed.get_rank()  
torch.distributed._functional_collectives.all_reduce  
torch.Tensor  
torch.nn.Module  
torch.nn.Linear  
torch.nn.SiLU  
torch.nn.init.trunc_normal_  ",2.1.0a0+gitc9f947d,12.0
pytorch,https://github.com/pytorch/pytorch/issues/107821,2023-08-23T21:36:32Z,`torch.distributions.Pareto.sample` sometimes gives `inf`,YES,torch.distributions.Pareto.sample,2.1.0a0+git51d0d12,12.1
pytorch,https://github.com/pytorch/pytorch/issues/107820,2023-08-23T21:32:53Z,`add_image_with_boxes` method from `torch.utils.tensorboard.writer.SummaryWriter` is broken,YES,torch.utils.tensorboard.writer.add_image_with_boxes,2.0.1,12.2.128
pytorch,https://github.com/pytorch/pytorch/issues/107678,2023-08-22T10:29:26Z,[Torch.fx] Torch fx failed to trace torch extension library,YES,<torch.fx.symbolic_trace>,2.1.0.dev20230821,11.8
pytorch,https://github.com/pytorch/pytorch/issues/107661,2023-08-22T04:12:13Z,A backward bug of dtensor seems to be caused by new_empty_strided,YES,<torch.distributed._tensor.api.new_empty_strided>,2.1.0a0+git849fbc6,None
pytorch,https://github.com/pytorch/pytorch/issues/107582,2023-08-21T11:00:36Z,[FakeTensor] `to` doesn't error with `allow_non_fake_inputs=False`,YES,torch.to,main,<CUDA version>
pytorch,https://github.com/pytorch/pytorch/issues/107556,2023-08-20T17:56:12Z,Integer multiplication overflow when running torch.nn.AdaptiveAvgPool2d,YES,torch.nn.AdaptiveAvgPool2d,1.13.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/107555,2023-08-20T17:54:59Z,Integer multiplication overflow when running torch.nn.MaxUnpool3d,YES,<torch.nn.MaxUnpool3d>,1.13.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/107554,2023-08-20T17:53:26Z,Integer multiplication overflow when running torch.diagflat,YES,<torch.diagflat>,1.13.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/107552,2023-08-20T17:37:18Z,Storage size calculation overflowed when running torch.nn.functional.interpolate,YES,<torch.nn.functional.interpolate>,1.13.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/107550,2023-08-20T17:32:11Z,Integer multiplication overflow when running torch.eye,YES,<torch.eye>,1.13.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/107548,2023-08-20T16:09:15Z,Integer calculation overflow when running torch.nn.functional.adaptive_avg_pool2d,YES,<torch.nn.functional.adaptive_avg_pool2d>,1.13.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/107546,2023-08-20T13:15:52Z,Integer overflow when running torch.nn.functional.upsample_bilinear,YES,torch.nn.functional.upsample_bilinear,1.13.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/107545,2023-08-20T13:12:49Z,Integer overflow when running torch.nn.functional.upsample,YES,torch.nn.functional.upsample,1.13.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/107544,2023-08-20T13:03:07Z,Integer overflow when running torch.nn.ReplicationPad3d,YES,<torch.nn.ReplicationPad3d>,1.13.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/107543,2023-08-20T12:24:46Z,Integer overflow when running torch.nn.AdaptiveAvgPool2d,YES,torch.nn.AdaptiveAvgPool2d,1.13.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/107541,2023-08-20T12:04:34Z,Integer overflow when running torch.nn.MaxUnpool2d,YES,torch.nn.MaxUnpool2d,1.13.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/107540,2023-08-20T12:01:24Z,Index out of bound when running torch.gather,YES,torch.gather,1.13.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/107539,2023-08-20T11:56:07Z,Integer overflow when running torch.nn.functional.max_unpool2d,YES,torch.nn.functional.max_unpool2d,1.13.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/107528,2023-08-19T23:16:26Z,Inconsistent results when running torch.arctanh,YES,torch.arctanh,1.12.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/107503,2023-08-19T02:37:47Z,[nightly][jit] bad constant exponent (e+38.f) in default_program fused_mul_div_add,YES,torch.jit.trace,2.1.0.dev20230817+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/107433,2023-08-18T02:34:31Z,No checks when running torch.nn.functional.ctc_loss with bogus inputs,YES,torch.nn.functional.ctc_loss,1.12.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/107432,2023-08-18T02:32:42Z,"Inconsistent results when running torch.nn.functional.embedding_bag on CPU (1.12.0, 1.13.0)",YES,torch.nn.functional.embedding_bag,1.12.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/107429,2023-08-18T01:36:33Z,Abort when running torch.set_num_interop_threads,YES,torch.set_num_interop_threads,1.12.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/107352,2023-08-17T03:02:53Z,dist.scatter is incompatible with transpose/permute operation,YES,<torch.dist.scatter>,2.0.1+cu117,cu117
pytorch,https://github.com/pytorch/pytorch/issues/107298,2023-08-16T15:12:16Z,dist.destroy_process_group did not destroy the process group well,YES,<torch.dist.destroy_process_group>,2.1.0.dev20230809+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/107256,2023-08-15T20:52:10Z,`torch.float8_e4m3fn` does not support `torch.cat`,YES,"torch.float8_e4m3fn  
torch.cat  
torch.ones  ",1.13.0,"The issue description does not specify a particular CUDA version. Therefore, I cannot provide a CUDA version based on the given text."
pytorch,https://github.com/pytorch/pytorch/issues/107201,2023-08-15T02:15:38Z,The difference between channels last backward and channels first backward of AvgPool2d on CUDA is too large,YES,torch.nn.AvgPool2d,PyTorch master,"The issue description does not specify a particular CUDA version. Therefore, I cannot provide a CUDA version as it is not mentioned in the text provided."
pytorch,https://github.com/pytorch/pytorch/issues/107175,2023-08-14T21:05:05Z,"sdp_kernel causes dynamo error on torch.compile(model, fullgraph=True)",YES,<torch.backends.cuda.sdp_kernel>,<Library Version> 2.0.0,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the given information."
pytorch,https://github.com/pytorch/pytorch/issues/107133,2023-08-14T12:14:26Z,torch.inverse throws error when DP but not in DDP or single GPU,YES,torch.inverse,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/107125,2023-08-14T09:18:31Z,combining `vmap` with NN containing `MaxPool2d' leads to discrepancies in output,YES,"torch.vmap  
torch.func.jacrev  
torch.nn.MaxPool2d",2.0.1,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/107102,2023-08-13T18:00:25Z,Error when using sparse_coo tensor with optimizer,YES,"<torch.sparse_coo>  
<torch.optim.Adam>  
<torch.autograd.backward>  ",<2.0.1+cu118>,11.8
pytorch,https://github.com/pytorch/pytorch/issues/107087,2023-08-12T16:21:00Z,RuntimeError with operations on torch.float8_e5m2 and torch.float_e4m3fn data types,YES,"torch.float8_e5m2  
torch.float8_e4m3fn",2.1.0a0+git89fd1b8,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/107081,2023-08-12T08:02:07Z,[FSDP] summon_full_params won't change parameters,YES,<torch.FSDP.summon_full_params>,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/107026,2023-08-11T09:51:25Z,a bug about tensor stride,YES,"torch.rand  
torch.transpose  
torch.contiguous  
torch.is_contiguous",2.0.0a0+gitc263bd4,"The description does not mention a specific CUDA version. It only includes the PyTorch version, which is '2.0.0a0+gitc263bd4'. Therefore, I cannot provide a CUDA version based on the given information."
pytorch,https://github.com/pytorch/pytorch/issues/106851,2023-08-09T08:05:50Z,RPC all_gather doesn't work with dynamic world size (world_size=None),YES,<rpc.api._all_gather>,1.13.0+cu116,11.6
pytorch,https://github.com/pytorch/pytorch/issues/106780,2023-08-08T13:32:31Z,inf and nan are mapped to quant_min in torch.fake_quantize_per_tensor_affine,YES,<torch.fake_quantize_per_tensor_affine>,2.0.1+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/106692,2023-08-07T05:36:43Z,torch.polygamma inconsistent with scipy.special.polygamma for n >= 1,YES,torch.polygamma,1.9.0,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/106690,2023-08-07T04:34:47Z,DDP grads not synced when static_graph=True and module output is a dict subclass?,YES,<torch.distributed.DistributedDataParallel>,2.0.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/106665,2023-08-05T12:53:17Z,Inconsistency between CPU and GPU for `Linear()` layer with input size 0,YES,torch.nn.Linear,2.0.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/106637,2023-08-04T21:44:48Z,Using retain_graph in backward() with FSDP,YES,<torch.distributed.fsdp.FullyShardedDataParallel>,<Library Version> 2.0.0.dev20230717,11.8
pytorch,https://github.com/pytorch/pytorch/issues/106634,2023-08-04T21:04:36Z,Confusing error message for DataLoader with num_workers=0 and non-zero timeout,YES,torch.utils.data.DataLoader,2.0.0,"The issue description does not explicitly mention a CUDA version. However, it does reference the version ""2.0.0"". This likely refers to the version of PyTorch rather than CUDA. Without specific mention of a CUDA version, it cannot be determined from the provided text.

Therefore, the answer is:

<No CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/106630,2023-08-04T18:47:08Z,no_grad() changes output of TransformerDecoder module during evaluation ,YES,torch.no_grad,2.0.1+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/106614,2023-08-04T14:19:47Z,Case study of torch.compile / cpp inductor on CPU: min_sum / mul_sum with 1d / matmul-like with static / dynamic shapes,YES,<torch.compile>,3.1.0.dev20230802+cpu,"The issue description does not explicitly mention a CUDA version. It primarily discusses CPU capabilities and configurations related to AVX512 and options for dynamic shapes in PyTorch. Therefore, there is no CUDA version referenced in the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/106584,2023-08-03T23:23:15Z,Lacking commutativity of `tensor.expand` and `tensor.flatten`,YES,torch.flatten,2.1.0 nightly,"The CUDA version mentioned in the issue description is not explicitly stated. However, the version of PyTorch referenced is ""2.1.0 nightly,"" which typically corresponds to the latest CUDA version at the time of its release. Unfortunately, without specific mention of the CUDA version in the text, I cannot provide a definitive answer. 

If you need the CUDA version for PyTorch 2.1.0 nightly, you may want to refer to the PyTorch release notes or documentation for that specific version."
pytorch,https://github.com/pytorch/pytorch/issues/106579,2023-08-03T21:47:27Z,"Boolean valued images loaded from disk, when converted to torch int/float tensor, the True valued pixels gets converted to 255 instead of 1",YES,<torch.tensor>,1.9.0+cu111,"11.1, 10.2, 11.7"
pytorch,https://github.com/pytorch/pytorch/issues/106469,2023-08-02T17:48:04Z,Extreme slowdown of torch.mm for certain sizes and strides with bfloat16,YES,torch.mm,1.13.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/106467,2023-08-02T16:24:15Z,nn.CrossEntropyLoss with invalid target generates corrups memory eventualy leading to CUDA error: an illegal memory access,YES,<torch.nn.CrossEntropyLoss>,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/106362,2023-08-01T04:00:04Z,Calling ops.aten.embedding_bag() function got silent crash,YES,torch.ops.aten.embedding_bag,2.0.1+CPU,"The issue description mentions the version ""2.0.1+CPU"", which indicates that it is a CPU version and does not specify a CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/106243,2023-07-29T01:03:48Z,OneCycleLR's state_dict includes a full reference to the optimizer,YES,torch.optim.lr_scheduler.OneCycleLR,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/106220,2023-07-28T20:21:10Z,torch compile does not work with torch.nn.functional.softmax ?,YES,torch.nn.functional.softmax,<Library Version> 2.0.0,"The issue description does not mention a specific CUDA version. Therefore, the answer is:

<No CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/106006,2023-07-26T03:43:07Z,[torch.compile] assertion sometimes ignored with inductor backend,YES,"torch.compile  
torch._dynamo",2.1.0.dev20230725+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/105918,2023-07-25T13:01:29Z,Torch.jit : RuntimeError:  Unable to extract string literal index for ModuleDict,YES,torch.jit.script,torch-2.1.0.dev20230523,There is no specific CUDA version mentioned in the provided issue description.
pytorch,https://github.com/pytorch/pytorch/issues/105917,2023-07-25T12:49:22Z,Torch.jit.frontend.NotSupportedError: not supporting functions with variable number of arguments.,YES,torch.jit.script,torch-2.1.0.dev20230523,"The issue description does not explicitly mention a CUDA version. Therefore, it is not possible to identify a CUDA version from the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/105916,2023-07-25T12:41:36Z,Missing coalesced flag from `torch.autograd.Function.backward`,YES,torch.autograd.Function.backward,2.0.1+cpu,None
pytorch,https://github.com/pytorch/pytorch/issues/105878,2023-07-24T22:55:32Z,FakeTensor detach() gives meta tensor other than FakeTensor under `torch._C._DisableTorchDispatch()`,YES,torch.detach,2.1.0.dev20230524+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/105840,2023-07-24T15:00:59Z,[FSDP] FSDP doesn't work (random accuracy performance) when using `param_init_fn` and `sync_module_states=True`,YES,<torch.nn.FullyShardedDataParallel>,2.0.1+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/105768,2023-07-21T22:03:11Z,torch.compile uses more memory when using less parameters,YES,torch.compile,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/105635,2023-07-20T03:48:09Z,FSDP with gradient checkpointing lead to redundant allgathers during backward,YES,<torch.FSDP>,2.1.0.dev20230709+cu121,cu121
pytorch,https://github.com/pytorch/pytorch/issues/105548,2023-07-19T17:23:18Z,torch.sparse.sampled_addmm doesn't compute gradients for 3D tensors,YES,torch.sparse.sampled_addmm,2.0.1+cu117,cu117
pytorch,https://github.com/pytorch/pytorch/issues/105326,2023-07-17T14:37:29Z,Can't vmap over torch.tensor constructor,YES,"torch.tensor  
torch.vmap",main,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/104959,2023-07-11T12:33:15Z,Saving a LightningModule torch.jit.ScriptModule is incompatible with torch.amp.autocast,YES,"torch.amp.autocast  
torch.jit.script",torch==2.0.1,"The issue description does not explicitly mention the CUDA version. Therefore, based on the provided information, I cannot identify a specific CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/104867,2023-07-10T12:19:05Z,rfftn and irfftn operations in pt2 return different results compared to v1.12.1,YES,"torch.fft.rfftn  
torch.fft.irfftn",2.0.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/104857,2023-07-10T02:44:26Z,Torch's `LayerNorm` and Adam optimizer vs those in tensorflow,YES,"torch.LayerNorm
torch.optim.Adam",2.0.0+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/104933,2023-07-08T03:37:16Z,torch.func.jvp fails with BERT training,YES,torch.func.jvp,<Library Version> 1.13.0,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version to provide."
pytorch,https://github.com/pytorch/pytorch/issues/104776,2023-07-07T15:41:47Z,torch.nn.CrossEntropyLoss: class weighting changes label_smoothing,YES,torch.nn.CrossEntropyLoss,torch 1.12.1+cu113,cu113
pytorch,https://github.com/pytorch/pytorch/issues/104732,2023-07-06T20:08:35Z,"torch.jit.trace says ""Arguments for call are invalid"" on torch.ops.aten.sub(3, x, alpha=3)",YES,torch.ops.aten.sub,2.1.0.dev20230705+cpu,None
pytorch,https://github.com/pytorch/pytorch/issues/104712,2023-07-06T16:43:20Z,torch.jit slicing error (styleganv2),YES,torch.jit,1.5.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/104704,2023-07-06T14:02:02Z,generate_vmap_rule=True sometimes gives batched grad_output,YES,"torch.func.jacrev  
torch.func.vmap",main,"The issue description does not mention any specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/104701,2023-07-06T12:17:30Z,System memory leak when using different input size of torch.nn.Conv3d,YES,<torch.nn.Conv3d>,2.0.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/104698,2023-07-06T10:31:43Z,Incorrect Error Message Ordering for nn.AdaptiveAvgPool2d with Incorrect output_size ,YES,torch.nn.AdaptiveAvgPool2d,2.1.0.dev20230622+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/104620,2023-07-05T08:36:54Z,`torch.distributed.rpc.backend_registry.register_backend` fails to update `BackendType` enum,YES,torch.distributed.rpc.backend_registry.register_backend,2.0.0+cpu,None
pytorch,https://github.com/pytorch/pytorch/issues/104602,2023-07-04T14:16:11Z,"F.adaptive_avg_pool3d(input, 1) returns infinity in half precision",YES,<torch.nn.functional.adaptive_avg_pool3d>,"torch==1.10.1+cu113, torch==1.13.1+cu117","10, 11.7"
pytorch,https://github.com/pytorch/pytorch/issues/104405,2023-06-29T11:50:26Z,Detailed error: Tensor-likes are not close! When use torch.jit.trace,YES,<torch.jit.trace>,Torch 1.13.0,cuda11.7
pytorch,https://github.com/pytorch/pytorch/issues/104297,2023-06-27T21:01:43Z,"torch.distributed.all_to_all_single & alltoall_base, size limit INT_MAX",YES,"<torch.distributed.all_to_all_single>
<torch.distributed.alltoall_base>",<2.0.1>,11.7
pytorch,https://github.com/pytorch/pytorch/issues/104194,2023-06-26T16:37:04Z,View ops on fake tensors can dispatch `detach`es to backend kernels,YES,<torch.detach>,2.1.0a0+git4ff3108,None
pytorch,https://github.com/pytorch/pytorch/issues/104174,2023-06-26T07:44:44Z,distributed.scatter memory leak in source rank,YES,torch.distributed.scatter,2.0.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/104107,2023-06-23T16:21:58Z,Tensor to_sparse fails on large matrices,YES,<torch.Tensor.to_sparse>,1.13.0a0+936e930,11.8
pytorch,https://github.com/pytorch/pytorch/issues/104081,2023-06-23T00:22:03Z,Distributing HSDP checkpoint writing for load balancing ,YES,"torch.distributed.fsdp.FullyShardedDataParallel  
torch.distributed.fsdp.MixedPrecision  
torch.distributed.fsdp.ShardingStrategy  
torch.distributed.fsdp.wrap.enable_wrap  
torch.distributed.fsdp.wrap.transformer_auto_wrap_policy  
torch.distributed.fsdp.wrap.wrap  
torch.distributed._shard.checkpoint.FileSystemWriter  
torch.distributed._shard.checkpoint.SavePlan  
torch.distributed._shard.checkpoint.save_state_dict  ",Pytorch nightlies,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/103749,2023-06-16T13:59:29Z,SDPA produces NaN with padding mask,YES,<torch.nn.functional.scaled_dot_product_attention>,Current master,The description does not explicitly mention a CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/103681,2023-06-15T16:31:01Z,Exported model with dropout incorrectly applies dropout during eval,YES,torch.ops.aten.dropout.default,1.8.0,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/103589,2023-06-14T12:57:48Z,`interpolate` with `antialias=True` on CUDA doesn't work if the difference of spatial size is large,YES,torch.nn.functional.interpolate,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/103518,2023-06-13T14:30:16Z,`gradcheck` produces false positives with sparse inputs when `masked=False`.,YES,<torch.sparse.sampled_addmm>,Current master,"The issue description does not explicitly mention a specific CUDA version. Therefore, there is no CUDA version referenced in the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/103483,2023-06-13T03:17:38Z,torch._dynamo.exc.Unsupported: Tensor.backward with aten_graph=True,YES,"torch._dynamo.exc.Unsupported
torch.ops.aten.view.default
torch.ops.aten.t.default
torch.ops.aten.addmm.default
torch.ops.aten.relu.default
torch.ops.aten.detach.default
torch.ops.aten._log_softmax.default
torch.ops.aten.nll_loss_forward.default",master,"The issue description does not explicitly mention a CUDA version. Therefore, based on the provided information, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/103397,2023-06-11T20:55:04Z,LayerNorm freeze processes using torch multiprocessing,YES,<torch.nn.LayerNorm>,torch==2.0.1,"The issue description does not explicitly mention a specific CUDA version. Therefore, it is not possible to identify a CUDA version from the provided text. 

If you have further context or information, I can help analyze that."
pytorch,https://github.com/pytorch/pytorch/issues/103316,2023-06-09T09:58:06Z,binary_cross_entropy (loss) seems to be giving incorrect values for very negative logits,YES,torch.nn.functional.binary_cross_entropy,1.12.1,"The CUDA version mentioned in the description is not explicitly stated. The only version provided is ""1.12.1,"" which refers to the PyTorch version. Therefore, I cannot identify a specific CUDA version from the given information."
pytorch,https://github.com/pytorch/pytorch/issues/103243,2023-06-08T09:23:35Z,torch.cuda.memory_reserved always returns 0 bytes,YES,torch.cuda.memory_reserved,1.13.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/103060,2023-06-06T04:19:11Z,Symbolic trace error about torch.nn.functional.pad,YES,<torch.nn.functional.pad>,1.13.0+cu116,11.6
pytorch,https://github.com/pytorch/pytorch/issues/102963,2023-06-05T08:47:20Z,torch.svd fails on large matrices,YES,torch.svd,1.13.0a0+936e930,11.8
pytorch,https://github.com/pytorch/pytorch/issues/102953,2023-06-05T06:17:13Z,TypeError: (): incompatible function arguments,YES,torch.distributed.init_process_group,torch2.0.0,cu117
pytorch,https://github.com/pytorch/pytorch/issues/102948,2023-06-05T03:16:15Z,[onnx] aten::cumprod cannot be exported to ONNX,YES,torch.aten::cumprod,2.0.1+cu117,cu117
pytorch,https://github.com/pytorch/pytorch/issues/102936,2023-06-04T16:17:09Z,torch.cuda.is_available() returns False on GTX 1650 with cuda 11.7 and torch==2.0.0+cpu,YES,torch.cuda.is_available(),2.0.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/102894,2023-06-02T23:51:09Z,"BCELoss and BCEWithLogitsLoss differ when one of the input logits is float(""inf"")",YES,"torch.nn.BCEWithLogitsLoss  
torch.nn.BCELoss  ",1.12.1,"The CUDA version mentioned in the description is not explicitly stated. The only version provided is ""1.12.1,"" which appears to refer to a library version, likely PyTorch. The issue does not specify the CUDA version directly. Therefore, based on the provided text, the CUDA version cannot be identified."
pytorch,https://github.com/pytorch/pytorch/issues/102832,2023-06-02T09:02:01Z,TypeError: (): incompatible function arguments,YES,torch.distributed.init_process_group,2.0.0+cu117,cu117
pytorch,https://github.com/pytorch/pytorch/issues/102610,2023-05-31T11:06:47Z,Cannot invoke prims.sum with output_dtype,YES,<torch.ops.prims.sum>,2.1.0.dev20230526+cpu,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/102609,2023-05-31T10:58:54Z,[prims] torch.ops.aten.le decomposition confuses scalars and tensors,YES,"torch.ops.aten.le  
torch.ops.prims.le",2.1.0.dev20230526+cpu,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/102438,2023-05-27T15:25:05Z,DDP multi node multi gpu inconsistent params,YES,<torch.distributed>,2.0.1,11.8
pytorch,https://github.com/pytorch/pytorch/issues/102435,2023-05-27T10:30:08Z,torch.distributed.all_reduce() has inconsistent behavior,YES,torch.distributed.all_reduce,<torch 1.12.0>,CUDA-11.7
pytorch,https://github.com/pytorch/pytorch/issues/102400,2023-05-26T20:27:31Z,Unexpected Behavior when using torch.isclose(),YES,<torch.isclose>,Nightly,"The issue description does not explicitly mention a specific CUDA version. It only states ""Nightly"" under the Versions section, which typically refers to the latest development version of the software, but does not specify an exact CUDA version. Therefore, no CUDA version can be identified from the provided text. 

If you have further questions or need assistance with something else, feel free to ask!"
pytorch,https://github.com/pytorch/pytorch/issues/102355,2023-05-26T07:36:55Z,Data type mismatch in `batch_isend_irecv` docstring example,YES,torch.distributed.batch_isend_irecv,1.13,"The description does not mention a specific CUDA version. Therefore, I cannot provide a CUDA version as requested."
pytorch,https://github.com/pytorch/pytorch/issues/102261,2023-05-25T07:34:07Z,ExponentialLR unexpectedly calls `step()` when init argument `last_epoch` is larger than -1,YES,<torch.optim.lr_scheduler.ExponentialLR>,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/102197,2023-05-24T19:55:43Z,torch.func.jvp fails when acting on a DistributedDataParallel model,YES,torch.func.jvp,2.1.0.dev20230522+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/102109,2023-05-23T09:30:31Z,Can't vmap over a slice expression,YES,torch.vmap,1.10.0,The issue description provided does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/101968,2023-05-22T09:21:04Z,CrossEntropyLoss output difference on Windows vs. Linux,YES,torch.nn.CrossEntropyLoss,1.11.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/101855,2023-05-19T05:15:11Z,torch.jit.trace() Floating point exception,YES,torch.jit.trace,1.10.2+cu111,cu111
pytorch,https://github.com/pytorch/pytorch/issues/101850,2023-05-19T03:12:11Z,Unexpected modification to CPU affinity of Dataloader workers,YES,torch.utils.data.DataLoader,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/101669,2023-05-17T06:15:34Z,[refs] inplace references resize the input to match the broadcasted input shape,YES,<torch._refs.add_>,master,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/101666,2023-05-17T03:53:35Z,Unexpected behavior of fmod op in some float32 input,YES,torch.fmod,1.10.0,The description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/101653,2023-05-17T01:17:16Z,Unexpected behavior comparing uint8 tensor to value greater than 255,YES,<torch.tensor>,1.0.0,11.8
pytorch,https://github.com/pytorch/pytorch/issues/101632,2023-05-16T23:58:51Z,torch.profiler.profile has an empty python replay stack under certain circumstances,YES,<torch.profiler.profile>,2.0.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/101428,2023-05-15T17:57:33Z,2D inputs to linear layers run up to 25% slower than 4D ones on some Nvidia GPUs,YES,torch.nn.Linear,2.0.1,"CUDA 11.7, CUDA 11.8"
pytorch,https://github.com/pytorch/pytorch/issues/101402,2023-05-15T12:05:58Z,DataParallel for nested modules,YES,<torch.nn.DataParallel>,1.10.1,"The issue description does not explicitly mention a specific CUDA version. However, it references the PyTorch version (1.10.1) and the GPU used (V100). The CUDA version typically associated with PyTorch 1.10.1 is CUDA 11.3. 

Therefore, the CUDA version can be identified as:

CUDA 11.3"
pytorch,https://github.com/pytorch/pytorch/issues/101385,2023-05-15T06:28:47Z,torch.Tensor.is_sparse returns false for non-COO sparse tensors,YES,<torch.Tensor.is_sparse>,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/101356,2023-05-14T08:31:07Z,inductor: inductor conv2d get a different size and stride with eager mod when input channel is zero,YES,torch.nn.Conv2d,2.1.0a0+git6211ead,None
pytorch,https://github.com/pytorch/pytorch/issues/101249,2023-05-12T02:59:52Z,`einsum` is about 40x slower on CUDA than manually multiplying and summing,YES,torch.einsum,<torch 2.0.0>,12.1
pytorch,https://github.com/pytorch/pytorch/issues/101241,2023-05-12T01:28:50Z,Different results with vmap when using torch.jit.script,YES,"<torch.jit.script>
<torch.jit.fork>
<torch.func.vmap>
<torch.autograd.grad>",2.0.0+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/101160,2023-05-11T06:33:41Z,Fine-tuning HuggingFace wav2vec 2.0 with `torch.compile`,YES,<torch.compile>,2.0.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/101075,2023-05-10T14:18:05Z,torch.lobpcg producing different largest eigenvalue than scipy and np.linalg.eig,YES,<torch.lobpcg>,1.13.1+cu116,11.6
pytorch,https://github.com/pytorch/pytorch/issues/101031,2023-05-10T00:05:06Z,[PT2] torch.compile doesn't perform horizontal fusion,YES,torch.compile,2.1.0.dev20230504,11.8
pytorch,https://github.com/pytorch/pytorch/issues/100985,2023-05-09T16:44:54Z,"native_batch_norm has different size results on ""CPU"" vs ""META"" device",YES,torch.ops.aten.native_batch_norm,2.1.0.dev20230505+cpu,None
pytorch,https://github.com/pytorch/pytorch/issues/100932,2023-05-09T01:26:39Z,torch.concat fails with float16 input in autocast(device_type=cpu) context,YES,<torch.concat>,2.0.0,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/100850,2023-05-08T05:08:12Z,[BUG] Poor torch.bmm performance on H100,YES,torch.bmm,1.14.0a0+44dac51,12.0
pytorch,https://github.com/pytorch/pytorch/issues/100838,2023-05-07T22:18:48Z,Dynamo infers different return type vs. eager for `torch.ops.aten`,YES,"torch.ops.aten.baddbmm  
torch.ops.aten.full  
torch.ops.aten.threshold_backward",2.1.0.dev20230428+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/100807,2023-05-06T18:19:52Z,[torch.compile] returns output with WRONG SHAPE after `cat_slice_cat`,YES,torch.compile,2.1.0.dev20230503+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/100784,2023-05-06T02:54:35Z,[CUDA RPC] Incorrect results of GPU Tensor transferring using RPC when parallelized with other GPU programs,YES,<torch.distributed.rpc>,2.0.0+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/100705,2023-05-05T10:14:38Z,torch.cuda.amp.GradScaler initialization ,YES,torch.cuda.amp.GradScaler,"torch-1.6, torch-1.12","The issue description does not explicitly mention a CUDA version. Therefore, based on the provided information, no CUDA version can be identified."
pytorch,https://github.com/pytorch/pytorch/issues/100582,2023-05-03T21:29:21Z,"Synchronization issue when combining DPP and RPC - ""Parameter marked twice""",YES,"<torch.distributed.rpc>  
<torch.nn.parallel.DistributedDataParallel>  
<torch.distributed>",2.0.0+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/100528,2023-05-03T06:13:43Z,Backward hook execution order changes when input.requires_grad is False,YES,"torch.nn.modules.module.register_module_full_backward_pre_hook  
torch.nn.modules.module.register_module_full_backward_hook",1.14.0a0+44dac51,12.0
pytorch,https://github.com/pytorch/pytorch/issues/100411,2023-05-01T20:06:07Z,Sparse Matrix nnz Overflow when casting from COO to CSR,YES,"torch.sparse_coo_tensor  
torch.sparse_csr_tensor  
to_sparse_csr  ",1.12.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/100241,2023-04-28T10:50:28Z,Interaction of torch.no_grad and torch.autocast context managers with torch.compile,YES,"<torch.no_grad>
<torch.autocast>
<torch.compile>",N/A,N/A
pytorch,https://github.com/pytorch/pytorch/issues/100158,2023-04-27T09:27:21Z,`torch.sparse_csc_tensor` matrix multiplication produces MKL error SPARSE_STATUS_ALLOC_FAILED when density is too high,YES,torch.sparse_csc_tensor,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/100145,2023-04-27T03:18:10Z,This flag not work : torch.backends.cudnn.allow_tf32 = False  ,YES,"torch.backends.cudnn.allow_tf32  
torch.backends.cuda.matmul.allow_tf32",1.13.0a0+08820cb,"The issue description does not explicitly mention a CUDA version. However, it provides the version of PyTorch being used, which is 1.13.0a0+08820cb. Typically, to find the compatible CUDA version for a specific PyTorch version, one would refer to the PyTorch documentation or release notes.

Since the issue description does not state a specific CUDA version, I cannot provide one based solely on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/100105,2023-04-26T17:06:24Z,[pt2-functorch] torch.func.functional_call works with func.vmap but breaks for func.grad,YES,"<torch.func.functional_call>
<torch.func.grad>
<torch.func.vmap>
<torch.compile>
<functorch.make_functional_with_buffers>
<functorch.vmap>
<functorch.grad>",master,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/100087,2023-04-26T12:44:11Z,TransformerEncoderLayer behavior inconsistent between training and evaluation mode,YES,<torch.nn.TransformerEncoderLayer>,2.0.0+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/100085,2023-04-26T12:40:00Z,[regression] torch.norm with out dtype bfloat16 cause runtime error,YES,torch.norm,2.0.0,CUDA version: 11
pytorch,https://github.com/pytorch/pytorch/issues/100051,2023-04-26T01:53:22Z,Issue of HistogramObserver to handle abnormal value,YES,torch.ao.quantization.observer.HistogramObserver,2.1.0a0+gita15539d,None
pytorch,https://github.com/pytorch/pytorch/issues/99932,2023-04-24T22:17:50Z,FSDP + gradient clipping raises an odd warning with the simplest model on torch 2.0,YES,"<torch.distributed.fsdp.FullyShardedDataParallel>  
<torch.distributed.fsdp.api.ShardingStrategy>",2.0.0+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/99821,2023-04-23T09:56:25Z,Tracer cannot infer type of Seq2SeqLMOutput,YES,<torch.jit.trace>,torch2.0,<CUDA version>
pytorch,https://github.com/pytorch/pytorch/issues/99652,2023-04-20T19:17:12Z,DistributedDataParallel doesn't work with complex buffers,YES,torch.nn.parallel.DistributedDataParallel,2.1.0.dev20230416+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/99556,2023-04-19T19:09:46Z,torch.func.jacrev fails if model contains full_backward_hook,YES,torch.func.jacrev,<2.0.0>,11.8
pytorch,https://github.com/pytorch/pytorch/issues/99432,2023-04-18T17:39:17Z,[DTensor] parallelize_module failed with nn.Transformer and the PairwiseParallel plan,YES,"<torch.distributed.tensor.parallel.parallelize_module>
<torch.nn.Transformer>
<torch.distributed.tensor.parallel.PairwiseParallel>",2.1.0.dev20230417,11.7
pytorch,https://github.com/pytorch/pytorch/issues/99414,2023-04-18T12:45:34Z,The meta implementation of `index_put` does not do any check,YES,torch.index_put_,master,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version as requested."
pytorch,https://github.com/pytorch/pytorch/issues/99410,2023-04-18T11:41:22Z,"torch.nn.functional.multilabel_margin_loss cuda lacks checking of ""out of bound""",YES,"torch.nn.functional.multilabel_margin_loss  
torch.nn.MultiLabelMarginLoss",2.0.0+cu118,cu118
pytorch,https://github.com/pytorch/pytorch/issues/99265,2023-04-16T13:44:32Z,the error message of torch.addcmul is wrong,YES,torch.addcmul,2.0.0+cu118,cu118
pytorch,https://github.com/pytorch/pytorch/issues/99149,2023-04-14T13:59:29Z,"Spectral Normalization can not be applied to Conv{1,2,3}d",YES,torch.nn.utils.parametrizations.spectral_norm,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/99147,2023-04-14T12:53:28Z,`torch.sparse.sum` backward fails when reducing over dense dimensions.,YES,<torch.sparse.sum>,Current master,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/99012,2023-04-13T03:40:26Z,"[BUG]Float32 attention mask not working with torch.autocast(""cpu"")",YES,"torch.autocast  
torch.nn.MultiheadAttention",torch==2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/98948,2023-04-12T16:53:22Z,behaviour of `torch.tensor()` changes after editing `Tensor.__getitem__`,YES,<torch.tensor>,2.0.0+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/98939,2023-04-12T15:19:18Z,"torch.dist with minus norm returns tensor(0.), while with -inf can return result",YES,torch.dist,2.0.0,118
pytorch,https://github.com/pytorch/pytorch/issues/98929,2023-04-12T09:54:46Z,torch.sparse_csr_tensor() stops gradients,YES,torch.sparse_csr_tensor(),2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/98861,2023-04-11T19:00:20Z,Sparse Tensor not working for `torch.cat`,YES,torch.cat,2.0.0a0+1767026,12.1
pytorch,https://github.com/pytorch/pytorch/issues/98675,2023-04-08T16:29:08Z,torch.matmul with batched CSR matrix,YES,torch.matmul,2.0.0+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/98668,2023-04-08T06:01:09Z,Cannot use `checkpoint_sequential` with `torch.compile`,YES,torch.utils.checkpoint.checkpoint_sequential,2.1.0.dev20230406+cu118,CUDA 11.7 and CUDA 11.8
pytorch,https://github.com/pytorch/pytorch/issues/98566,2023-04-07T02:30:08Z,`F.interpolate` and `F.grid_sample` - documentation error and bug,YES,"<torch.nn.functional.interpolate>
<torch.nn.functional.grid_sample>",1.13.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/98557,2023-04-06T23:43:55Z,torch.jit.script codegen warning with cuda and vmap,YES,"torch.jit.script  
torch.vmap",2.0,The CUDA version is not explicitly mentioned in the issue description provided.
pytorch,https://github.com/pytorch/pytorch/issues/98541,2023-04-06T21:19:23Z,Memory corruption using torch.ops.* to access re-registered operator,YES,torch.ops.foo.sum,master,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/98537,2023-04-06T20:29:49Z,Segfault when using torch.ops.* to access de-registered op,YES,torch.ops.foo.sum,master,"The issue description does not mention any specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/98499,2023-04-06T13:03:23Z,`torch.nn.utils.rnn.unpad_sequence` modifies arguments in-place,YES,torch.nn.utils.rnn.unpad_sequence,1.13.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/98498,2023-04-06T12:36:57Z,"Higher order derivatives not working when setting compute device to `torch.device(""mps"")`",YES,<torch.func.functional_call>,2.0.0,"The issue description does not explicitly mention a CUDA version. However, it does mention the use of `torch.device(""cuda"")`, indicating that CUDA is being used, but no specific version is provided. Therefore, it is not possible to extract a CUDA version from the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/98422,2023-04-05T16:34:29Z,[FX] Symbolic trace over `torch.Tensor.${fn}` APIs,YES,"torch.Tensor.flatten  
torch.Tensor.flip",2.1.0.dev20230403+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/98286,2023-04-04T09:01:24Z,"When I use the DDP model, I use a custom loss function, when the batch size changes during training, the process will be stuck.",YES,<torch.nn.parallel.DistributedDataParallel>,1.13.0+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/98210,2023-04-03T13:27:17Z,torch.jit.script + legacy executor mode has diff in some pattern,YES,torch.jit.script,torch==2.0.0,"The issue description does not explicitly mention a CUDA version. Therefore, no CUDA version can be identified from the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/98203,2023-04-03T08:48:21Z,"AssertionError: was expecting embedding dimension of 22, but got 1320",YES,<torch.nn.TransformerEncoder>,1.12.0,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/98142,2023-04-01T13:35:10Z,double free or corruption (fasttop),YES,"torch.normal  
torch.rand  
torch.to  
torch::kFloat32  
torch::kCPU  ",2.0.0,11.8.89
pytorch,https://github.com/pytorch/pytorch/issues/98136,2023-04-01T08:58:55Z,A Segment Fault can be triggered in torch._grid_sampler_2d_cpu_fallback,YES,torch._grid_sampler_2d_cpu_fallback,2.1.0.dev20230331+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/98089,2023-03-31T18:25:16Z,GroupNorm cpu/gpu parity tests fail with pretty large differences,YES,torch.nn.GroupNorm,master,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/98075,2023-03-31T10:36:41Z,Unexpected results with torch.nn.functional.layer_norm,YES,torch.nn.functional.layer_norm,1.13.1+cu116,11.6
pytorch,https://github.com/pytorch/pytorch/issues/98064,2023-03-31T07:13:17Z,Module 'Sequential' has no attribute '_modules' :,YES,torch.jit.script,2.0.0+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/98004,2023-03-30T19:15:37Z,matmul with CSR matrix in inference mode throws an exception,YES,torch.matmul,1.13.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/98002,2023-03-30T18:53:40Z,DataLoader with collate_fn that returns tensors in GPU memory raises warnings when deleted,YES,torch.utils.data.DataLoader,PyTorch 1.13.0+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97913,2023-03-29T21:40:31Z,BUG torch.jit.annotate on List + torch.stack give wrong DTYPE,YES,"torch.jit.annotate  
torch.stack  ",2.0.0,cu117
pytorch,https://github.com/pytorch/pytorch/issues/97872,2023-03-29T09:47:35Z,nn.linear not support bfloat16,YES,<torch.nn.Linear>,torch2.0 and torch1.13,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/97847,2023-03-29T01:52:30Z,CUDA 10.2 cudnn 8.2.4 run Conv2d error,YES,torch.nn.functional.conv2d,pytorch 1.8.0,CUDA 10.2
pytorch,https://github.com/pytorch/pytorch/issues/97823,2023-03-28T20:39:52Z,Using `param in param_list` can trigger `non-singleton dimension` error?,YES,<torch.nn.Parameter>,2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97784,2023-03-28T15:15:01Z,torch.compile fails with torch._dynamo.exc.TorchRuntimeError on  a function that contains a torch script module,YES,"torch.compile  
torch._dynamo.exc.TorchRuntimeError  
torch.jit.trace",2.0 stable,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version as referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/97668,2023-03-27T13:54:17Z,sparse_csr_tensor matmul wrong output in bfloat16,YES,<torch.sparse_csr_tensor>,2.0.0+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97432,2023-03-23T09:16:05Z,suspicious memory leak when increase DataLoader's prefetch_factor and enable pin_memory,YES,<torch.cudaHostAlloc>,1.13.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97329,2023-03-22T08:33:52Z,"torch.ops.aten.pow(2.0, 3) return unexpected value with complex type",YES,torch.ops.aten.pow,torch==2.1.0.dev20230320+cpu,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/97286,2023-03-21T21:39:08Z,`jacrev` and `jacfwd` raise an error that `Sparse CSR tensors do not have strides`,YES,"torch.jacrev
torch.jacfwd",2.1.0.dev20230311+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97283,2023-03-21T20:52:00Z,`jacfwd` fails when computing the gradient for `channels_last` tensor,YES,<torch.func.jacfwd>,2.1.0.dev20230311+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97189,2023-03-20T21:57:52Z,torch.randint range for torch.int64 dtype seems wrong,YES,torch.randint,2.0.0+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97128,2023-03-20T06:01:36Z,"MultiHeadAttention, fast path broken with `bias=False` or uneven number of heads",YES,torch.nn.MultiheadAttention,<2.0.0>,11.8
pytorch,https://github.com/pytorch/pytorch/issues/97111,2023-03-19T16:18:44Z,TransformerEncoder truncates output when some token positions are masked by `src_key_padding_mask` across batch,YES,torch.nn.TransformerEncoder,1.13.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97097,2023-03-18T20:42:07Z,torch.cuda.FloatTensor().normal_() generate (partially) different sample on different gpu machines,YES,torch.cuda.FloatTensor().normal_(),2.0.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97088,2023-03-18T11:01:11Z,A Segment Fault can be triggered in torch.embedding,YES,<torch.embedding>,2.1.0.dev20230307+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97087,2023-03-18T10:58:24Z,A Segment Fault can be triggered in torch.adjoint,YES,<torch.adjoint>,2.1.0.dev20230307+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/97030,2023-03-17T16:21:33Z,DDP static graph fails for static model,YES,<torch.nn.parallel.DistributedDataParallel>,<Library Version> 1.11.0,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version based on the given information."
pytorch,https://github.com/pytorch/pytorch/issues/97004,2023-03-17T06:11:36Z,Small learning rate with `capturable=True` causes Adam optimizer to blow up model parameters.,YES,torch.optim.Adam,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/96789,2023-03-15T00:30:22Z,`cumprod` triggers INTERNAL ASSERT FAILED when `out` is a tensor on cuda but input is on cpu,YES,<torch.cumprod>,2.0.0.dev20230105,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96779,2023-03-14T21:36:12Z, Segmentation fault (core dumped) during Torch finetuning (at random step),YES,<torch.enable_grad>,2.1.0.dev20230313+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96769,2023-03-14T20:15:43Z,`sparse.mm` triggers INTERNAL ASSERT FAILED when backwarding,YES,<torch.sparse.mm>,2.0.0.dev20230105,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96704,2023-03-14T00:31:19Z,`logical_xx` operations trigger INTERNAL ASSERT FAIL when `input` is complex tensor on cuda and `other` is on cpu,YES,"torch.logical_and  
torch.logical_or  
torch.logical_xor  ",2.0.0.dev20230105,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96693,2023-03-13T21:46:59Z,"torch.compile mode=""max-autotune"" precision appears to be lower",YES,torch.compile,2.1.0.dev20230307,11.8
pytorch,https://github.com/pytorch/pytorch/issues/96579,2023-03-11T00:54:04Z,Wrong return type from operation on custom tensor inside registered hook ,YES,<torch.Tensor>,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/96448,2023-03-09T21:33:59Z,Unable to move torch.jit.load-ed models to XLA devices,YES,"torch.jit.load  
torch.jit.save  
torch.nn.Module.to  ",pt-1.13,"The description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version based on the provided information."
pytorch,https://github.com/pytorch/pytorch/issues/96428,2023-03-09T17:05:36Z,Compressed sparse constructor allows mixed `int32/int64` indices which leads to dtype promotion/demotion in conversions.,YES,torch.sparse_csr_tensor,Current master,"The issue description does not mention a specific CUDA version. Therefore, I cannot provide a CUDA version as it is not referenced in the text."
pytorch,https://github.com/pytorch/pytorch/issues/96379,2023-03-09T01:29:11Z,"`dynamo.export` ""input not consistent with traced input"" error when input default value type is `torch.Tensor`.",YES,torch._dynamo.export,master,"The issue description does not mention a specific CUDA version. Therefore, the answer is:

<No CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/96316,2023-03-08T18:29:32Z,`FractionalMaxPool3d` INTERNAL ASSERT FAILED when computing `jacrev`,YES,"<torch.nn.FractionalMaxPool3d>
<torch.func.jacrev>",2.0.0.dev20230105,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96277,2023-03-08T06:10:16Z,A Segment Fault can be triggered in torch.adaptive_max_pool1d with an edge case,YES,<torch.adaptive_max_pool1d>,2.1.0.dev20230307+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96276,2023-03-08T06:06:12Z,A Segment Fault can be triggered in torch.geqrf with an edge case,YES,<torch.geqrf>,2.1.0.dev20230307+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96275,2023-03-08T05:59:47Z,A Segment Fault can be triggered in torch.pinverse,YES,<torch.pinverse>,2.1.0.dev20230307+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/96265,2023-03-08T02:17:45Z,RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation,YES,<torch.autograd>,1.11.0+cu115,11.5
pytorch,https://github.com/pytorch/pytorch/issues/96187,2023-03-07T10:40:44Z,`gradgradcheck` does not work with sparse inputs.,YES,"<torch.autograd.gradgradcheck>  
<torch.autograd.gradcheck>  
<torch.autograd.gradcheck>  ",Current master.,"The issue description does not explicitly mention a CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/96110,2023-03-06T17:42:50Z,torch.where behaves differently from in place replacement,YES,torch.where,1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/95785,2023-03-01T15:38:37Z,Fully quantized model (`torch.quantization.convert`) produces incorrect output compared to analytical solution,YES,<torch.quantization.convert>,1.13.0a0+gitfdf4b6e,10.8
pytorch,https://github.com/pytorch/pytorch/issues/95776,2023-03-01T12:32:33Z,`torch.Tensor.is_set_to` raises `NotImplementedError` when inputs contain sparse tensor ,YES,torch.Tensor.is_set_to,"1.12.0, 1.13.0, 1.13.1","11.3.1, 11.6"
pytorch,https://github.com/pytorch/pytorch/issues/95768,2023-03-01T07:23:52Z,Inconsistent behaviour of torch.all(),YES,<torch.all>,1.13.1+cu116,11.6
pytorch,https://github.com/pytorch/pytorch/issues/95756,2023-03-01T02:51:34Z,`torch.nanmedian` return a negative value when input is empty ,YES,"torch.nanmedian
torch.median","1.12.0, 1.13.0, 1.13.1","11.3.1, 11.6"
pytorch,https://github.com/pytorch/pytorch/issues/95724,2023-02-28T19:15:01Z,"distributed training: lots of ""Exception ignored"" at the end of each epoch",YES,"torch.distributed.init_process_group  
torch.utils.data.distributed.DistributedSampler  
torch.distributed.barrier  
torch.multiprocessing.spawn",1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/95718,2023-02-28T18:13:05Z,functorch.compile.memory_efficient_fusion errors with: RuntimeError: forward() Expected a value of type 'Tensor (inferred)' for argument 'primals_356' but instead found type 'int'. ,YES,"<torch.ops.aten.add>
<torch.ops.aten.view>",1.13.1+cu116,11.6
pytorch,https://github.com/pytorch/pytorch/issues/95604,2023-02-27T09:22:52Z,"Performance bugs exists in multiple convolution operations(e.g., `Convtranspose2d`) when useing the `groups` argument",YES,"<torch.nn.ConvTranspose2d>
<torch.nn.LazyConv2d>
<torch.nn.LazyConvTranspose2d>",1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/95590,2023-02-27T03:31:48Z,Confusing error messages from `torch.nn.LazyLinear` in different versions.,YES,"torch.nn.LazyLinear
torch.nn.LSTMCell",1.12.0,"11.3.1
11.6"
pytorch,https://github.com/pytorch/pytorch/issues/95548,2023-02-25T07:11:02Z," torch.distributions.kumaraswamy.Kumaraswamy generates samples outside its support (0,1)",YES,torch.distributions.kumaraswamy.Kumaraswamy,1.13.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/95463,2023-02-24T10:45:14Z,`add/add_` for CSC: errors when trying to access non-existent `crow_indices`.,YES,"torch.add  
torch.add_  
torch.to_sparse_csc",Current master,"The issue description does not mention a specific CUDA version. Therefore, the response is:

<No CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/95460,2023-02-24T09:04:00Z,torch.profiler.tensorboard_trace_handler Generates an incorrect JSON file,YES,torch.profiler.tensorboard_trace_handler,1.12.1,cuda112
pytorch,https://github.com/pytorch/pytorch/issues/95434,2023-02-24T01:50:57Z,It seems that `torch.Tensor.addmv` and `torch.Tensor.addr` will check some inputs' dtype if and only if in `backward()`,YES,"<torch.Tensor.addr>
<torch.Tensor.addmv>",<pytorch 1.13.1>,11.6
pytorch,https://github.com/pytorch/pytorch/issues/95432,2023-02-24T01:47:35Z,Regression bug in `torch.nn.ReLU6` and `torch.nn.Hardtanh` that `inplace=True` doesn't work in PyTorch 1.10.0~1.13.1,YES,"torch.nn.ReLU6
torch.nn.Hardtanh",1.10.0 to 1.13.1,11.1
pytorch,https://github.com/pytorch/pytorch/issues/95309,2023-02-22T19:34:45Z,`torch.distributed.Store` triggers INTERNAL ASSER FAILED when seting,YES,torch.distributed.Store,2.0.0.dev20230105,11.7
pytorch,https://github.com/pytorch/pytorch/issues/95116,2023-02-18T11:48:49Z,"When using `ceil_mode=True`, `torch.nn.AvgPool1d` could get negative shape.",YES,"torch.nn.AvgPool1d
torch.nn.MaxPool","1.9.0, 1.10.0, 1.11.0, 1.12.0, 1.13.0","11.1, 11.3, 11.6"
pytorch,https://github.com/pytorch/pytorch/issues/95108,2023-02-18T01:15:23Z,`torch.nn.LazyLinear` crash when using torch.bfloat16 dtype in pytorch 1.12.0 and 1.13.0,YES,"torch.nn.LazyLinear  
torch.bfloat16  
torch.nn.Linear  
torch._C._nn.linear  ","1.12.0, 1.13.0","11.3, 11.6"
pytorch,https://github.com/pytorch/pytorch/issues/94990,2023-02-16T17:28:50Z,interactions between views + autograd.Function + AOTAutograd causes memory leak,YES,"torch.autograd.Function  
torch.ops.test.foo_fwd  
torch.ops.test.foo_bwd  
torch.compile  
torch._dynamo.graph_break  ",<2.0.0>,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version as it is not referenced in the text."
pytorch,https://github.com/pytorch/pytorch/issues/94966,2023-02-16T05:14:28Z,CSR matrix add_ error with RuntimeError: CUDA error: kernel launch failure when calling cusparseXcsrgeam2Nnz,YES,"torch.optim.sgd._single_tensor_sgd  
torch.sparse_coo_tensor.add_  ",2.0.0.dev20230213+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94855,2023-02-14T21:47:26Z,torch.compile breaks reproducibility,YES,<torch.compile>,2.0.0.dev20230213+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94808,2023-02-14T09:09:10Z,Memory Corruption in torch.lstm caused by edge cases,YES,<torch.lstm>,2.0.0.dev20230210+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/94773,2023-02-13T22:24:56Z,`torch.compile` doesn't consider the alias tensor created by `tensor[:]`,YES,<torch.compile>,2.0.0.dev20230213,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94704,2023-02-12T16:32:30Z,`where` triggers INTERNAL ASSERT FAILED when `out` is a long tensor due to mixed types,YES,torch.where,2.0.0.dev20230105,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94698,2023-02-12T06:25:54Z,A segment fault can be triggered in torch.avg_pool1d,YES,torch.avg_pool1d,2.0.0.dev20230210+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/94696,2023-02-12T06:10:39Z,A segment fault can be triggered in torch.max_pool1d_with_indices,YES,<torch.max_pool1d_with_indices>,2.0.0.dev20230210+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/94669,2023-02-11T08:51:26Z,A segment fault can be triggered in torch.svd,YES,<torch.svd>,2.0.0.dev20230210+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/94668,2023-02-11T08:48:00Z,A segment fault can be triggered in torch.lstm with edge cases,YES,torch.lstm,2.0.0.dev20230210+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/94594,2023-02-10T13:50:03Z,A segment fault can be triggered in torch.histogramdd,YES,<torch.histogramdd>,2.0.0.dev20230210+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/94593,2023-02-10T13:46:52Z,Memory corruptions can be triggered in torch._remove_batch_dim,YES,<torch._remove_batch_dim>,2.0.0.dev20230210+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/94591,2023-02-10T13:37:37Z,Issue with `upsample_nearest2d` decomposition,YES,<torch._C._nn.upsample_nearest2d>,2.0.0a0+gitf58ba55,11.6
pytorch,https://github.com/pytorch/pytorch/issues/94590,2023-02-10T13:28:31Z,A Segment Fault can be triggered in torch.affine_grid_generator,YES,<torch.affine_grid_generator>,2.0.0.dev20230210+cu118,11.8
pytorch,https://github.com/pytorch/pytorch/issues/94542,2023-02-09T21:02:31Z,torch.lgamma CUDA driver error,YES,torch.lgamma,1.13.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94511,2023-02-09T14:35:11Z,"Performance does not meet expectations when training OPT-30 with FSDP, there may be problems with cpu offloading",YES,"<torch.distributed.fsdp.FullyShardedDataParallel>
<torch.distributed.fsdp.CPUOffload>",torch==1.13.1+cu116,CUDA version: 11.6
pytorch,https://github.com/pytorch/pytorch/issues/94451,2023-02-08T23:05:47Z,"`jacrev` raise ""Cannot access storage of TensorWrapper"" error when computing the grad of `storage`",YES,<torch.func.jacrev>,2.0.0.dev20230105,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94443,2023-02-08T22:12:06Z,A better error msg for `cuda.jiterator` when input is on `cpu`,YES,torch.cuda.jiterator,2.0.0.dev20230105,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94428,2023-02-08T20:00:39Z,nn.TransformerEncoderLayer fastpath (BetterTransformer) is much slower with src_key_padding_mask,YES,"<torch.nn.TransformerEncoder>
<torch.nn.TransformerEncoderLayer>",1.13.1+cu116,11.6
pytorch,https://github.com/pytorch/pytorch/issues/94397,2023-02-08T14:11:22Z,jacfwd and jacrev are fundamentally broken for complex inputs,YES,"<torch.jacfwd>
<torch.jacrev>",master,"The issue description does not mention a specific CUDA version. Therefore, the response is:

<CUDA version>"
pytorch,https://github.com/pytorch/pytorch/issues/94388,2023-02-08T10:04:50Z,Inconsistent results when using torch.Tensor.bernoulli with float instead of Tensor probabilities,YES,<torch.Tensor.bernoulli>,1.13.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94371,2023-02-08T04:45:22Z,"QAT + torch.autocast does not work with default settings, missing fused fake_quant support for half",YES,"<torch.autocast>
<torch.ao.quantization.get_default_qat_qconfig_mapping>
<torch.ao.quantization.quantize_fx.prepare_fx>
<torch.fused_moving_avg_obs_fake_quant>",master,"The issue description does not explicitly mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/94336,2023-02-07T20:49:15Z,`scatter` fails the gradient computation in reverse mode for `src` when `index` is empty,YES,<torch.scatter>,2.0.0.dev20230105,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94333,2023-02-07T20:29:00Z,cpu log1p for bfloat16 gives wrong result.,YES,torch.log1p,2.0.0a0+git59c1b50,12.0
pytorch,https://github.com/pytorch/pytorch/issues/94304,2023-02-07T12:35:54Z,"RuntimeError: p.block != nullptr && p.block->ptr != nullptr INTERNAL ASSERT FAILED at ""../c10/cuda/CUDACachingAllocator.cpp"":1275, please report a bug to PyTorch.",YES,torch.load,pytorch 1.12.0+cu113,cu113
pytorch,https://github.com/pytorch/pytorch/issues/94208,2023-02-06T19:30:11Z,`zeros_like` + `fill_` makes the gradient computation in forward mode fail,YES,<torch.zeros_like>,2.0.0.dev20230105,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94186,2023-02-06T15:19:42Z,Addition of hybrid CSR tensors produces incorrect and invalid CSR tensor,YES,torch.sparse_csr,master,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/94183,2023-02-06T15:01:12Z,Addition of batch CSR tensors produces incorrect and invalid CSR tensor,YES,torch._validate_sparse_csr_tensor_args,master,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/94125,2023-02-04T04:45:08Z,A Floating Point Exception can be trigerred in torch._C._nn.slow_conv3d,YES,torch._C._nn.slow_conv3d,1.13.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94115,2023-02-04T01:58:14Z,`cat` fails the gradient computation in forward mode with empty tensors when used with legacy vmap,YES,torch.cat,2.0.0.dev20230105,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94111,2023-02-04T01:32:54Z,`svd` triggers INTERNAL ASSERT FAILED when computing jacobian in forward mode,YES,torch.svd,2.0.0.dev20230105,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94086,2023-02-03T23:06:55Z,`MSELoss` fails to compute the gradients when inputs have different dtype,YES,<torch.nn.MSELoss>,2.0.0.dev20230105,11.7
pytorch,https://github.com/pytorch/pytorch/issues/94085,2023-02-03T22:57:22Z,`unfold` fails in forward mode when unfolding a scalar tensor,YES,<torch.unfold>,<2.0.0.dev20230105>,11.7
pytorch,https://github.com/pytorch/pytorch/issues/93982,2023-02-02T19:44:24Z,`linalg.lstsq` fails the gradient computation in forward mode,YES,torch.linalg.lstsq,2.0.0.dev20230105,11.7
pytorch,https://github.com/pytorch/pytorch/issues/93846,2023-02-01T13:43:01Z,large number of temporary files generated when using dataloader with num_workers>0,YES,torch.utils.data.DataLoader,1.10.1+cu111,11.1
pytorch,https://github.com/pytorch/pytorch/issues/93826,2023-02-01T06:51:35Z,torch.jit.script does not work with DataParallel,YES,"<torch.jit.script>  
<torch.nn.parallel.data_parallel>",1.13.1,12.0
pytorch,https://github.com/pytorch/pytorch/issues/93275,2023-01-30T16:05:48Z,Bug in torch.linalg.svd ,YES,<torch.linalg.svd>,1.12.0+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/92912,2023-01-24T19:41:54Z,functorch.functionalize doesn't error out with logcumsumexp.out,YES,torch.logcumsumexp,Nightly,"The issue description does not mention a specific CUDA version. Therefore, the response is:

<None>"
pytorch,https://github.com/pytorch/pytorch/issues/92910,2023-01-24T18:24:24Z,torch.jit.save() generates different contents in a file among different endian machines,YES,"torch.jit.save  
torch.jit.load",2.0.0a0+git215f4fc,None
pytorch,https://github.com/pytorch/pytorch/issues/92866,2023-01-23T23:11:50Z,JIT Function Fails when run a second time,YES,<torch.jit.script>,1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/92835,2023-01-23T18:19:41Z,Double free when running torch.linalg.ldl_solve,YES,torch.linalg.ldl_solve,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/92828,2023-01-23T17:37:44Z,segfault when running torch.igamma,YES,torch.igamma,1.12.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/92818,2023-01-23T16:33:34Z,Segfault when running torch.atan2,YES,torch.atan2,1.13.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/92812,2023-01-23T12:35:23Z,"torch.fx fails to trace through ""+"" op between torch.Size and torch.fx.proxy.Proxy",YES,"torch.fx.symbolic_trace  
torch.fx.proxy.Proxy  
torch.Size",1.13.1+cu116,11.6
pytorch,https://github.com/pytorch/pytorch/issues/92811,2023-01-23T11:55:54Z,[complex] Jacobian of a non-holomorphic complex valued function,YES,torch.conj,master,"The issue description does not mention any specific CUDA version. Therefore, I cannot provide a CUDA version from the information given."
pytorch,https://github.com/pytorch/pytorch/issues/92804,2023-01-23T06:27:18Z,Cuda streams and torch.compile,YES,torch._dynamo.optimize,2.0.0.dev20230122+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/92794,2023-01-23T03:01:16Z,Segmentation fault when running torch.ge,YES,torch.ge,1.12.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/92783,2023-01-23T00:57:32Z,Process get killed when running torch.combinations,YES,torch.combinations,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/92781,2023-01-23T00:31:23Z,Floating point exception when running torch.nn.AdaptiveMaxPool3d,YES,<torch.nn.AdaptiveMaxPool3d>,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/92778,2023-01-22T23:41:36Z,Process get killed when running torch.normal,YES,torch.normal,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/92776,2023-01-22T23:33:45Z,segfault when running torch.lu_unpack,YES,torch.lu_unpack,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/92742,2023-01-21T03:55:09Z,[JIT] Consecutive use of `addmm` Leads to Exception,YES,torch.addmm,2.0.0.dev20230119+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/92740,2023-01-21T03:41:21Z,[JIT] Applying `conv2d` over Constants Leads to Exception,YES,"torch.nn.functional.relu
torch.nn.Conv2d
aten::conv2d",2.0.0.dev20230119+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/93515,2023-01-21T01:16:11Z,Dynamo can not trace 'int(a_scalar_tensor.item())',YES,<torch._dynamo.export>,<Library Version> 2.0.0,"The description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/92674,2023-01-20T08:15:59Z,[JIT] `Linear` + `BatchNorm2d` Trigger Inconsistency between Eager Mode and JIT,YES,"torch.jit.optimize_for_inference  
torch.nn.Linear  
torch.nn.BatchNorm2d  ",2.0.0.dev20230119+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/92594,2023-01-18T23:25:40Z,INTERNAL ASSERT FAILED when mixed dtypes for `addcmul_`,YES,<torch.addcmul_>,2.0.0.dev20230105,11.7
pytorch,https://github.com/pytorch/pytorch/issues/92563,2023-01-18T18:46:56Z,[JIT] INTERNAL ASSERT FAILED when `Conv2d` and `clamp` used together,YES,"torch.nn.Conv2d  
torch.clamp  
torch.jit.optimize_for_inference  ",1.14.0a0+gitbdc9911,11.6
pytorch,https://github.com/pytorch/pytorch/issues/92252,2023-01-16T15:01:17Z,"`model.to(""cuda:0"")` does not release all CPU memory",YES,<torch.to>,1.13.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/92251,2023-01-16T14:52:21Z,"`torch.load(..., map_location=""cuda:0"")` allocates memory on both CPU and GPU",YES,torch.load,1.13.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/92250,2023-01-16T14:27:19Z,torch.cuda.is_available() returns True even if the CUDA hardware can't run pytorch,YES,<torch.cuda.is_available>,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/92171,2023-01-13T19:50:19Z,PyTorch 1.13.1 hangs with `torch.distributed.init_process_group`,YES,<torch.distributed.init_process_group>,1.13.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/92141,2023-01-13T09:42:19Z,Improve the error message in `linalg.eigh` when using large matrices,YES,torch.linalg.eigh,1.13.0,"There is no CUDA version mentioned in the issue description. The build settings indicate `USE_CUDA=0`, which means that CUDA is not being used in this case."
pytorch,https://github.com/pytorch/pytorch/issues/92128,2023-01-13T02:36:29Z, Forward arguments are not updated in DDP,YES,<torch.nn.parallel.DistributedDataParallel>,1.9.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/91879,2023-01-09T12:01:48Z,ddp vs fsdp,YES,"torch.ddp  
torch.fsdp  
torch.amp  
torch.cuda  
torch.bfloat16  
torch.cuda.current_device  ",<Library Version> 1.13.0,"The CUDA version mentioned in the issue description is not explicitly stated. Therefore, I cannot provide a specific CUDA version based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/91863,2023-01-08T23:31:55Z,torch.Categorical samples indexes with 0 probability when given logits as argument,YES,torch.distributions.Categorical,2.0.0.dev20230105,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/91760,2023-01-05T13:15:35Z,Inconsistent rank among torch.distributed primitives,YES,<torch.distributed.gather>,ad782ff7df950dfda64e271dd4ee3c6128971103,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/91716,2023-01-04T19:01:31Z,linspace (and arange) behaves differently on GPU and CPU,YES,"<torch.linspace>
<torch.arange>",1.13.0+cu116,11.6
pytorch,https://github.com/pytorch/pytorch/issues/91633,2023-01-03T15:47:18Z,Segmentation fault when running torch.nn.functional.fractional_max_pool3d on torch 1.13.1,YES,torch.nn.functional.fractional_max_pool3d,1.13.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/91629,2023-01-02T20:10:33Z,vmap + nn.SyncBatchNorm.convert_sync_batchnorm,YES,"<torch.vmap>
<torch.nn.SyncBatchNorm.convert_sync_batchnorm>",<1.13.0>,"The issue description does not mention any specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/91593,2023-01-02T18:03:06Z,Division by zero error when running torch.nn.functional.lp_pool1d,YES,<torch.nn.functional.lp_pool1d>,1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/91557,2022-12-31T16:20:43Z,Segmentation fault when running torch.nn.AdaptiveMaxPool3d,YES,<torch.nn.AdaptiveMaxPool3d>,1.8.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/91556,2022-12-31T16:11:21Z,Overflow when running torch.nn.AdaptiveMaxPool3d on torch 1.12.0 and 1.13.1,YES,torch.nn.AdaptiveMaxPool3d,1.13.0+cu116,11.6
pytorch,https://github.com/pytorch/pytorch/issues/91553,2022-12-31T15:03:20Z,Segmentation fault when running torch.nn.AdaptiveMaxPool2d,YES,torch.nn.AdaptiveMaxPool2d,1.8.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/91552,2022-12-31T14:58:08Z,Overflow when running torch.nn.AdaptiveMaxPool2d,YES,<torch.nn.AdaptiveMaxPool2d>,1.12.0+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/91535,2022-12-30T10:24:23Z,"When dist.broadcast float32 to int64, it will silently generate wrong results",YES,<torch.distributed.broadcast>,1.10.0,11.3.1
pytorch,https://github.com/pytorch/pytorch/issues/91533,2022-12-30T08:02:35Z,Cannot cast float64 to float32,YES,<torch.nn.Linear>,1.12.1,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/91396,2022-12-27T03:42:21Z,[JIT] Zero-channel conv2d cannot be applied with `optimize_for_inference`,YES,torch.jit.optimize_for_inference,1.14.0.dev20221202+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/91375,2022-12-24T21:03:33Z,Internal Assert failed,YES,<torch.cuda.CUDAGraphsC10Utils>,1.13.0+cu116,11.6
pytorch,https://github.com/pytorch/pytorch/issues/93493,2022-12-21T21:21:05Z,tensor.to_sparse() handling indices incorrectly under dynamo/fake tensor,YES,torch.Tensor.to_sparse,1.13.0,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/91245,2022-12-21T13:37:52Z,Segmentation faults in DataLoader (in latest torch version).,YES,torch.utils.data.DataLoader,1.13.1+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/91156,2022-12-20T06:12:31Z,`quantile` fails for `float16`/`half` inputs,YES,<torch.quantile>,1.13.0+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/90999,2022-12-16T14:49:24Z,overflow (?) on cuda tensor after matrix multiplication,YES,"torch.clone  
torch.cuda  
torch.rand  
torch.float32  
torch.matmul  
torch.unsqueeze  
torch.argmin  
torch.min  
torch.argmax  
torch.max  
torch.transpose  ",1.12.1+cu116,cu116
pytorch,https://github.com/pytorch/pytorch/issues/90998,2022-12-16T14:09:27Z,"Crash in `index_select` with singleton `self`, non-singleton `index`",YES,torch.index_select,1.13.0+cpu,"CUDA used to build PyTorch: Could not collect  
CUDA runtime version: Could not collect  "
pytorch,https://github.com/pytorch/pytorch/issues/90992,2022-12-16T08:51:29Z,as_strided_scatter : INTERNAL_ASSERT_FAILED for requires_grad=True and non-config input,YES,<torch.as_strided_scatter>,master,The description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/90916,2022-12-15T12:49:35Z,Functorch does not work with CrossEntropyLoss and label=-100,YES,torch.nn.CrossEntropyLoss,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/90842,2022-12-14T15:12:28Z,nn.MultiheadAttention softmax inconsistent in training mode ,YES,torch.nn.MultiheadAttention,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/90793,2022-12-13T20:58:39Z,nn.CrossEntropyLoss error out when the sample size is large,YES,<torch.nn.CrossEntropyLoss>,1.12.1+cu113,11.3
pytorch,https://github.com/pytorch/pytorch/issues/90760,2022-12-13T12:26:43Z,Large slow down by not calling `torch.set_num_threads`,YES,<torch.set_num_threads>,1.12.1,None
pytorch,https://github.com/pytorch/pytorch/issues/90752,2022-12-13T08:32:36Z,Adam (fused=True) issues,YES,torch.optim.Adam,pytorch 1.13,"The issue description does not explicitly mention a CUDA version. Therefore, based on the provided text, it is not possible to identify a specific CUDA version."
pytorch,https://github.com/pytorch/pytorch/issues/90695,2022-12-12T14:43:26Z,`torch.empty` produces incorrect tensors with `layout=sparse_csr|sparse_csc` on the CPU,YES,torch.empty,Current master.,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/90613,2022-12-10T14:35:50Z,`torch.inverse` multi-threading RuntimeError: lazy wrapper should be called at most once,YES,<torch.inverse>,1.13.0+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/90367,2022-12-07T08:23:55Z,[JIT] INTERNAL ASSERT FAILED `torch.add` with boolean primitive constant,YES,torch.add,1.14.0.dev20221202+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/90366,2022-12-07T08:16:55Z,[JIT] INTERNAL ASSERT FAILED `torch.mul` with boolean primitive constant,YES,torch.mul,1.14.0.dev20221202+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/90365,2022-12-07T07:57:54Z,[JIT] INTERNAL ASSERT FAILED when dispatching for `torch.Tensor.view`,YES,<torch.Tensor.view>,1.14.0.dev20221202+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/90261,2022-12-06T06:14:55Z,Why torch.mode return different value between CPU and GPU,YES,torch.mode,pytorch v1.10.2 with CUDA10.2,CUDA10.2
pytorch,https://github.com/pytorch/pytorch/issues/90245,2022-12-06T01:53:56Z,[Distributed] `Invalid scalar type` when `dist.scatter()` boolean tensor,YES,"<torch.distributed.scatter>
<torch.distributed.all_reduce>",1.13.0+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/90138,2022-12-04T16:14:51Z,p.block != nullptr && p.block->ptr != nullptr INTERNAL ASSERT FAILED,YES,<torch.nn.modules.module>,pytorch1.8+cu11,11
pytorch,https://github.com/pytorch/pytorch/issues/90057,2022-12-02T13:12:54Z,[GradScaler] Inconsistent scale values across different GPUs caused by uneven inputs for AMP DDP training,YES,torch.cuda.amp.GradScaler,pytorch 1.12.1,"The issue description does not explicitly mention a specific CUDA version. However, it is referenced that the user is using ""2 * A100"" GPUs, which typically implies compatibility with CUDA 11.x or higher, as A100 GPUs require CUDA 11.0 or later for optimal performance.

Therefore, the CUDA version referenced is:

CUDA 11.x"
pytorch,https://github.com/pytorch/pytorch/issues/89757,2022-11-28T05:33:34Z,third-order gradient of torch.pow with tensor args and certain input returns NaN,YES,torch.pow,1.13.0+cu117,cu117
pytorch,https://github.com/pytorch/pytorch/issues/89718,2022-11-27T09:54:16Z,`torch.Tensor.flatten` Trigger Segmentation Fault when trying to provide and output named dim,YES,torch.Tensor.flatten,1.14.0a0+gitbdc9911,11.6
pytorch,https://github.com/pytorch/pytorch/issues/89716,2022-11-27T06:08:57Z,DDP hangs on forward pass of transformer,YES,"<torch.distributed.init_process_group>
<torch.nn.parallel.DistributedDataParallel>",1.12.1+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/89686,2022-11-25T14:51:19Z,MultiProcess tests fail when run on nodes with 1 GPU,YES,torch.distributed.barrier,1.12.1,The CUDA version mentioned in the issue description is 2.10.3.
pytorch,https://github.com/pytorch/pytorch/issues/89684,2022-11-25T13:47:59Z,`positive_semidefinite` constraint fails on CUDA 11.7,YES,"torch.distributions.constraints.positive_semidefinite  
torch.linalg.eigvalsh",PyTorch 1.12.1,CUDA 11.7
pytorch,https://github.com/pytorch/pytorch/issues/89675,2022-11-25T05:22:27Z,[ONNX] torch.onnx.export snapshots the grads as constants in onnx when op is in cuda device,YES,"<torch.onnx.export>
<torch.autograd.grad>",1.13.0,11.7
pytorch,https://github.com/pytorch/pytorch/issues/89673,2022-11-25T02:24:49Z,MPS bug on `torch.transpose` and `torch.log`,YES,"torch.transpose  
torch.log",1.13.0a0+git7c98e70,None
pytorch,https://github.com/pytorch/pytorch/issues/89634,2022-11-24T14:34:12Z,[ONNX] torch.onnx.export can not export the grad of conv when the op is in CPU  ,YES,torch.onnx.export,1.13.0+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/89630,2022-11-24T14:19:49Z,[dynamo] RuntimeError: Failed running call_function aten.nll_loss_backward(*(FakeTensor(FakeTensor(...,YES,torch.ops.aten.nll_loss_backward,1.14.0.dev20221114+cpu,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/89629,2022-11-24T14:15:32Z,[dynamo] RuntimeError: Failed running call_function aten.convolution_backward(*(FakeTensor(FakeTensor(..,YES,<torch.ops.aten.convolution_backward>,1.14.0.dev20221114+cpu,None
pytorch,https://github.com/pytorch/pytorch/issues/89275,2022-11-18T08:00:56Z,"torch.addcdiv: input, tensor1, and tensor2 parameters should be of the same type",YES,<torch.addcdiv>,1.9.0,"The issue description provided does not mention any specific CUDA version. Therefore, it is not possible to identify a CUDA version from the given text."
pytorch,https://github.com/pytorch/pytorch/issues/89254,2022-11-18T01:00:59Z,"`torch.nn.ReplicationPad2D` Report ""invalid configuration argument"" Error under Compute Sanitizer",YES,torch.nn.ReplicationPad2d,1.14.0a0+gitbdc9911,11.6
pytorch,https://github.com/pytorch/pytorch/issues/89218,2022-11-17T16:57:43Z,"`torch.nn.LayerNorm` Abort with ""invalid device ordinal"" Error",YES,torch.nn.LayerNorm,1.14.0a0+gitbdc9911,11.6
pytorch,https://github.com/pytorch/pytorch/issues/89197,2022-11-17T07:12:52Z,Collective operations do not work with `torch.BoolTensor`s on `gloo` and raise `Invalid scalar type`,YES,"<torch.distributed.gather>
<torch.distributed.all_gather>
<torch.distributed.reduce>
<torch.distributed.all_reduce>
<torch.distributed.broadcast>",1.13.0.dev20220927+cpu,"The issue description does not mention a specific CUDA version. It states ""CUDA used to build PyTorch: Could not collect"" and ""CUDA runtime version: Could not collect,"" indicating that the CUDA version information is not available. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/89125,2022-11-16T09:48:15Z,binary_cross_entropy/bce_with_logits (+ other loss functions) for nested_tensor,YES,torch.nn.functional.binary_cross_entropy,1.9.0,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/89080,2022-11-15T19:15:55Z,Unable to backprop through dense weighted sum of sparse_coo_tensors,YES,<torch.sparse.sum>,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/88950,2022-11-12T16:54:50Z,`torch.nn.functional.embedding_bag` Trigger RuntimeError under UndefinedBehaviorSanitizer,YES,torch.nn.functional.embedding_bag,1.14.0a0+git6e5f736,11.8.89
pytorch,https://github.com/pytorch/pytorch/issues/88949,2022-11-12T16:51:50Z,`torch.set_rng_state` Trigger RuntimeError under UndefinedBehaviorSanitizer,YES,torch.set_rng_state,1.14.0a0+git6e5f736,11.8.89
pytorch,https://github.com/pytorch/pytorch/issues/88948,2022-11-12T16:48:35Z,torch.linalg.matrix_rank memory leak,YES,torch.linalg.matrix_rank,1.13.0a0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/88947,2022-11-12T16:48:12Z,`torch.Tensor.msort` Trigger RuntimeError under UndefinedBehaviorSanitizer,YES,torch.Tensor.msort,1.14.0a0+git6e5f736,11.8.89
pytorch,https://github.com/pytorch/pytorch/issues/88945,2022-11-12T16:45:29Z,`torch.linalg.eigvals` Trigger RuntimeError under UndefinedBehaviorSanitizer,YES,torch.linalg.eigvals,1.14.0a0+git6e5f736,11.3.1
pytorch,https://github.com/pytorch/pytorch/issues/88944,2022-11-12T16:42:23Z,`torch.topk` Trigger RuntimError under UndefinedBehaviorSanitizer,YES,<torch.topk>,1.14.0a0+git6e5f736,11.8.89
pytorch,https://github.com/pytorch/pytorch/issues/88943,2022-11-12T16:39:18Z,`torch.vander` Trigger RuntimeError with UndefinedBehaviorSanitizer,YES,torch.vander,1.14.0a0+git6e5f736,11.8.89
pytorch,https://github.com/pytorch/pytorch/issues/88942,2022-11-12T16:34:02Z,`torch.svd_lowrank` Trigger RuntimeError under UndefinedBehaviorSanitizer,YES,torch.svd_lowrank,1.14.0a0+git6e5f736,11.8.89
pytorch,https://github.com/pytorch/pytorch/issues/88941,2022-11-12T16:30:33Z,`torch.linalg.lstsq` Trigger RuntimeError under UndefinedBehaviorSanitizer,YES,<torch.linalg.lstsq>,1.14.0a0+git6e5f736,11.3.1
pytorch,https://github.com/pytorch/pytorch/issues/88648,2022-11-08T03:33:47Z,`MultiMarginLoss` doesn't check the value of `target` on CUDA,YES,<torch.nn.MultiMarginLoss>,1.12.1+cu113,11.3
pytorch,https://github.com/pytorch/pytorch/issues/88647,2022-11-08T03:22:27Z,`ConvTranspose` fails on CPU but returns an empty tensor on CUDA,YES,<torch.nn.functional.conv_transpose2d>,1.12.1+cu113,11.3
pytorch,https://github.com/pytorch/pytorch/issues/88643,2022-11-08T01:53:29Z,pack_sequence() always fail after set_default_tensor_type to CUDA,YES,<torch.nn.utils.rnn.pack_sequence>,1.13.0+cu117,11.7
pytorch,https://github.com/pytorch/pytorch/issues/88621,2022-11-07T23:48:03Z,"[FSDP] FSDP produces different gradient norms vs DDP, and w/ grad norm clipping creates different training results",YES,"torch.distributed.ReduceOp.SUM  
torch.linalg.vector_norm  
FSDP.clip_grad_norm_  
nn.utils.clip_grad_norm_  ",<pytorch version>,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/88598,2022-11-07T19:26:41Z,"Despite having aten::diag_embed.out, torch.diag_embed doesn't support out= argument",YES,torch.diag_embed,master,"The issue description does not mention a specific CUDA version. Therefore, I cannot provide a CUDA version as it is not referenced in the text."
pytorch,https://github.com/pytorch/pytorch/issues/88591,2022-11-07T18:05:08Z,`pack_padded_sequence` not compatible with deterministic mode it calls `torch.scatter`,YES,"torch.pack_padded_sequence
torch.scatter",1.14.0.dev20221107,11.6.112
pytorch,https://github.com/pytorch/pytorch/issues/88574,2022-11-07T13:34:59Z,Add support for `torch.Generator` in the FX IR,YES,torch.Generator,master,"The issue description does not mention any specific CUDA version. Therefore, there is no CUDA version to provide from the information given."
pytorch,https://github.com/pytorch/pytorch/issues/88563,2022-11-07T04:20:47Z,`nn.functional.embedding_bag` Trigger out-of-bound Read under Compute Sanitizer,YES,<torch.nn.functional.embedding_bag>,1.14.0a0+git81042d3,11.6
pytorch,https://github.com/pytorch/pytorch/issues/88491,2022-11-04T13:31:41Z,torch.set_grad_enabled results in RuntimeError with torch.jit.script,YES,"<torch.set_grad_enabled>
<torch.jit.script>",1.12.0+cu116,11.6
pytorch,https://github.com/pytorch/pytorch/issues/88474,2022-11-04T03:19:18Z,Mixed precision training fails due to NaN in batch norm running_mean,YES,<torch.cuda.amp.autocast>,<Library Version> 1.10.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/88448,2022-11-03T20:55:51Z,linear mm weight and bias dtypes mismatch bypasses,YES,"torch.addmm
torch.mm
torch.linear",master `2bda2baad787923b064c747e619e62a6af969940`,"The issue description does not explicitly mention a specific version of CUDA. Therefore, I cannot provide a CUDA version based solely on the information in the description."
pytorch,https://github.com/pytorch/pytorch/issues/88447,2022-11-03T20:30:11Z,`unique` will reverse the input when `sort=False` on cpu (not sorting),YES,torch.unique,1.13.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/93604,2022-11-02T18:28:13Z,TorchBench - moco - RuntimeError: Tensors must be CUDA and dense,YES,<torch.distributed.all_gather>,<torch 2.0.0>,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the information provided."
pytorch,https://github.com/pytorch/pytorch/issues/88191,2022-11-01T13:34:29Z,`torch.nn.RReLU` not reporting `lower > upper` on CUDA,YES,<torch.nn.RReLU>,1.12.1+cu113,11.3
pytorch,https://github.com/pytorch/pytorch/issues/88185,2022-11-01T09:27:57Z,`torch.mm` Trigger RuntimeError with UndefinedBehaviorSanitizer,YES,torch.mm,1.14.0a0+gita86278b,11.6.124
pytorch,https://github.com/pytorch/pytorch/issues/88147,2022-10-31T22:22:44Z,Conv2d is not deterministic when input tensor has different strides,YES,<torch.nn.Conv2d>,1.13.0+cu116,11.6
pytorch,https://github.com/pytorch/pytorch/issues/88144,2022-10-31T22:13:24Z,AvgPool2D output shapes are inconsistent when ceil_mode=True,YES,<torch.nn.AvgPool2d>,1.12.1+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/87992,2022-10-28T17:14:10Z,torch.rand(...) is not consistent for large shape dimensions across GPUs (with the same random seed),YES,torch.rand,1.12.1,"CUDA version: 10.2, 11.3"
pytorch,https://github.com/pytorch/pytorch/issues/87964,2022-10-28T05:45:27Z,`torch.distributed` crash with abort only inside if,YES,<torch.distributed.FileStore>,pytorch 1.12.1,"The issue description does not mention a specific CUDA version. Therefore, the response would be:

<No CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/87961,2022-10-28T05:33:59Z,crash in `torch.package.PackageExporter`,YES,torch.package.PackageExporter,1.12.1,The CUDA version is not explicitly mentioned in the provided issue description.
pytorch,https://github.com/pytorch/pytorch/issues/87960,2022-10-28T05:27:50Z,crash when call `torch.set_num_interop_threads` twice,YES,torch.set_num_interop_threads,1.12.1,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/87841,2022-10-27T04:11:29Z,`max_unpool3d` will trigger an assertion fail under compute sanitizer,YES,<torch.nn.functional.max_unpool3d>,1.14.0a0+gita86278b,11.6
pytorch,https://github.com/pytorch/pytorch/issues/87792,2022-10-26T16:14:52Z,`AvgPool` and `MaxPool` will crash in JIT w/o profiling executor,YES,"<torch.nn.AdaptiveMaxPool3d>
<torch.nn.AdaptiveAvgPool3d>",1.14.0.dev20221020,11.6
pytorch,https://github.com/pytorch/pytorch/issues/87787,2022-10-26T15:45:52Z,`BatchNorm` a 0-shape tensor will crash in JIT trace w/o profiling executor on cuda,YES,torch.nn.BatchNorm1d,1.14.0.dev20221020,11.6
pytorch,https://github.com/pytorch/pytorch/issues/87745,2022-10-26T00:48:00Z,TorchDynamo: there has a accuracy issue for conv+unary(binary) post ops for gpu path,YES,torch._inductor.codegen.triton,1.10.0,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/87556,2022-10-22T19:59:39Z,The behavior of cast `NaN` is different on cpu and cuda,YES,"torch.tensor  
torch.int32  
torch.clone  
torch.type  ",1.12.1,11.6
pytorch,https://github.com/pytorch/pytorch/issues/87433,2022-10-21T01:14:51Z,index_select() applied in sparse tensor can't backprop,YES,<torch.index_select>,1.12.1+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/87390,2022-10-20T19:08:02Z,`chunk` a 0-dim tensor will crash in JIT script w/o profiling executor,YES,torch.chunk,1.14.0.dev20221020,11.6
pytorch,https://github.com/pytorch/pytorch/issues/87276,2022-10-19T09:43:09Z,torch.save throws ValueError: ctypes objects containing pointers cannot be pickled,YES,torch.save,torch-1.11.0,"The issue description does not mention a specific CUDA version. Therefore, the response is:

<Cuda version not mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/87236,2022-10-18T20:01:58Z,nvprims.div doesn't work with FakeTensor cpu scalars,YES,torch.ops.nvprims.div,Latest master,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/87170,2022-10-18T04:51:53Z,torch.linalg.cond gives inconsistent results on CPU/CUDA,YES,torch.linalg.cond,torch 1.12.1+cu113,cu113
pytorch,https://github.com/pytorch/pytorch/issues/87131,2022-10-17T21:04:20Z,torch.clamp does not clamp out of -0 from 0 when ran on the CPU,YES,<torch.clamp>,torch 1.12.1,"The issue description does not explicitly mention a specific CUDA version. Therefore, the answer is:

<No CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/86918,2022-10-13T17:37:59Z,torch.cat on empty tensor is bogus,YES,torch.cat,master,"The description does not mention any specific CUDA version. Therefore, I cannot provide a CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/86849,2022-10-12T23:28:55Z,`torch.distributed.all_reduce` allocates excess GPU memory when using NCCL backend,YES,<torch.distributed.all_reduce>,1.12.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/86798,2022-10-12T15:29:52Z,TF32 conv_transpose2d with groups has bad precision compared to fp32,YES,<torch.nn.functional.conv_transpose2d>,nightly,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/86791,2022-10-12T13:24:15Z,We don't have an op for vulkan_prepack::conv2d_clamp_prepack but it isn't a special case.,YES,torch._C._jit_pass_vulkan_optimize_for_mobile,1.12.1+cu113,cu113
pytorch,https://github.com/pytorch/pytorch/issues/86597,2022-10-10T14:52:16Z,JIT returns different values for `cos + frac` on cpu,YES,"torch.cos  
torch.frac  
torch.jit.trace  
torch.sinh",1.13.0.dev20221001,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86595,2022-10-10T14:38:23Z,JIT model with `relu+div+sgn` will crash when computing the gradient,YES,"torch.nn.functional.relu  
torch.div  
torch.sgn  
torch.jit.trace  
torch.autograd.functional.jacobian",1.13.0.dev20221001,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86594,2022-10-10T14:35:21Z,JIT model with mean will crash when computing the gradients on cuda,YES,torch.mean,1.13.0.dev20221001,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86558,2022-10-09T19:20:05Z,Data conversion ops ignore `memory_format=torch.contiguous_format` ,YES,torch.bool,master (67434c70df5df353944f6ba876d9dd06b669bacd),"The issue description does not mention a specific CUDA version. Therefore, the answer is:

<No CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/86554,2022-10-09T14:30:04Z,[NvFuser] would change the output for some inaccurate dtype,YES,"torch.fmod  
torch.add  
torch.sub  
torch.jit.trace  
torch._C._jit_set_nvfuser_single_node_mode  ",1.13.0.dev20221001,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86553,2022-10-09T14:26:13Z,`topk` will return the wrong value and could read out-of-bound value after jit,YES,torch.topk,1.13.0.dev20221001,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86552,2022-10-09T14:07:16Z,`max_unpool` and `max_pool` will trigger INTERNAL ASSERT FAIL in JIT,YES,"torch.nn.functional.max_unpool1d
torch.nn.functional.max_pool",1.13.0.dev20221001,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86551,2022-10-09T14:03:45Z,`MultiLabelMarginLoss` will return incorrect values in JIT after the first run on cuda,YES,"<torch.nn.MultiLabelMarginLoss>
<torch.nn.functional.relu>
<torch.sin>
<torch.nn.functional.tanhshrink>
<torch.Tensor.resize_as_>",1.13.0.dev20221001,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86532,2022-10-08T14:41:03Z,Conv2d will crash by using `jit.trace`,YES,"torch.nn.functional.conv2d  
torch.jit.trace  ",1.13.0.dev20221001,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86529,2022-10-08T14:16:44Z,[NvFuser] JIT model with `mul+atan+sgn` will access illegal memory on cuda when computing gradient,YES,"torch.mul  
torch.atan  
torch.sgn  
torch.jit.script  
torch.random.manual_seed  
torch.empty  
torch.uniform_  
torch.autograd.functional.jacobian",1.13.0.dev20221001,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86465,2022-10-07T17:13:07Z,Wrong results with torch.linalg.inv on batched matrices when using cuda,YES,torch.linalg.inv,1.12.1,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86456,2022-10-07T10:50:28Z,`SyncBatchNorm` doesn't work with subclass of `torch.Tensor`,YES,<torch.nn.SyncBatchNorm>,1.13.0a0+d321be6,11.7
pytorch,https://github.com/pytorch/pytorch/issues/86449,2022-10-07T05:14:14Z,torch.cuda.empty_cache() is not working,YES,torch.cuda.empty_cache(),1.12.1+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/86356,2022-10-06T07:39:53Z,`conv_transpose` is not similar to `nn.grad.conv_input` when `output_padding` is passed with non-default values.,YES,"torch.conv_transpose2d
torch.nn.grad.conv2d_input",master,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/86326,2022-10-05T21:31:11Z,`torch.special.round` doesn't support the same dtypes as `torch.round`,YES,"torch.special.round  
torch.round",master (82d9592f1ba),"The description does not explicitly mention a CUDA version. Therefore, the response is:

<No CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/86162,2022-10-03T22:44:49Z,torch.nn.functional.one_hot only works for int64,YES,torch.nn.functional.one_hot,1.12.1+cu113,11.3
pytorch,https://github.com/pytorch/pytorch/issues/86124,2022-10-03T16:09:23Z,torch.jit.trace throwing Invalid name for qualified name eror ,YES,torch.jit.trace,1.11.0+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/85889,2022-09-29T10:29:24Z,High occupation on GPU 0 when converting Tensor to multi GPU,YES,"<torch.device>
<torch.cat>
<torch.cuda.set_device>
<torch.Tensor.to>
<torch.Tensor.cuda>",1.8.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/85877,2022-09-29T01:15:09Z,JIT model could return 'NaN' gradient after the first execution,YES,"<torch.jit.script>
<torch.autograd.functional.jacobian>
<torch.nn.functional.relu>
<torch.cos>
<torch.acos>",1.13.0.dev20220923+cu116,11.6
pytorch,https://github.com/pytorch/pytorch/issues/85813,2022-09-28T15:15:59Z,TorchScript error for `Enum` inside a module,YES,<torch.jit.script>,1.12.0a0+2c916ef,11.6
pytorch,https://github.com/pytorch/pytorch/issues/85805,2022-09-28T14:08:38Z,"`vector_norm` will trigger ""Tracing failed sanity checks"" for JIT when ord is boolean tensor",YES,torch.linalg.vector_norm,1.13.0.dev20220923+cu116,11.6
pytorch,https://github.com/pytorch/pytorch/issues/85804,2022-09-28T14:00:56Z,JIT fails to trace `sparse.mm` with a strange error,YES,torch.sparse.mm,1.13.0.dev20220923+cu116,11.6
pytorch,https://github.com/pytorch/pytorch/issues/85792,2022-09-28T06:48:25Z,TorchScript causes range_check error after a few iterations of forward-backward passes,YES,"torch.jit.load  
torch.randn  
torch.randn_like  
aten::dropout  
aten::add  
aten::to  
aten::mean  
aten::sub  
aten::pow  
aten::sqrt  
aten::div  
prim::TupleConstruct  
prim::TupleUnpack  
prim::ListConstruct  
prim::Constant  ",1.12.0+cu113,CUDA 11.3
pytorch,https://github.com/pytorch/pytorch/issues/85791,2022-09-28T06:31:47Z,nn.CrossEntropyLoss overflow with FP16 and minibatch,YES,torch.nn.CrossEntropyLoss,"1.8.2, 1.12.1","The issue description does not explicitly mention a CUDA version. It only references the versions of PyTorch tested (1.8.2 and 1.12.1). Therefore, I cannot provide a specific CUDA version based on the given information."
pytorch,https://github.com/pytorch/pytorch/issues/85671,2022-09-26T21:59:52Z,nn.Embedding weights are not synced across processes with DistributedDataParallel when other parameters are present,YES,<torch.nn.Embedding>,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/85656,2022-09-26T20:35:29Z,"[functorch] conv.{1, 2, 3}d should raise errors",YES,torch.nn.functional.conv2d,1.13.0a0+git83c73f7,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/85642,2022-09-26T17:24:46Z,Could not run 'aten::native_batch_norm' with arguments from the 'SparseCUDA' backend.  using batch_norm,YES,<torch.aten::native_batch_norm>,1.12.1+cu113,cu113
pytorch,https://github.com/pytorch/pytorch/issues/85588,2022-09-24T02:27:47Z,`linalg.norm` cannot compute the grad in forward mode after script,YES,<torch.linalg.norm>,1.13.0.dev20220923+cu116,11.6
pytorch,https://github.com/pytorch/pytorch/issues/85585,2022-09-24T01:37:47Z,`as_tensor` will return a different dtype with script,YES,torch.as_tensor,1.13.0.dev20220923+cu116,cu116
pytorch,https://github.com/pytorch/pytorch/issues/85570,2022-09-23T21:29:47Z,`mvlgamma_` will fail when compiling with trace `jit`,YES,<torch.mvlgamma_>,1.13.0.dev20220919,11.6
pytorch,https://github.com/pytorch/pytorch/issues/85558,2022-09-23T18:32:22Z,torch.Tensor.transpose().contiguous() on dimension of size 1 gives  wrong stride ,YES,"torch.Tensor.transpose  
torch.Tensor.contiguous",1.12.0a0+bd13bc6,11.6
pytorch,https://github.com/pytorch/pytorch/issues/85514,2022-09-22T22:09:09Z,The reload `MultiLabelMarginLoss` will have different gradients on cuda,YES,torch.nn.MultiLabelMarginLoss,1.13.0.dev20220919,11.6
pytorch,https://github.com/pytorch/pytorch/issues/85505,2022-09-22T21:34:38Z,The reload model has different (and strange) forward computation from original model with `LSTMCell`,YES,<torch.nn.LSTMCell>,1.13.0.dev20220919,11.6
pytorch,https://github.com/pytorch/pytorch/issues/85397,2022-09-21T09:25:08Z,torch.nn.utils.prune.remove reorders the parameters of a module unexpectedly,YES,<torch.nn.utils.prune.remove>,1.9.1+cu111,11.1
pytorch,https://github.com/pytorch/pytorch/issues/85329,2022-09-20T08:25:03Z,Crash in `torch.package.PackageExporter`,YES,torch.package.PackageExporter,1.12.1+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/85227,2022-09-18T11:59:41Z,topk returns different results with the same input in cuda and cpu,YES,torch.topk,1.10.2,11.3
pytorch,https://github.com/pytorch/pytorch/issues/85217,2022-09-17T17:26:23Z,Segmentation fault in native_batch_norm,YES,<torch.native_batch_norm>,1.11.0a0+gitbc2c6ed,N/A
pytorch,https://github.com/pytorch/pytorch/issues/85214,2022-09-17T17:26:03Z,Segmentation fault in mkldnn_reorder_conv2d_weight and mkldnn_reorder_conv3d_weight,YES,"<torch._C._nn.mkldnn_reorder_conv2d_weight>
<torch._C._nn.mkldnn_reorder_conv3d_weight>",1.11.0a0+gitbc2c6ed,N/A
pytorch,https://github.com/pytorch/pytorch/issues/85088,2022-09-15T15:46:42Z,reentrant torch.utils.checkpoint does not work with NamedTuple outputs,YES,torch.utils.checkpoint,main,"The issue description does not mention any specific CUDA version. Therefore, the answer is:

<No CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/85082,2022-09-15T10:35:56Z,"[NNC] loop vectorization fails, `Ramp` and `Broadcast` undefined",YES,"<torch._C._te.Ramp>
<torch._C._te.Broadcast>",1.12.1+cu113,11.3
pytorch,https://github.com/pytorch/pytorch/issues/85072,2022-09-15T06:02:50Z,Segmentation fault in `torch.jit.wait`,YES,<torch.jit.wait>,1.12.1+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/84990,2022-09-14T07:55:01Z,Segmentation fault in `torch.futures.collect_all`,YES,<torch.futures.collect_all>,1.12.1+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/84864,2022-09-12T15:13:00Z,torch.utils.checkpoint (with use_reentrant=False) doesn't work with all PyTorch features that set TLS,YES,"torch.utils.checkpoint  
torch.utils._python_dispatch.TorchDispatchMode",<Library Version> 1.13.0,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the text provided."
pytorch,https://github.com/pytorch/pytorch/issues/84673,2022-09-07T23:38:05Z,Autograd will take `init` module API into account when using `jit`,YES,torch.nn.init.ones_,1.12.1,"The issue description does not explicitly mention a CUDA version. Therefore, there is no CUDA version to provide."
pytorch,https://github.com/pytorch/pytorch/issues/84630,2022-09-07T12:47:37Z,"torch.nn.functional.interpolate fails on some degenerate shapes, but passes on others",YES,torch.nn.functional.interpolate,1.10.0,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/84628,2022-09-07T12:35:10Z,INTERNAL ASSERT when the type of argument is not considered in JIT,YES,"torch.jit.trace
torch.add",1.12.1,"The description does not explicitly mention a CUDA version. Therefore, there is no CUDA version referenced in the text."
pytorch,https://github.com/pytorch/pytorch/issues/84530,2022-09-05T06:18:47Z,`tensordot` not working for dtype int32 and lower when there is only 1 element in the given axis,YES,torch.tensordot,1.12.1+cu113,11.3
pytorch,https://github.com/pytorch/pytorch/issues/84304,2022-08-30T20:39:16Z,op for aten::bitwise_and during torch.jit.trace,YES,<torch.aten::bitwise_and>,1.11.0+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/84265,2022-08-30T13:03:40Z,torch.Tensor.to.dtype_layout overload is not available in Python,YES,torch.Tensor.to,Latest master,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/84202,2022-08-29T16:30:16Z,Gradient value calculation error in MultiLabelMarginLoss,YES,<torch.nn.functional.multilabel_margin_loss>,1.12.1+cu113,11.3
pytorch,https://github.com/pytorch/pytorch/issues/84175,2022-08-27T23:34:52Z,FSDP Forward order differs from that of first run,YES,<torch.generate>,1.12.1+cu113,11.3
pytorch,https://github.com/pytorch/pytorch/issues/83851,2022-08-22T16:09:04Z,torch fx cannot trace assert for some cases,YES,torch.fx,1.11.0a0+gitbc2c6ed,11.4
pytorch,https://github.com/pytorch/pytorch/issues/83824,2022-08-21T20:49:17Z,RuntimeError: Interrupted system call when doing distributed training,YES,torch.distributed.init_process_group,1.12.0+cu116,11.6
pytorch,https://github.com/pytorch/pytorch/issues/83818,2022-08-21T07:59:58Z,torch.linalg.eigh crashe for matrices of size 2895×2895 or larger on eigen and M1,YES,torch.linalg.eigh,1.12.0,"The issue description does not explicitly mention a CUDA version. Therefore, it is not possible to provide a CUDA version based on the given text."
pytorch,https://github.com/pytorch/pytorch/issues/83733,2022-08-19T11:40:39Z,BCELoss results in autocast CUDA warning,YES,torch.nn.BCELoss,torch==1.12.1+cpu,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/83702,2022-08-18T21:46:59Z,Inconsistency between index_select and __get_item__,YES,"torch.index_select  
torch.__get_item__",master,<Cuda version>
pytorch,https://github.com/pytorch/pytorch/issues/83494,2022-08-16T01:57:34Z,`torch.pinverse` produces wrong output!,YES,<torch.pinverse>,torch 1.12.1,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/83493,2022-08-16T01:39:12Z,Calling torch.linalg.cholesky on a CPU tensor requires compiling PyTorch with LAPACK.,YES,<torch.linalg.cholesky>,1.13.0a0+git65f7fa8,11.7
pytorch,https://github.com/pytorch/pytorch/issues/83388,2022-08-14T05:29:09Z,"zero-numel tensor has ""RuntimeError: strides[cur - 1] == sizes[cur] * strides[cur] INTERNAL ASSERT FAILED"" in multi-thread.",YES,<torch.jit>,1.9.0a0+d69c22d.cuda11.4.bionic.aurora.b38,11.4
pytorch,https://github.com/pytorch/pytorch/issues/83229,2022-08-11T03:55:32Z,torch.nn.MaxUnpool2d get negative size tensor,YES,"torch.nn.MaxUnpool2d  
torch.nn.MaxUnpool3d  ",1.8.1,"The issue description provided does not mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/83214,2022-08-10T22:53:03Z,torchgen.model.FunctionSchema.parse fails with following ops' schema ,YES,"torch.aten.to  
torch.prims.as_strided  
torch.prims.copy_to  
torch.prims.resize  
torch.aten.add.t  
torch.aten.eq.enum  
torch.aten.mul.left_t  
torch.aten.mul.right_  
torch.aten.ne.enum  
torch.aten.rot90  
torch.aten.fft_fft2  
torch.aten.fft_fft2.out  
torch.aten.fft_ifft2  
torch.aten.fft_ifft2.out  
torch.aten.fft_rfft2  
torch.aten.fft_rfft2.out  
torch.aten.fft_irfft2  
torch.aten.fft_irfft2.out  
torch.aten.fft_hfft2  
torch.aten.fft_hfft2.out  
torch.aten.fft_ihfft2  
torch.aten.fft_ihfft2.out  ",master,"The issue description does not mention any specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/83052,2022-08-09T03:24:31Z,FSDP init can crash with shared parameters,YES,<torch.distributed.fsdp.FullyShardedDataParallel>,main,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/82894,2022-08-05T17:56:30Z,linalg and lu tests fail when run in parallel on linux cuda,YES,"<torch.linalg>  
<torch.lu>  
<torch.lu_factor>  
<torch.lu_solve>  
<torch.linalg.ldl_solve>  
<torch.cuda.synchronize>  
<torch.manual_seed>  
<torch.cuda.manual_seed_all>  ",1.13.0a0+gita22ba1e,11.6
pytorch,https://github.com/pytorch/pytorch/issues/82886,2022-08-05T16:02:52Z,CUDA graph capturing fails for nn.Embedding and large batch sizes,YES,<torch.nn.Embedding>,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/82635,2022-08-02T05:16:53Z,[Torchscript] torch.min returns wrong gradient when inputs are equal,YES,"<torch.min>
<torch.max>",1.13.0a0+340c412,11.7
pytorch,https://github.com/pytorch/pytorch/issues/82634,2022-08-02T05:11:21Z,[Torchscript] some activations backward are not fused when used with linear,YES,torch.erf,1.13.0a0+340c412,11.7
pytorch,https://github.com/pytorch/pytorch/issues/82610,2022-08-01T21:34:03Z,contiguous() not work for rank 1 length 1 tensor.,YES,"<torch.tensor>  
<torch.tensor().real>  
<torch.tensor().real.contiguous>  
<torch.tensor().real.contiguous().stride>",<torch 1.12.0>,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/82510,2022-07-29T23:43:46Z,Complex addition result in NaN when it shouldn't,YES,"torch.view_as_real
torch.view_as_complex",1.12.0+cu113,11.3
pytorch,https://github.com/pytorch/pytorch/issues/82465,2022-07-29T10:59:19Z,Does torch.utils.checkpoint compatible with torch.cuda.make_graphed_callables?,YES,"<torch.cuda.make_graphed_callables>
<torch.utils.checkpoint.checkpoint_sequential>",1.11.0+cu113,11.3
pytorch,https://github.com/pytorch/pytorch/issues/82308,2022-07-27T12:52:17Z,torch.einsum gets wrong results randomly when training with multi-gpu,YES,<torch.einsum>,1.12.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/82303,2022-07-27T09:19:22Z,Race condition between torch.tensor's view and /= (/= returns incorrect result),YES,<torch.Tensor>,1.11.0+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/82282,2022-07-27T00:28:15Z,`torch.matrix_exp` doesn't handle NaN properly,YES,torch.matrix_exp,1.12.0,"The issue description does not explicitly mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/82218,2022-07-26T13:30:30Z,OOM during backward() leads to memory leaks,YES,"<API Name>
torch.autograd.backward",1.12.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/82217,2022-07-26T12:31:21Z,backward not available for index and mask ,YES,"<torch.index_select>
<torch.masked_select>",1.8.2+cu111,cu111
pytorch,https://github.com/pytorch/pytorch/issues/82212,2022-07-26T09:16:42Z,"RuntimeError: ""reflection_pad2d"" not implemented for 'Half' in autocast enabled region",YES,torch.nn.functional.reflection_pad2d,torch==1.12.0,The CUDA version is not explicitly mentioned in the provided GitHub issue description.
pytorch,https://github.com/pytorch/pytorch/issues/82200,2022-07-26T02:51:02Z,"model.to(device) takes time forever on A40-8Q, NVIDIA. cuda11.1, torch1.9.1.",YES,torch.model.to,1.9.1+cu111,cuda11.1
pytorch,https://github.com/pytorch/pytorch/issues/82185,2022-07-25T23:52:07Z,DataLoader: `pin_memory` should respect object attributes before object collection type,YES,<torch.utils.data.DataLoader.pin_memory>,1.11.0+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/82077,2022-07-24T22:26:24Z,Using DDP with num_workers > 0 hangs before entering the first training epoch loop,YES,"<torch.utils.data.distributed.DistributedSampler>  
<torch.utils.data.DataLoader>  
<torch.nn.CrossEntropyLoss>  
<nn.parallel.DistributedDataParallel>  
<torch.enable_grad>  ",1.13.0.dev20220724+cu116,11.6
pytorch,https://github.com/pytorch/pytorch/issues/82068,2022-07-23T22:02:27Z,upsample_bilinear2d() received an invalid combination of arguments,YES,<torch.nn.functional.interpolate>,1.11 and nightly,"The issue description does not explicitly mention a CUDA version. It only references PyTorch versions (1.10.1, 1.11, and nightly). Therefore, no CUDA version can be identified from the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/82065,2022-07-23T20:00:21Z,optimize_for_mobile vulkan_prepack::conv2d_clamp_prepack,YES,"<torch.utils.mobile_optimizer.optimize_for_mobile>  
<torch._C._jit_pass_vulkan_optimize_for_mobile>  
<vulkan_prepack::conv2d_clamp_prepack>  ",1.12.0+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/81868,2022-07-21T11:21:16Z,grid_sample and mode='bilinear' induces errors at discrete pixel locations,YES,<torch.nn.functional.grid_sample>,1.12.0+cu113,cu113
pytorch,https://github.com/pytorch/pytorch/issues/81648,2022-07-18T18:20:56Z,`sparse_coo.to_dense()` produces different results between CPU and CUDA backends for boolean non-coalesced inputs.,YES,"torch.sparse_coo_tensor  
torch.to_dense",Current master,"The issue description does not explicitly mention a specific CUDA version. It only refers to ""Current master."" Therefore, there is no CUDA version provided in the description."
pytorch,https://github.com/pytorch/pytorch/issues/81554,2022-07-15T15:10:49Z,float' object is not callable when using scheduler.step() with MultiplicativeLR,YES,torch.optim.lr_scheduler.MultiplicativeLR,1.10.2,11.3
pytorch,https://github.com/pytorch/pytorch/issues/81545,2022-07-15T05:45:01Z,Precision error from torch.distributed.send() to recv(),YES,"<torch.distributed.send>
<torch.distributed.recv>",1.12.0,"11.6, 11.1"
pytorch,https://github.com/pytorch/pytorch/issues/81361,2022-07-12T22:10:11Z,Segfault with fake tensor,YES,"torch.handle_torch_function_no_python_arg_parser  
at::_ops::t::redispatch  
torch::ADInplaceOrView::t  
at::_ops::linear::call  ",1.13.0,"The issue description does not mention a specific CUDA version. Therefore, based on the provided information, there is no CUDA version to identify."
pytorch,https://github.com/pytorch/pytorch/issues/81323,2022-07-12T15:10:02Z,`torch.overrides.get_testing_overrides` does not function as intended for native tensor methods/operations,YES,<torch.overrides.get_testing_overrides>,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/81195,2022-07-11T05:12:01Z,torch._weight_norm with specified dim returns wrong output,YES,<torch._weight_norm>,1.12.0,"The description does not explicitly mention any specific CUDA version. Therefore, the answer is:

<N/A>"
pytorch,https://github.com/pytorch/pytorch/issues/80946,2022-07-06T10:00:28Z,torch.nn.functional.linear fails for multi-dimensional bias from torch 1.12,YES,torch.nn.functional.linear,1.12.0,"The issue description does not explicitly mention a CUDA version. Therefore, there is no CUDA version referenced in the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/80861,2022-07-05T09:41:49Z,"jit.freeze throws RuntimeError: stack_out && stack_out->size() == 1 INTERNAL ASSERT FAILED at ""../torch/csrc/jit/passes/frozen_conv_folding.cpp"":281",YES,<torch.jit.freeze>,1.12.0+cu113,11.3
pytorch,https://github.com/pytorch/pytorch/issues/80808,2022-07-03T14:17:11Z,"The result of doing a dot product between two vectors, using einsum, depends on another unrelated vector",YES,<torch.einsum>,1.11.0+cu113,11.3
pytorch,https://github.com/pytorch/pytorch/issues/80804,2022-07-03T05:03:05Z,`torch.renorm` gives wrong gradient for 0-valued input when `p` is even and `maxnorm=0`.,YES,<torch.renorm>,1.11.0,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/80803,2022-07-03T04:52:04Z,`hardshrink` gives wrong gradient for 0 input when `lambd` is 0.,YES,torch.hardshrink,1.11.0,The CUDA version is not explicitly mentioned in the provided issue description.
pytorch,https://github.com/pytorch/pytorch/issues/80762,2022-07-01T07:34:08Z,`atan2` will gradcheck fail when `other` is a tensor with `int8` dtype,YES,torch.atan2,1.11.0,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based solely on the given text. If you need a specific CUDA version related to the PyTorch version mentioned (1.11.0), it typically supports CUDA 10.2 and 11.3. However, no specific CUDA version is stated in the issue itself."
pytorch,https://github.com/pytorch/pytorch/issues/80761,2022-07-01T07:22:01Z,`det` will return wrong gradient for `1x1` matrix with 0 value.,YES,torch.det,1.11.0,"The issue description does not mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/92033,2022-06-30T22:07:18Z,Unable to use vmap atop torch.distribution functionality,YES,"torch.distributions.Independent  
torch.distributions.Normal  
functorch.make_functional_with_buffers  
functorch.jacrev  
functorch.vmap",<1.12.0>,The issue description provided does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/80738,2022-06-30T20:46:37Z,Output for `aten::_native_multi_head_attention` appears inconsistent with entry in `native_functions.yaml`,YES,<torch.ops.aten._native_multi_head_attention>,1.12.0+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/80588,2022-06-30T03:54:25Z,Semi-reproducible random torch.baddbmm NaNs,YES,torch.baddbmm,1.11.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/80580,2022-06-30T01:05:05Z,`torch.ops.aten.find` inconsistent with `str.find`,YES,torch.ops.aten.find,1.13.0.dev20220623+cpu,Could not collect
pytorch,https://github.com/pytorch/pytorch/issues/80496,2022-06-29T03:37:12Z,DDP find_unused_parameters=True does not work for Sparse gradients,YES,torch.DDP,torch==1.7.0+cu110,cu110
pytorch,https://github.com/pytorch/pytorch/issues/80488,2022-06-29T01:16:35Z,Negative values still produced by torch.nn.functional.kl_div,YES,torch.nn.functional.kl_div,pytorch==1.12.0,11.3.1
pytorch,https://github.com/pytorch/pytorch/issues/80302,2022-06-26T16:12:30Z,AttributeError: 'LinearPackedParams' object has no attribute '_modules',YES,torch.nn.modules.module,1.11.0+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/80208,2022-06-24T05:09:30Z,F.binary_cross_entropy_with_logits unexpected behaviour,YES,<torch.F.binary_cross_entropy_with_logits>,1.11.0,There is no CUDA version mentioned in the provided GitHub issue description.
pytorch,https://github.com/pytorch/pytorch/issues/80206,2022-06-24T03:20:22Z,`soft_margin_loss` gives wrong gradient when `target` with dtype uint8,YES,<torch.nn.functional.soft_margin_loss>,1.11.0,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version based on the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/80204,2022-06-24T03:18:52Z,`max_unpool` gives wrong gradient when `indices` has duplicate,YES,<torch.nn.functional.max_unpool1d>,1.11.0,"The description does not explicitly mention a CUDA version. However, it provides the PyTorch version (1.11.0), which typically corresponds to a specific CUDA version. For PyTorch 1.11.0, the supported CUDA versions are 10.2 and 11.3. Since no specific CUDA version is directly mentioned, I am unable to provide just one. 

If you need one of the supported versions, please specify which one you prefer. 

Here's the CUDA version that can be inferred from PyTorch 1.11.0:

CUDA version: 10.2 or 11.3"
pytorch,https://github.com/pytorch/pytorch/issues/80061,2022-06-22T18:43:33Z,pow CUDA tensor raised to CPU scalar tensor result can't backward properly,YES,torch.pow,"1.11.0, 1.13.0","The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/80022,2022-06-22T09:24:49Z,memory leaking when doing all_to_all_single communication ,YES,<torch.all_to_all_single>,1.10.0,"CUDA 11.3, CUDA 11.1"
pytorch,https://github.com/pytorch/pytorch/issues/79888,2022-06-20T19:08:37Z,`CosineAnnealingWarmRestarts` does not update parameters added with `add_param_group`,YES,"<torch.optim.Optimizer.add_param_group>
<torch.optim.lr_scheduler.CosineAnnealingWarmRestarts>",1.10.2+cu113,11.3
pytorch,https://github.com/pytorch/pytorch/issues/79705,2022-06-16T14:25:56Z,gradgradcheck fails for torch.native_layer_norm,YES,<torch.native_layer_norm>,1.11.0+cu113,cu113
pytorch,https://github.com/pytorch/pytorch/issues/79703,2022-06-16T14:01:23Z,Float and double tensors randomly initialized with the same seed get different values for size >= 16,YES,"torch.manual_seed  
torch.normal_  ",1.11.0.post2,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/79542,2022-06-14T17:39:46Z,Unable to use a parameter with torch.sparse_coo layout with DDP,YES,"torch.sparse_coo_tensor  
torch.nn.parallel.DistributedDataParallel  ",1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/79388,2022-06-13T01:17:48Z,Init connect timeout when use torch.distributed.run,YES,<torch.distributed.run>,1.11.0+cu102,CUDA version 10.2
pytorch,https://github.com/pytorch/pytorch/issues/79171,2022-06-09T00:03:29Z,DistributedDataParallel `static_graph=True` fails to handle unused parameters,YES,<torch.nn.parallel.distributed.DistributedDataParallel>,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/79145,2022-06-08T19:24:47Z,Redundant info are saved when using torch.save to save part of torch.tensor,YES,<torch.save>,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/78987,2022-06-07T01:26:18Z,DataLoader leaking resources?,YES,torch.utils.data.DataLoader,1.11.0+cpu,There is no CUDA version mentioned in the provided description.
pytorch,https://github.com/pytorch/pytorch/issues/78961,2022-06-06T19:50:23Z,[forwardAD] torch.no_grad has no effect under forward_ad,YES,torch.no_grad,master,"The issue description does not mention a specific CUDA version. It only states ""master"" under the Versions section."
pytorch,https://github.com/pytorch/pytorch/issues/78638,2022-06-01T18:23:38Z,"torch.distributed.init_process_group(backend=""nccl"") NCCL version error",YES,torch.distributed.init_process_group,1.11.0,11.0
pytorch,https://github.com/pytorch/pytorch/issues/78487,2022-05-30T12:59:44Z,torch.fx: symbolic_trace: ones() received an invalid combination of arguments,YES,<torch.fx.symbolic_trace>,1.13.0.dev20220530+cpu,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/78486,2022-05-30T10:59:41Z,Exception in torch.jit.script doesn't indicate where in the code the problem lies.,YES,torch.jit.script,1.11.0a0+gitbc2c6ed,11.7
pytorch,https://github.com/pytorch/pytorch/issues/78484,2022-05-30T09:50:06Z,torch.lerp: discrepancy between CUDA and CPU (with extremal inputs),YES,torch.lerp,master,"The issue description does not mention a specific CUDA version. Therefore, the answer is:

<CUDA version>"
pytorch,https://github.com/pytorch/pytorch/issues/78482,2022-05-30T05:22:21Z,RuntimeError: Event device type CUDA does not match blocking stream’s device type CPU ,YES,torch.autograd.backward,1.11.0a0,N/A
pytorch,https://github.com/pytorch/pytorch/issues/78475,2022-05-29T14:23:20Z,`with torch.backends.cudnn.flags(deterministic=True)` doesn't give an exception for ctc_loss backward on CUDA,YES,"torch.backends.cudnn.flags  
torch.use_deterministic_algorithms  
F.ctc_loss",1.9.0,11.1
pytorch,https://github.com/pytorch/pytorch/issues/78444,2022-05-28T01:47:09Z,`layer_norm` triggers INTERNAL ASSERT with input requiring grad + zero-size int tensor,YES,<torch.nn.functional.layer_norm>,1.11.0,"The description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version based on the provided information."
pytorch,https://github.com/pytorch/pytorch/issues/78443,2022-05-28T01:29:24Z,`index_fill` will trigger INTERNAL ASSERT when float tensor requiring grad + int tensor,YES,<torch.index_fill>,1.11.0,"The issue description does not mention a specific CUDA version. Therefore, the answer is:

<No CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/78435,2022-05-27T21:33:23Z,fx.Tracer with param_shapes_constant=True not working for RobertaForMaskedLM,YES,<torch.fx.Tracer>,1.10.2,CUDA version: 11.3.1
pytorch,https://github.com/pytorch/pytorch/issues/78413,2022-05-27T07:47:11Z,torch.angle differs from np.angle for -0.,YES,torch.angle,master,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/78346,2022-05-26T07:35:05Z,`gradcheck` fails for `torch.distribution.transform` APIs in forward mode,YES,"torch.distributions.transforms.AbsTransform  
torch.distributions.transforms.SigmoidTransform  
torch.distributions.transforms.TanhTransform  
torch.distributions.transforms.SoftmaxTransform",1.11.0,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/78255,2022-05-25T14:33:08Z,Floating point exception in _conv_depthwise2d,YES,<torch._C._nn._conv_depthwise2d>,1.11.0a0+git1efeb37,N/A
pytorch,https://github.com/pytorch/pytorch/issues/78249,2022-05-25T10:24:51Z,`max_unpool2d` is not deterministic,YES,<torch.nn.MaxUnpool2d>,1.11.0,"The CUDA version is not explicitly mentioned in the provided issue description. Therefore, I cannot provide a specific CUDA version based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/78201,2022-05-24T20:43:51Z,nn.Sequential causes fx.replace_pattern to not find any match. ,YES,"<torch.nn.Sequential>  
<torch.fx.replace_pattern>  
<torch.fx.symbolic_trace>  
<torchvision.models.resnet.Bottleneck>  ",1.11.0+cu113,11.3
pytorch,https://github.com/pytorch/pytorch/issues/78153,2022-05-24T01:53:35Z,`pack_sequence` crash,YES,torch.nn.utils.rnn.pack_sequence,1.11.0,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/78151,2022-05-24T01:29:33Z,`ctc_loss` will backward crash,YES,<torch.nn.functional.ctc_loss>,1.11.0,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version based on the provided information."
pytorch,https://github.com/pytorch/pytorch/issues/78143,2022-05-24T00:50:01Z,`baddmm` triggers INTERNAL ASSERT FAILED when input requires grad,YES,<torch.baddbmm>,1.11.0,"The issue description provided does not mention a specific CUDA version. Therefore, the answer is:

<No CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/78141,2022-05-24T00:44:42Z,"`matmul, mm` triggers INTERNAL ASSERT FAILED when input requires grad",YES,"<torch.matmul>
<torch.mm>",1.11.0,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/78131,2022-05-23T21:11:42Z,Segfault in _pad_packed_sequence,YES,<torch._pad_packed_sequence>,1.11.0,N/A
pytorch,https://github.com/pytorch/pytorch/issues/78130,2022-05-23T21:10:35Z,Segfault in _grid_sampler_2d_cpu_fallback,YES,<torch._grid_sampler_2d_cpu_fallback>,1.11.0,N/A
pytorch,https://github.com/pytorch/pytorch/issues/78129,2022-05-23T21:09:16Z,Segfault in _embedding_bag_forward_only,YES,<torch._embedding_bag_forward_only>,1.11.0,N/A
pytorch,https://github.com/pytorch/pytorch/issues/78128,2022-05-23T21:06:50Z,Segfault in torch._C._nn.thnn_conv2d,YES,<torch._C._nn.thnn_conv2d>,1.11.0,N/A
pytorch,https://github.com/pytorch/pytorch/issues/78127,2022-05-23T21:05:18Z,Segfault in torch._C._nn.reflection_pad2d,YES,<torch._C._nn.reflection_pad2d>,1.11.0,N/A
pytorch,https://github.com/pytorch/pytorch/issues/78126,2022-05-23T21:00:57Z,Segfault in max_unpool3d,YES,<torch.max_unpool3d>,1.11.0,N/A
pytorch,https://github.com/pytorch/pytorch/issues/78125,2022-05-23T20:57:10Z,Segfault in grid_sampler_3d,YES,<torch.grid_sampler_3d>,1.11.0,N/A
pytorch,https://github.com/pytorch/pytorch/issues/78122,2022-05-23T20:50:38Z,Segfault in bincount,YES,<torch.bincount>,1.11.0,N/A
pytorch,https://github.com/pytorch/pytorch/issues/78109,2022-05-23T18:21:24Z,Doesn't work when register hook to torch.nn.MultiheadAttention.out_proj,YES,torch.nn.MultiheadAttention.out_proj,1.9.1+cu111,11.1
pytorch,https://github.com/pytorch/pytorch/issues/78075,2022-05-23T10:40:18Z,torch.multiprocessing.spawn raise PicklingError inside a decorator,YES,<torch.multiprocessing.spawn>,1.10.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/78047,2022-05-21T20:07:36Z,DDP multi host with single GPU each. ,YES,<torch.cuda.set_device>,1.11.0+cu115,CUDA version: 11.5
pytorch,https://github.com/pytorch/pytorch/issues/77967,2022-05-20T15:15:44Z,Inference Tensors should not be allowed to hold `grad_fn`,YES,torch.ops.aten.sin.default,master,"The issue description does not mention a specific CUDA version. It only includes the term ""master"" under the Versions section, which typically refers to the main development branch of the software rather than a specific version of CUDA. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/77963,2022-05-20T14:46:58Z,`logaddexp2` fails to backward,YES,<torch.logaddexp2>,1.11.0,"The issue description does not mention a specific CUDA version. Therefore, I cannot provide a CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/77951,2022-05-20T07:48:48Z,`topk` returns different results with the same input twice in cuda,YES,torch.topk,pytorch: 1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/77901,2022-05-19T21:23:21Z,Heap corruption in slow_conv_transpose3d,YES,torch._C._nn.slow_conv_transpose3d,1.11.0,N/A
pytorch,https://github.com/pytorch/pytorch/issues/77900,2022-05-19T21:23:17Z,Floating point exception in slow_conv3d,YES,torch._C._nn.slow_conv3d,1.11.0,N/A
pytorch,https://github.com/pytorch/pytorch/issues/77899,2022-05-19T21:23:13Z,Floating point exception in native_channel_shuffle,YES,torch.native_channel_shuffle,1.11.0,N/A
pytorch,https://github.com/pytorch/pytorch/issues/77894,2022-05-19T21:22:46Z,Floating point exception in channel_shuffle,YES,torch.channel_shuffle,1.11.0,N/A
pytorch,https://github.com/pytorch/pytorch/issues/77893,2022-05-19T21:22:41Z,Segmentation fault in _remove_batch_dim,YES,torch._remove_batch_dim,1.11.0,N/A
pytorch,https://github.com/pytorch/pytorch/issues/77838,2022-05-19T08:28:27Z,Sporadic convolution error with dilation=0,YES,torch.nn.Conv2d,1.12.0a0+gitd40a240,11.3
pytorch,https://github.com/pytorch/pytorch/issues/77821,2022-05-19T02:45:17Z,cannot convert to channels last format for conv2d conv3d hybrid model,YES,"torch.channels_last  
torch.channels_last_3d  ",1.10.0a0+gitcb9f926,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/77814,2022-05-19T01:35:36Z,"`addmv, mv` will trigger INTERNAL ASSERT FAILED when input requiring grad",YES,"<torch.addmv>
<torch.mv>",1.11.0,"The issue description does not explicitly mention a specific CUDA version. Therefore, no CUDA version can be identified from the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/77576,2022-05-16T18:14:58Z,`stateless.functional_call` doesn't work with `nn.DataParallel`,YES,"torch.nn.DataParallel  
torch.nn.utils.stateless.functional_call",Nightly,"The issue description does not mention a specific CUDA version. It only states ""Nightly"" in the versions section. Therefore, no CUDA version can be identified from the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/77527,2022-05-16T08:24:55Z,CUDA: Illegal memory access in `torch.linalg.solve()`,YES,<torch.linalg.solve>,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/77515,2022-05-16T00:52:17Z,Inplace Bool API + `sum` will trigger INTERNAL ASSERT FAILED,YES,"<torch.eq_>
<torch.index_fill_>",1.11.0,The description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/77514,2022-05-16T00:41:37Z,`max_pool1d` can succeed when padding is negative for tensor requiring grad,YES,<torch.nn.functional.max_pool1d>,1.11.0,The CUDA version is not explicitly mentioned in the issue description provided.
pytorch,https://github.com/pytorch/pytorch/issues/77397,2022-05-13T01:27:22Z,Large numerical inconsistency for `torch.einsum` on RTX30 series GPU.,YES,<torch.einsum>,1.11.0+cu113,11.3
pytorch,https://github.com/pytorch/pytorch/issues/77354,2022-05-12T15:49:38Z,"outputs_[i]->uses().empty()INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1646755853042/work/torch/csrc/jit/ir/ir.cpp"":1314, please report a bug to PyTorch. ",YES,torch.jit.script,1.11.0,10.2.89
pytorch,https://github.com/pytorch/pytorch/issues/77231,2022-05-11T05:12:17Z,`torch.scatter_add` will succeed when the `index` is a complex tensor,YES,torch.scatter_add,1.11.0,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a CUDA version based on the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/77230,2022-05-11T04:11:59Z,fast `gradcheck` fails when outputs that do not require grad precede outputs that do,YES,<torch.linalg.slogdet>,1.11.0,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/77067,2022-05-09T10:31:19Z,Unable to continue adding modules to `nn.Sequential` after using `del` method,YES,torch.nn.Sequential,1.11.0,"The issue description does not mention a specific CUDA version. Therefore, the response would be:

<no CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/77050,2022-05-08T15:57:54Z,ENORMOUS OVERHEAD from mp.get_context('spawn'),YES,<torch.multiprocessing.get_context>,1.10.2,"The issue description does not mention a specific CUDA version. Therefore, the answer is:

<No CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/76960,2022-05-06T13:55:35Z,"When using Rsqrt, the output of the 1/x process is very likely to have nan/inf",YES,torch.Rsqrt,Pytorch1.8,11.2
pytorch,https://github.com/pytorch/pytorch/issues/76954,2022-05-06T12:47:55Z,ReduceSum causes the inconsistency of output between frameworks,YES,torch.ReduceSum,Pytorch1.8,11.2
pytorch,https://github.com/pytorch/pytorch/issues/76885,2022-05-05T09:03:39Z,EmbeddingBag: Does CUDA calculate error in EmbeddingBag forward when include_last_offset=True ?,YES,<torch.nn.EmbeddingBag>,36420b5e8cce9c783903bbc210ed7f2b6535ebf5,11.2
pytorch,https://github.com/pytorch/pytorch/issues/76806,2022-05-04T10:03:31Z,torch.Tensor.__rdiv__ long x scalar float type promotion is incorrect,YES,torch.Tensor.__rdiv__,<Library Version> 1.13.0,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/76804,2022-05-04T09:53:27Z,"torch.add bool x bool allows integer alpha, inconsistent with other dtype type checking",YES,torch.add,<Library Version> 1.10.0,The description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/76798,2022-05-04T08:22:17Z,`gradcheck` for `torch.solve` may trigger INTERNAL ASSERT FAILED,YES,<torch.solve>,1.11.0,"The issue description does not mention a specific CUDA version. Therefore, based on the provided information, there is no CUDA version referenced."
pytorch,https://github.com/pytorch/pytorch/issues/76786,2022-05-04T02:35:22Z,"`cumprod, prod` will backward fail if `dtype` argument is different than the dtype of input tensor",YES,"<torch.cumprod>
<torch.prod>",1.11.0,"The issue description does not mention a specific CUDA version. Therefore, it is not possible to identify a CUDA version from the provided information."
pytorch,https://github.com/pytorch/pytorch/issues/76785,2022-05-04T02:31:27Z,"`addr, baddmm, dist, l1_loss` will backward fail when input tensors have different dtypes",YES,"torch.addr  
torch.baddbmm  
torch.dist  
torch.nn.functional.l1_loss  ",1.11.0,"The CUDA version mentioned in the GitHub issue description is not explicitly stated. The issue primarily discusses bugs related to PyTorch functionality. Therefore, there is no specific CUDA version provided in the text. 

If you are looking for a specific CUDA version information, it might typically be found in the PyTorch installation documentation or related release notes, but it is not present in the provided description."
pytorch,https://github.com/pytorch/pytorch/issues/76783,2022-05-04T02:02:09Z,`gradcheck` fails for `torch.trace`,YES,"<torch.gradcheck>
<torch.trace>",1.11.0,"The issue description does not explicitly mention a CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/76780,2022-05-04T00:47:35Z,`gradcheck` should support the comparison of NaN,YES,torch.autograd.gradcheck,1.11.0,"The issue description does not mention a specific CUDA version. Therefore, the answer is:

<no CUDA version>"
pytorch,https://github.com/pytorch/pytorch/issues/76778,2022-05-04T00:28:15Z,`torch.addmv` backward fails,YES,torch.addmv,1.11.0,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/76760,2022-05-03T18:27:04Z,hanging process with init_process_group(backend='mpi') cannot be killed ,YES,"<torch.distributed.init_process_group>
<torch.distributed.get_rank>
<torch.distributed.get_world_size>
<torch.distributed.all_gather>
<torch.distributed.destroy_process_group>",1.8.1+cpu,"The issue description does not mention a specific CUDA version. Therefore, I cannot provide a CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/76705,2022-05-02T23:50:13Z,"Torch `x += y.bmm(z)` is faster than `x.baddbmm_(y, z)`",YES,<torch.badbmm_>,1.12.0a0+fb,"The issue description does not mention a specific CUDA version. Therefore, I cannot provide a CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/76654,2022-05-02T12:22:21Z,`torch.sort` does not exploit parallelization when invoked without the `dim` parameter.,YES,torch.sort,pytorch 1.12.0,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/76646,2022-05-02T02:42:47Z,`torch.nn.HuberLoss` backwards unexpectedly fail,YES,"<torch.nn.HuberLoss>
<torch.nn.SmoothL1Loss>",1.11.0,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/76644,2022-05-02T02:09:23Z,`torch.smm` backward fail with strange error message,YES,<torch.smm>,1.11.0 and nightly version,"The issue description does not explicitly mention a specific CUDA version. Therefore, no CUDA version can be identified from the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/76585,2022-04-29T11:02:16Z,`torch.linalg.cond` has different results for tensor requiring autograd,YES,<torch.linalg.cond>,1.11.0,"The issue description does not mention a specific CUDA version. Therefore, I cannot provide a CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/76571,2022-04-29T00:29:32Z,torch.unique() nondeterministic behavior on nan inputs (on GPU),YES,torch.unique,1.12.0a0+gite816e17,11.1
pytorch,https://github.com/pytorch/pytorch/issues/76528,2022-04-28T10:49:49Z,Discrepancy in einsum when done in batch vs non-batch,YES,torch.einsum,1.10.0+cu111,11.1
pytorch,https://github.com/pytorch/pytorch/issues/76527,2022-04-28T10:47:38Z,Non target rank receives result of 'reduce' op when backend is 'gloo',YES,<torch.distributed.reduce>,1.12.0a0+git0aa3c39,"The issue description does not mention a specific CUDA version. It only references the PyTorch version '1.12.0a0+git0aa3c39'. Therefore, it cannot be determined from this description what the CUDA version is."
pytorch,https://github.com/pytorch/pytorch/issues/76522,2022-04-28T04:40:04Z,`torch.clamp` does not distribute gradients as element-wise`min/max` do,YES,"torch.clamp  
torch.min  
torch.max",1.11.0+cpu,CUDA used to build PyTorch: Could not collect
pytorch,https://github.com/pytorch/pytorch/issues/76487,2022-04-27T21:15:27Z,`F.interpolate` uses incorrect size when `align_corners=True`,YES,<F.interpolate>,Any currently available pytorch version.,"The issue description does not explicitly mention a specific CUDA version. Therefore, it can be stated that:

Any currently available pytorch version. 

There is no specific CUDA version referenced."
pytorch,https://github.com/pytorch/pytorch/issues/76410,2022-04-26T22:17:12Z,TorchFunction handling and overload resolution very slow in `torch.ops`,YES,<torch.ops.profiler._record_function_exit>,2291960d3f0ea0f7c4e99e955bc9a23783ef70a3,"The issue description does not explicitly mention a CUDA version. Therefore, based on the provided text, there is no specific CUDA version referenced."
pytorch,https://github.com/pytorch/pytorch/issues/76389,2022-04-26T18:03:21Z,Error in DistributedDataParallel with 'CPU' device,YES,torch.nn.parallel.DistributedDataParallel,1.4.0,"The issue description does not mention any specific CUDA version. Therefore, there is no CUDA version to provide."
pytorch,https://github.com/pytorch/pytorch/issues/76347,2022-04-25T22:28:40Z,`index_select` allows negative `index` for sparse but not for strided `self`,YES,torch.index_select,1.12.0a0+git02f4803,11.2
pytorch,https://github.com/pytorch/pytorch/issues/76344,2022-04-25T22:06:43Z,"[ONNX] Use topk to export max(dim,keepdim) to onnx",YES,"<torch.max>
<torch.topk>
<torch.onnx.export>",1.10.2,11.3
pytorch,https://github.com/pytorch/pytorch/issues/76294,2022-04-25T02:29:44Z,torch._remove_batch_dim is interceptable by __torch_function__ / batch tensors don't print correctly,YES,"torch._remove_batch_dim  
torch._add_batch_dim  
torch.vmap  
torch.ceil  
aten::is_nonzero  ",master,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/76287,2022-04-24T15:38:37Z,torch.elastic fails to shutdown despite crashed processes,YES,torch.distributed.elastic.multiprocessing.api,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/76282,2022-04-24T07:57:07Z,`torch.cuda.amp.GradScaler` may skip parameter synchronization required by post localSGD optimizer,YES,<torch.cuda.amp.GradScaler>,PyTorch 1.11,"The issue description does not explicitly mention a specific CUDA version. Therefore, based on the provided text, there is no CUDA version referenced."
pytorch,https://github.com/pytorch/pytorch/issues/76069,2022-04-19T21:10:02Z,`init_process_group` hanging on HPC multi-node system w GPU ,YES,<torch.distributed.init_process_group>,1.10.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/76043,2022-04-19T15:35:44Z,RuntimeError: bucket_count == per_bucket_sizes.size() INTERNAL ASSERT FAILED,YES,torch.nn.parallel.distributed,1.12.0.dev20220419+cu113,11.3
pytorch,https://github.com/pytorch/pytorch/issues/76025,2022-04-19T09:21:30Z,Numerical instability: matrix multiplication got different results on cpu and gpu ,YES,torch.tensor,pytorch 1.10.1,11.3.1
pytorch,https://github.com/pytorch/pytorch/issues/76012,2022-04-19T02:27:00Z,torch.nn.LayerNorm is very slow on GPU (much slower than a custom LayerNorm version in the ConvNext model),YES,torch.nn.LayerNorm,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/75912,2022-04-15T20:21:57Z,"interaction with psychopy during imports, script exits with: free(): invalid pointer. Aborted (core dumped)",YES,<torch>,1.11.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/75903,2022-04-15T18:48:12Z,torch.jit.script'd function very slow on first invocation on latest nightly,YES,torch.jit.script,1.12.0.dev20220415-py3.9_cuda11.3_cudnn8.3.2_0,"CUDA 11.3, CUDA 11.1"
pytorch,https://github.com/pytorch/pytorch/issues/75862,2022-04-15T08:04:15Z,LayerNorm and GroupNorm with num_groups=1 not equivalent,YES,"<torch.nn.LayerNorm>
<torch.nn.GroupNorm>",1.11.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/75788,2022-04-14T09:50:58Z,`torch.jit.script` Script functions do return `requires_grad = False` if `torch.no_grad()` has been used,YES,"<torch.jit.script>
<torch.no_grad()>",1.11.0+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/75733,2022-04-13T14:27:35Z,"`torch.sum, prod, cumsum, cumprod, sparse.sum` INTERNAL ASSERT FAIL",YES,"torch.sum  
torch.prod  
torch.cumsum  
torch.cumprod  
torch.sparse.sum",1.11.0,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a CUDA version based on the given information."
pytorch,https://github.com/pytorch/pytorch/issues/75662,2022-04-12T12:43:52Z,"multiprocessing and torch.tensor, Cannot allocate memory error",YES,torch.multiprocessing,1.10.2,CUDA runtime version: 11.6.112
pytorch,https://github.com/pytorch/pytorch/issues/75652,2022-04-12T07:11:58Z,Op segfaults with ForwardAD and Subclassed Tensor as Tangent,YES,<torch.autograd.forward_ad>,master,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/75549,2022-04-09T01:52:41Z,`torch.linalg.lstsq` raises `CUBLAS_STATUS_EXECUTION_FAILED` for large `B` in CUDA tensors,YES,torch.linalg.lstsq,1.10.1+cu113,11.3
pytorch,https://github.com/pytorch/pytorch/issues/75503,2022-04-08T07:49:20Z,make_dual errors out when primal is a Tensor and tangent is a subclass Tensor,YES,torch.autograd.forward_ad.make_dual,master,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/75495,2022-04-08T05:23:51Z,Forward AD convolution fails for the empty backend ,YES,torch._C._ConvBackend.Empty,main,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/75480,2022-04-08T02:16:32Z,IndexError: Caught IndexError in replica 0 on device 0.,YES,<torch.nn.ParameterList>,pytorch==1.7.1,11.0
pytorch,https://github.com/pytorch/pytorch/issues/75427,2022-04-07T14:37:13Z,`torch.cuda.is_bf16_supported()` seem to not work properly,YES,torch.cuda.is_bf16_supported(),1.11.0+cu113,11.6
pytorch,https://github.com/pytorch/pytorch/issues/75371,2022-04-06T22:03:27Z,"NVFuser bad ""reshape"" performance",YES,torch.aten.reshape,master,"The description does not explicitly mention a specific CUDA version. Therefore, based on the information provided, it is not possible to identify a CUDA version from the issue description."
pytorch,https://github.com/pytorch/pytorch/issues/75363,2022-04-06T21:39:33Z,conv3d has numerical issue where same input produces output that are not bit-wise identical,YES,<torch.ops.aten._convolution>,1.11.0a0+17540c5,11.6
pytorch,https://github.com/pytorch/pytorch/issues/75342,2022-04-06T18:16:29Z,`torch.fx.operator_schemas.normalize_function` is too permissive,YES,torch.fx.operator_schemas.normalize_function,1.12.0.dev20220325,11.3
pytorch,https://github.com/pytorch/pytorch/issues/75240,2022-04-05T00:45:49Z,Large cumulative sums appear to be nondeterministic. ,YES,torch.Tensor.cumsum,1.11.0,11.6
pytorch,https://github.com/pytorch/pytorch/issues/75198,2022-04-04T16:36:02Z,`torch.cuda.get_device_name` fails to identify RTX 3090 Ti ,YES,torch.cuda.get_device_name,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/75147,2022-04-02T18:00:03Z,Dataloader hangs. Potential deadlock with `set_num_threads` in worker processes?,YES,torch.set_num_threads,1.11.0+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/75090,2022-04-01T06:55:38Z,torch.fx deepcopy bug,YES,torch.fx.symbolic_trace,1.8.0,CUDA version is not explicitly mentioned in the issue description.
pytorch,https://github.com/pytorch/pytorch/issues/75031,2022-03-31T18:51:32Z,comm hook error in BWD pass,YES,<torch.distributed>,1.11.0a0+17540c5,11.6
pytorch,https://github.com/pytorch/pytorch/issues/75025,2022-03-31T16:57:06Z,torch.cuda.init() unstacks existing CUDA contexts,YES,<torch.cuda.init>,1.10.1,"11.2, 11.6.1"
pytorch,https://github.com/pytorch/pytorch/issues/74909,2022-03-29T15:47:08Z,torch.package fails to import if dataclass and __future__.annotations present,YES,<torch.package>,1.11.0+cu113,11.3
pytorch,https://github.com/pytorch/pytorch/issues/74901,2022-03-29T07:50:03Z,"torch.fx.wrap will not work, when encapsulate the code",YES,"torch.fx.wrap
torch.fx.symbolic_trace",1.13.0,"The issue description does not mention a specific CUDA version. Therefore, the response would be:

N/A"
pytorch,https://github.com/pytorch/pytorch/issues/74842,2022-03-28T17:58:30Z,error in quantization by quantize_fx.prepare_fx,YES,<torch.quantization.quantize_fx.prepare_fx>,<1.10.0>,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version to provide."
pytorch,https://github.com/pytorch/pytorch/issues/74809,2022-03-27T13:53:47Z,Incorrect results for `torch.distributed.gather` for tensor created from permuted NumPy array,YES,<torch.distributed.gather>,1.11.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/74771,2022-03-25T19:27:05Z,[bug] `torch.multinomial` should throw error as documented,YES,torch.multinomial,1.12.0a0+git2e22c66,"The issue description does not mention any specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/74605,2022-03-23T09:45:16Z,torch.profiler.profile does't work well for CPU model when not using torch.profiler.schedule ,YES,"torch.profiler.profile  
torch.profiler.schedule  ",1.12.0a0+git6fceada,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/74420,2022-03-18T23:13:11Z,`torch.histogram` has wrong output dtype and doesn't support integer inputs,YES,torch.histogram,<PyTorch 1.9.0>,"The issue description provided does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/74301,2022-03-16T15:11:14Z,Error in lobpcg when using largest=False,YES,<torch.lobpcg>,1.10.0+cu111,11.1
pytorch,https://github.com/pytorch/pytorch/issues/74146,2022-03-13T11:56:36Z,RuntimeError: Connection reset by peer when backened by NCCL,YES,<torch.distributed.init_process_group>,1.10.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/74115,2022-03-11T18:17:58Z,DistributedDataParallel high peak memory usage with find_unused_parameters=True,YES,<torch.nn.parallel.DistributedDataParallel>,1.11.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/74112,2022-03-11T17:26:42Z,torch.jit.script does not custom state_dicts,YES,torch.jit.script,1.11.0,None
pytorch,https://github.com/pytorch/pytorch/issues/74095,2022-03-11T10:12:45Z,torchscript RNN modules cannot move between GPU,YES,"torch::jit::load  
torch::flip  
torch::lstm  
torch::nn::modules::rnn",1.11.0,CUDA 11.3
pytorch,https://github.com/pytorch/pytorch/issues/74058,2022-03-10T21:27:58Z,torch.fx.symbolic_trace is non-deterministic,YES,torch.fx.symbolic_trace,1.11.0,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/74034,2022-03-10T14:26:04Z,[torch.onnx] ONNX export failed on adaptive_avg_pool2d because input size not accessible not supported,YES,"<torch.onnx>  
<torch.onnx.utils>  
<torch.onnx.symbolic_helper>  
<torch.adaptive_avg_pool2d>",1.10.0+cu111,11.1
pytorch,https://github.com/pytorch/pytorch/issues/74014,2022-03-10T03:18:39Z,torch.utils.data.Dataset combined with pycuda issue,YES,torch.utils.data.Dataset,1.7.0,11.0
pytorch,https://github.com/pytorch/pytorch/issues/73870,2022-03-07T20:32:16Z,max_pool1d() returns when given invalid large `kernel_size` inputs,YES,torch.max_pool1d,1.10.1+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/73848,2022-03-07T09:22:50Z,torch.distributions.multinomial.Multinomial (an example mistake of docs)?,YES,<torch.distributions.multinomial.Multinomial>,1.10.2,11.3
pytorch,https://github.com/pytorch/pytorch/issues/73841,2022-03-06T22:18:14Z,max_pool1d(): `RuntimeError: [enforce fail at CPUAllocator.cpp:68] . DefaultCPUAllocator: can't allocate memory`,YES,torch.max_pool1d,1.10.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/73840,2022-03-06T21:57:33Z,"max_pool1d(): argument 'dilation' must be tuple of ints, but found element of type int at pos 1",YES,torch.max_pool1d,1.10.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/73832,2022-03-06T04:10:45Z,Legacy sparse tensor constructor (e.g. torch.cuda.sparse.FloatTensor) silently ignores device kwarg,YES,torch.cuda.sparse.FloatTensor,master,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version from the text provided."
pytorch,https://github.com/pytorch/pytorch/issues/73792,2022-03-04T17:03:58Z,Bug: torch.distributions.mixture_same_distribution._pad_mixture_dimension,YES,<torch.distributions.mixture_same_distribution._pad_mixture_dimension>,1.8.0,10.2.89
pytorch,https://github.com/pytorch/pytorch/issues/73661,2022-03-02T16:54:55Z,`torch.fx.ProxyableClassMeta` does not work if Proxy objects are not included in constructor arguments,YES,torch.fx.ProxyableClassMeta,1.10.2,None
pytorch,https://github.com/pytorch/pytorch/issues/73638,2022-03-02T08:10:43Z,addcdiv is failing the ASAN test for zero divisors,YES,torch.addcdiv,<Library Version> 1.13.0,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced."
pytorch,https://github.com/pytorch/pytorch/issues/73608,2022-03-01T23:09:45Z,`torch.jit.load` fails when function parameters use non-ASCII characters,YES,torch.jit.load,1.10.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/73568,2022-03-01T16:20:12Z,Bug when using `nn.Linear`,YES,<torch.nn.Linear>,1.10.2,10.2
pytorch,https://github.com/pytorch/pytorch/issues/73515,2022-02-28T16:28:24Z,`torch.distributed.nn.functional.all_gather`: Tensors must be contiguous,YES,"torch.distributed.nn.functional.all_gather  
torch.distributed.nn.functional._AlltoAll",1.10.2+cu113,11.3
pytorch,https://github.com/pytorch/pytorch/issues/73504,2022-02-28T12:52:37Z,`ge` and `div` behaves differently when converting an overflow number,YES,"torch.div  
torch.ge",1.10.1,The description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/73501,2022-02-28T11:15:12Z,`index_copy` has different index behavior with `index_fill`,YES,"torch.index_copy  
torch.index_fill",1.10.1,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/73469,2022-02-26T07:16:24Z,torch.fx failed when tracing functions from other Libs.,YES,<torch.fx>,github master branch,"The issue description does not specify any CUDA version. Therefore, I cannot provide a CUDA version that is referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/73332,2022-02-24T00:35:04Z,BatchNorm with LSTM in DistributedDataParallel throws one of the variables needed for gradient computation has been modified by an inplace operation,YES,"<torch.nn.BatchNorm1d>  
<torch.nn.LSTMCell>  
<torch.nn.SyncBatchNorm>  
<torch.nn.parallel.DistributedDataParallel>  ",1.9.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/73196,2022-02-21T21:26:48Z,`torch.pow` errors out on specific input,YES,torch.pow,1.11.0,"The issue description does not explicitly mention a CUDA version. Therefore, no CUDA version can be identified from the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/73190,2022-02-21T17:37:00Z,Segmentation fault in max_pool1d,YES,<torch.max_pool1d>,1.9.0a0+git4d36e60,None
pytorch,https://github.com/pytorch/pytorch/issues/73186,2022-02-21T17:31:43Z,Segmentation fault in fractional_max_pool3d,YES,<torch._C._nn.fractional_max_pool3d>,1.9.0a0+git4d36e60,None
pytorch,https://github.com/pytorch/pytorch/issues/73185,2022-02-21T17:30:40Z,Segmentation fault in fractional_max_pool2d,YES,torch._C._nn.fractional_max_pool2d,1.9.0a0+git4d36e60,None
pytorch,https://github.com/pytorch/pytorch/issues/73182,2022-02-21T17:28:15Z,Segmentation fault in _sobol_engine_scramble_,YES,<torch._sobol_engine_scramble_>,1.9.0a0+git4d36e60,None
pytorch,https://github.com/pytorch/pytorch/issues/73181,2022-02-21T17:26:58Z,Segmentation fault in _sobol_engine_initialize_state_,YES,torch._sobol_engine_initialize_state_,1.9.0a0+git4d36e60,None
pytorch,https://github.com/pytorch/pytorch/issues/73180,2022-02-21T17:25:46Z,Segmentation fault in _sobol_engine_ff_,YES,torch._sobol_engine_ff_,1.9.0a0+git4d36e60,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/73154,2022-02-20T10:20:12Z,`max_unpool2d` returns a tensor with negative dimension,YES,torch.nn.functional.max_unpool2d,1.10.1,"The issue description does not mention a specific CUDA version. Therefore, the answer is:

<No CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/73065,2022-02-18T07:36:53Z,`index_fill_` accepts wrong dtype for meta tensors,YES,torch.index_fill_,1.13.0,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/72911,2022-02-16T12:13:19Z,Vectorized Jacobian and Hessian errors with ffts,YES,"<torch.autograd.functional.hessian>
<torch.fft.irfft>
<torch.fft.rfft>
<torch.autograd.functional.jacobian>
<torch.fft.irfft>
<torch.fft.rfft>",1.10.1+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/72835,2022-02-15T00:35:42Z,torch.distributed hangs at barrier(),YES,torch.distributed,1.10.2,CUDA version 11.3
pytorch,https://github.com/pytorch/pytorch/issues/72821,2022-02-14T23:43:38Z,"`{Batch,Instance}Norm{1,2,3}d` works when `num_features != C`!",YES,"torch.nn.BatchNorm2d  
torch.nn.InstanceNorm2d  
torch.nn.LazyBatchNorm2d",1.10.1,"The issue description does not mention any specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/72525,2022-02-08T18:00:16Z,KL divergence between two Continuous Bernoulli is negative,YES,"<torch.distributions.kl_divergence>
<torch.distributions.ContinuousBernoulli>",1.10.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/72498,2022-02-08T06:21:36Z,torch.jit.script + torch.split + onnxruntime incompatibility,YES,"torch.jit.script  
torch.split  
torch.onnx.export  ",1.10.0a0+git36449ea,11.2
pytorch,https://github.com/pytorch/pytorch/issues/72418,2022-02-07T10:57:21Z,input.dim() == 4 INTERNAL ASSERT FAILED mkldnn/Pooling.cpp:201,YES,<torch._C._nn.adaptive_avg_pool2d>,1.4.0,10.1
pytorch,https://github.com/pytorch/pytorch/issues/72341,2022-02-04T17:55:53Z,InstanceNorm doesn't preserve memory format,YES,<torch.nn.InstanceNorm2d>,1.10.1+cu113,11.3
pytorch,https://github.com/pytorch/pytorch/issues/71784,2022-01-25T22:21:52Z,[JIT] addmm differs from eager mode,YES,torch.addmm,1.11.0a0+gitad36af8,None
pytorch,https://github.com/pytorch/pytorch/issues/71774,2022-01-25T18:11:30Z,matmul returns uninitialized memory for int64 tensors with inner dimension of zero,YES,"torch.matmul  
torch.__rmatmul__  
torch.addmm_decomposed  
torch.linalg_multi_dot",<Library Version> 1.13.0,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/71689,2022-01-24T02:08:52Z,torch.distributions.categorical.Categorical does not work with 0 batch size,YES,torch.distributions.categorical.Categorical,1.10.1+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/71678,2022-01-22T22:13:28Z,torch.bmm backward with sparse input,YES,torch.bmm,1.9.1+cu111,11.1
pytorch,https://github.com/pytorch/pytorch/issues/71636,2022-01-21T14:09:14Z,`torch.median` will return -2147483648 when input is an empty tensor,YES,torch.median,1.10.1,The description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/71635,2022-01-21T14:03:24Z,"`torch.nn.functional.{instance, batch}_norm` trigger INTERNAL ASSERT FAILED when input is empty tensor with `complex32`",YES,"torch.nn.functional.instance_norm
torch.nn.functional.batch_norm",1.10.1,"The issue description does not explicitly mention a CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/71629,2022-01-21T06:37:39Z,"`torch.{max,min}` have strange error message when `input.numel()==0`",YES,"torch.max  
torch.min  
torch.median",1.10.1,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/71541,2022-01-20T06:23:00Z,"`torch.sub` behaves differently with `add`, `mul`, `div`",YES,"torch.sub  
torch.add  
torch.mul  
torch.div  ",1.10.1,"The description does not explicitly mention a CUDA version. Therefore, I cannot provide a CUDA version based on the provided information."
pytorch,https://github.com/pytorch/pytorch/issues/71479,2022-01-19T16:37:46Z,[numpy compat] torch.stack and torch.tensor doesn't support nested list+tensors (NumPy does support) - at least document the difference in the error message,YES,"torch.stack
torch.tensor",1.9.1,"The issue description does not mention a specific CUDA version. It only includes the PyTorch version (1.9.1). Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/71477,2022-01-19T15:51:54Z,"`torch.cum{min,max}, torch.sort, argsort` do not check the `dim` when the input is 0-d tensor",YES,"torch.cummin
torch.cummax
torch.sort
torch.argsort",1.10.1,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/71470,2022-01-19T07:52:50Z,torch.jit.script failed to compile nn.MultiheadAttention when specifying the kdim and vdim parameters.,YES,"torch.jit.script  
torch.nn.MultiheadAttention  
F.multi_head_attention_forward",1.9.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/71414,2022-01-18T17:19:37Z,Gamma distribution returns some wrong extreme values ,YES,torch.distributions.Gamma,1.9.0+cu102,"10.2, 10.1, 11.4"
pytorch,https://github.com/pytorch/pytorch/issues/71288,2022-01-13T23:51:05Z,`memory_format` argument isn't supported in torchscript for tensor.is_contiguous(),YES,torch.is_contiguous,1.10.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/71222,2022-01-12T17:16:23Z,torch.linalg.lstsq is nondeterministic,YES,<torch.linalg.lstsq>,1.10.0+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/71204,2022-01-12T08:41:06Z,`torch.diag` unexpectedly fails,YES,"torch.diag
torch.diagonal",1.10.1,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/71084,2022-01-10T08:38:16Z,"`torch.{inverse,cholesky}` have wrong shape check of square matrices",YES,"torch.inverse  
torch.cholesky",1.10.1,"The issue description does not mention a specific CUDA version. Therefore, the answer is:

<Cuda version not mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/71082,2022-01-10T08:28:08Z,`torch.combinations` will allocate large memory when `r` is greater than the length of input,YES,torch.combinations,1.10.1,The CUDA version is not explicitly mentioned in the provided issue description.
pytorch,https://github.com/pytorch/pytorch/issues/71078,2022-01-10T06:04:25Z,"`torch.nn.{Constant,Zero}Pad` unexpectedly fail",YES,"torch.nn.ConstantPad2d  
torch.nn.ZeroPad2d",1.10.1,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/71059,2022-01-08T15:37:31Z,`torch.scatter` will return random value when `input` is empty tensor,YES,torch.scatter,1.10.1,The CUDA version is not explicitly mentioned in the issue description provided.
pytorch,https://github.com/pytorch/pytorch/issues/71058,2022-01-08T15:18:11Z,`torch.Tensor.where` cannot work when `y` is float,YES,torch.Tensor.where,1.10.1,"The issue description does not explicitly mention a CUDA version. Therefore, no CUDA version can be identified from the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/70954,2022-01-06T23:32:39Z,Training grouped Conv2D is slow,YES,<torch.nn.Conv2d>,1.10.0+cu113,The CUDA version mentioned in the description is cu113.
pytorch,https://github.com/pytorch/pytorch/issues/70702,2022-01-05T15:20:17Z,conv3d padding=`same` gradgradcheck fails on CUDA,YES,torch.autograd.gradgradcheck,<Library Version> 1.9.0,"The issue description does not mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/70701,2022-01-05T15:14:11Z,AdaptiveAvgPool1d - RuntimeError: shmem_size <= sharedMemPerBlockINTERNAL ASSERT FAILED,YES,<torch.AdaptiveAvgPool1d>,torch 1.10.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/70559,2022-01-02T18:01:59Z,Behavior of torch.nn.functional.interpolate with unchanged output size and recompute_scale_factor=False,YES,torch.nn.functional.interpolate,<Library Version> 1.12.0,The description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/70511,2021-12-30T02:55:26Z,Strange behavior of torch.jit.trace when moving parameters across device,YES,<torch.jit.trace>,1.10.1+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/70398,2021-12-25T07:21:20Z,`torch.broadcast_to` can create tensor with negative dimension.,YES,"torch.broadcast_to  
torch.expand",1.10.1,"The issue description does not mention a specific CUDA version. Therefore, I cannot provide a CUDA version from the text provided."
pytorch,https://github.com/pytorch/pytorch/issues/70397,2021-12-25T04:58:50Z,`torch.empty_strided` works when the stride is negative!,YES,torch.empty_strided,1.10.1,"The issue description does not explicitly mention a CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/70391,2021-12-24T13:28:38Z,linalg.lstsq INTERNAL ASSERT FAILED,YES,torch.linalg.lstsq,1.11.0.dev20211120,11.3
pytorch,https://github.com/pytorch/pytorch/issues/70388,2021-12-24T11:44:03Z,RuntimeError: tensor has too many (>25) dims when permuting tensor with GPU backend,YES,"<torch.permute>  
<torch.tensordot>",1.10.0+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/70386,2021-12-24T07:14:34Z,AT_ASSERT fail with DataLoaderOptions().drop_last(),YES,torch.DataLoaderOptions.drop_last,1.10.0+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/70352,2021-12-23T16:11:58Z,Rprop Optimizer: UnboundLocalError: local variable 'step_size_min' referenced before assignment,YES,<torch.optim.Rprop.step>,1.10.1+cu102,cu102
pytorch,https://github.com/pytorch/pytorch/issues/70348,2021-12-23T12:33:40Z,nan return by nn.CrossEntropyLoss when all the labels are ignore_index in torch 1.11,YES,torch.nn.CrossEntropyLoss,1.11.0a0+b6df043,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version based on the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/70347,2021-12-23T11:59:04Z,boolean mask + ellipsis lead to incorrect indexing,YES,<torch.BoolTensor>,1.10.1,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/70342,2021-12-23T03:31:37Z,Empty or NaN data pollute gradient even if they are not involved during backward,YES,torch.optim.AdamW,PyTorch 1.8,"The issue description does not explicitly mention a CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/70102,2021-12-17T11:47:32Z,torch jit script segm fault,YES,torch.jit.script,1.11.0.dev20211217+cu111,cu111
pytorch,https://github.com/pytorch/pytorch/issues/70047,2021-12-16T14:13:29Z,[JIT] Cannot `jit.export` a `@staticmethod`,YES,"<torch.jit.export>  
<torch.jit.script>",1.10.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/69972,2021-12-15T13:16:02Z,torchscript does not work with `SyncBatchNorm` layers,YES,"torch.nn.SyncBatchNorm  
torch.jit.script",1.11.0a0+gita31aea8,11.1
pytorch,https://github.com/pytorch/pytorch/issues/69931,2021-12-14T21:56:39Z,[JIT] torch.exp roughly 20 times slower in TorchScript vs. PyTorch,YES,torch.exp,1.10.0,11.2
pytorch,https://github.com/pytorch/pytorch/issues/69912,2021-12-14T17:50:54Z,"torch.nn.functional.ctc_loss with invalid input produce NaN or infinity gradient, while the batch entries are fine",YES,torch.nn.functional.ctc_loss,1.11.0.dev20211025+cpu,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/69831,2021-12-13T12:26:06Z,Incorrect error for integer `out=` dtypes when a float is expected.,YES,torch.linalg.svd,<Library Version> 1.13.0,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/69531,2021-12-07T15:33:29Z,"`(svd|pca)_lowrank`: backward is unstable when for a matrix `A`, the parameter `q` is set to a value `q > rank(A)`.",YES,torch.svd_lowrank,<Library Version> 1.9.0,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/69435,2021-12-05T19:52:10Z,"torch.is_tensor(obj) doesn't work with JIT, despite the fact that isinstance(obj, Tensor) already works with JIT",YES,<torch.is_tensor>,1.10.0+cu111,11.1
pytorch,https://github.com/pytorch/pytorch/issues/69433,2021-12-05T14:34:42Z,`torch.transpose` should raise an error when indexing 0 for 0 dimensional tensor.,YES,torch.transpose,1.10.0,11.1
pytorch,https://github.com/pytorch/pytorch/issues/69408,2021-12-04T02:04:13Z,`torch.hstack` should raise an error when tensor is 0 dimensional,YES,torch.hstack,1.10.0,11.1
pytorch,https://github.com/pytorch/pytorch/issues/69353,2021-12-03T08:04:23Z,Pruning `torch.nn.MultiheadAttention` causes RuntimeError,YES,"<torch.nn.MultiheadAttention>
<torch.nn.utils.prune.global_unstructured>",1.10.0+cu102,CUDA 10.2
pytorch,https://github.com/pytorch/pytorch/issues/69348,2021-12-03T06:55:32Z,`torch.sspaddmm` should broadcast the input tensor,YES,torch.sspaddmm,"1.10.0, 1.9.0",11.1
pytorch,https://github.com/pytorch/pytorch/issues/69325,2021-12-02T22:31:31Z,`nn.functional.fractional_max_pool2d` and `nn.functional.fractional_max_pool3d` produce incorrect output on non-contiguous inputs,YES,"torch.nn.functional.fractional_max_pool2d
torch.nn.functional.fractional_max_pool3d",1.13.0,"The issue description provided does not mention any specific CUDA version. Therefore, no CUDA version can be identified from the text."
pytorch,https://github.com/pytorch/pytorch/issues/69203,2021-12-01T09:04:14Z,Error in SVD cusolver on Linux,YES,"torch.linalg.svd  
torch.linalg.svdvals",1.10.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/68978,2021-11-29T09:02:48Z,SequentialLR cannot be used with ReduceLROnPlateau due to .step() not allowing for optional arguments,YES,"<torch.SequentialLR>
<torch.ReduceLROnPlateau>",1.10.0,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/68968,2021-11-29T02:07:56Z,torch.jit.trace with pack_padded_sequence  cannot do dynamic batch,YES,"<torch.jit.trace>  
<torch.nn.utils.rnn.pack_padded_sequence>  
<torch.nn.utils.rnn.pad_packed_sequence>  
<torch.jit.script>  ",1.8.1,10.1
pytorch,https://github.com/pytorch/pytorch/issues/68966,2021-11-28T22:10:39Z,`torch.ldexp` generated tests fail on call to `torch.mul`,YES,"torch.ldexp  
torch.mul",1.12.1,"The description provided does not mention any specific CUDA version. Therefore, I cannot provide a CUDA version based on the given information."
pytorch,https://github.com/pytorch/pytorch/issues/68622,2021-11-19T00:41:42Z,"torch.{tensor, Tensor, LongTensor, ...} isn't captured under `enable_python_mode()`",YES,torch.tensor,<PT master>,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/68610,2021-11-18T20:26:08Z,`torch.unique_consecutive`: passing positional optional arguments results in empty tensors,YES,torch.unique_consecutive,1.12.0,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/68595,2021-11-18T16:16:21Z,Inconsistent list indexing behavior,YES,torch.Tensor,1.10.0,11.5
pytorch,https://github.com/pytorch/pytorch/issues/68513,2021-11-17T08:15:05Z,torch.fx cannot trace torch.Size() properly,YES,"torch.fx
torch.Tensor.size
torch.zeros
torch.Tensor.view",1.10.0,"The issue description does not mention a specific CUDA version. If you are looking for the CUDA version associated with PyTorch 1.10.0, it is typically CUDA 11.3, but this is not explicitly stated in the issue. Therefore, the answer is:

<No CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/68420,2021-11-16T11:45:43Z,`nn.functional.max_unpool(2|3)d`: failing shape check for correct inputs (with `dilation > 1`) with specified `output_size`,YES,torch.nn.functional.max_unpool2d,1.8.0,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/68337,2021-11-15T12:16:39Z,"`nn.functional.max_unpool{n}d`: shape checks fail with `output_size=(C, ...)`.",YES,"<torch.nn.functional.max_pool1d>
<torch.nn.functional.max_unpool1d>",1.11.0a0+git4a58729,11.0
pytorch,https://github.com/pytorch/pytorch/issues/68323,2021-11-14T10:42:09Z,sparse.mm: CUDA error: internal error when calling `cusparseSpGEMM_workEstimation [...]`,YES,torch.sparse.mm,1.10.0+cu113,11.4
pytorch,https://github.com/pytorch/pytorch/issues/68301,2021-11-13T00:03:32Z,[FX] [BUG] Tensor.{inplace_method}_(.) is eliminated as dead code,YES,torch.Tensor.mul_,1.10.0+cu113,11.3
pytorch,https://github.com/pytorch/pytorch/issues/68169,2021-11-11T10:48:40Z,torch.histogram: Sum of PDFs is num of bins instead of 1,YES,torch.histogram,1.10.0,11.3
pytorch,https://github.com/pytorch/pytorch/issues/67983,2021-11-08T06:31:16Z,torch.jit.export does not working,YES,torch.jit.export,1.10.0+cpu,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/67970,2021-11-07T15:18:50Z,Error when using torch.fx on bert,YES,torch.fx.symbolic_trace,1.12.0,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/67740,2021-11-03T03:16:50Z,`torch.utils.data.random_split` example broken in 1.10,YES,torch.utils.data.random_split,1.10.0+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/67674,2021-11-02T09:48:44Z,torch.jit.script fails to find attribute '_modules' of nn.Module,YES,<torch.jit.script>,1.9.0,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/67651,2021-11-01T23:47:47Z,MultiHeadAttention in quantizable seems incorrect with batch_first=True,YES,torch.nn.MultiHeadAttention,1.12.0,The issue description provided does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/67481,2021-10-28T21:07:25Z,Batchnorm2D does behaves differently with different shapes when batch_size=1,YES,torch.nn.BatchNorm2d,1.9.0,11.1
pytorch,https://github.com/pytorch/pytorch/issues/67463,2021-10-28T19:25:28Z,`test_forward_mode_AD` hangs for `nn.functional.cosine_embedding_loss`,YES,torch.nn.functional.cosine_embedding_loss,viable/strict,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/67462,2021-10-28T19:20:47Z,Jacobian mismatch for `nn.functional.ctc_loss`,YES,torch.nn.functional.ctc_loss,1.10.0,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/67324,2021-10-27T08:20:24Z,torch.stft - fill_cuda not implemented for ComplexHalf,YES,torch.stft,1.10.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/66992,2021-10-21T03:29:24Z,Bug? :Run torch.unique twice get different running time? ,YES,"torch.unique  
torch.unique_consecutive",1.9.0,11.2
pytorch,https://github.com/pytorch/pytorch/issues/66963,2021-10-20T20:31:54Z,Significantly difference in execution time when convolution is run as nn.Conv2d and as nn.Sequential,YES,torch.nn.Conv2d,1.9.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/66868,2021-10-19T14:41:19Z,torch.triu behaves differently when diagonal out of range,YES,torch.triu,1.9.1,11.3
pytorch,https://github.com/pytorch/pytorch/issues/66751,2021-10-16T02:52:15Z,torch.nn.functional.embedding behave differently in two cases of cpu and cuda,YES,torch.nn.functional.embedding,1.9.0,11.4
pytorch,https://github.com/pytorch/pytorch/issues/66750,2021-10-16T02:40:55Z,torch.nn.EmbeddingBag behave differently in two cases of cpu and cuda,YES,torch.nn.EmbeddingBag,1.9.0,11.4
pytorch,https://github.com/pytorch/pytorch/issues/66707,2021-10-15T18:30:53Z,`layer_norm` needs to be done in fp32 for fp16 inputs,YES,torch.layer_norm,1.9.1+cu111,11.1
pytorch,https://github.com/pytorch/pytorch/issues/66504,2021-10-12T20:27:16Z,BatchNorm runtimeError: one of the variables needed for gradient computation has been modified by an inplace operation,YES,"<torch.nn.BatchNorm1d>  
<torch.nn.parallel.DistributedDataParallel>  
<torch.nn.SyncBatchNorm>  ",1.12.0,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/66197,2021-10-06T12:12:49Z,`torch.fx.replace_pattern` doesn't work with untraceable wrapped functions,YES,torch.fx.replace_pattern,1.11.0.dev20210930,"The issue description does not mention a specific CUDA version. Therefore, the answer is:

<CUDA version>"
pytorch,https://github.com/pytorch/pytorch/issues/66065,2021-10-04T06:35:51Z,torch.jit.script fails to cast explicit Optional parameter to bool,YES,torch.jit.script,1.9.0,not-used
pytorch,https://github.com/pytorch/pytorch/issues/65907,2021-09-30T08:28:19Z,Inplace error of BatchNorm layer in DistributedDataParallel module,YES,torch.nn.BatchNorm2d,1.9.1,11.1
pytorch,https://github.com/pytorch/pytorch/issues/65890,2021-09-30T07:15:30Z,`pad_packed_sequence` is not exactly the inverse of `pack_padded_sequence`,YES,"torch.nn.utils.rnn.pack_padded_sequence  
torch.nn.utils.rnn.pad_packed_sequence",1.9.1,10.2.89
pytorch,https://github.com/pytorch/pytorch/issues/65698,2021-09-27T16:43:13Z,torch.onnx.export error ,YES,torch.onnx.export,torch==1.2.0,10.0
pytorch,https://github.com/pytorch/pytorch/issues/65683,2021-09-27T09:45:27Z,`torch.nn.functional.l1_loss` fails gradgradcheck for complex inputs,YES,torch.nn.functional.l1_loss,1.13.0,The description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/65653,2021-09-25T10:43:30Z,[JIT] torch.load(model) fails for Unicode Variable Names.,YES,"<API Name>
torch.jit.load",1.9.1,11.1
pytorch,https://github.com/pytorch/pytorch/issues/65607,2021-09-24T09:10:55Z,torch.dtype is int in JIT instead of torch.dtype,YES,torch.dtype,1.9,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/65473,2021-09-22T15:29:05Z,API torch.ops.image.read_file reports RuntimeError - No such operator image::read_file,YES,torch.ops.image.read_file,1.9.0,CUDA version: 11.2
pytorch,https://github.com/pytorch/pytorch/issues/65400,2021-09-21T12:23:02Z,torch.sparse.sum on scalar sparse tensor fails when dim is specified,YES,torch.sparse.sum,1.10.0a0,"The issue description does not mention any specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/65392,2021-09-21T07:36:12Z,torch.sparse.sum result has wrong dtype when reducing over all dimensions,YES,torch.sparse.sum,1.10.0a0,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/65098,2021-09-15T21:34:12Z,Bug: torch.jit.load cannot load from IOBuffer,YES,torch.jit.load,1.9.0,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/65050,2021-09-15T09:11:11Z,torch.cross precision problem,YES,torch.cross,1.9,11.1
pytorch,https://github.com/pytorch/pytorch/issues/64818,2021-09-10T15:22:34Z,Inconsistent NaN handling by cholesky between CPU and CUDA,YES,"torch.linalg.cholesky
torch.linalg.cholesky_ex",1.9.0,+cu102
pytorch,https://github.com/pytorch/pytorch/issues/64794,2021-09-10T02:21:26Z,linalg.lstsq out variant fails internal assert because it uses non-inplace view op for some inputs,YES,torch.linalg.lstsq,<Library Version> 1.13.0,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/64525,2021-09-05T05:32:07Z,nn.TransformerEncoder - all nan values issues when src_key_padding_mask provided,YES,torch.nn.TransformerEncoder,<Library Version> 1.9.0,"The description provided does not mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/64292,2021-08-31T18:12:20Z,torch.empty_like not taking None as layout?,YES,torch.empty_like,master,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/64145,2021-08-29T19:37:28Z,F.conv2d: confusing error message when using uint8 input instead of float32,YES,torch.nn.functional.conv2d,<1.8.0>,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/64107,2021-08-27T19:48:12Z,as_tensor and negative strided np arrays,YES,<torch.as_tensor>,1.10.0,The issue description provided does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/64079,2021-08-27T03:26:49Z,torch.equal does not support sparse tensors,YES,torch.equal,1.10.0a0+git6f899c1,11.1
pytorch,https://github.com/pytorch/pytorch/issues/64041,2021-08-26T18:37:08Z,torch.unique acting up for a binary tensor,YES,torch.unique,1.9.0+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/63958,2021-08-25T15:50:10Z,torch.gather with sparse_grad=True does not work with SGD optimizer with momentum; gives bad error message,YES,"torch.gather
torch.optim.SGD",1.9.0+cpu,11.2.142
pytorch,https://github.com/pytorch/pytorch/issues/63870,2021-08-24T18:21:59Z,torch median / nanmedian w/ nans speed,YES,torch.nanmedian,1.8.1,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/63837,2021-08-24T05:37:44Z,`torch.scatter` doesn't fail correctly on CUDA (memory overlap),YES,torch.scatter,"<Library Version>
master","The issue description does not explicitly mention a specific CUDA version. Therefore, it is not possible to provide a CUDA version based solely on the given text."
pytorch,https://github.com/pytorch/pytorch/issues/63802,2021-08-23T20:46:07Z,[torch.distributed.launch|run] Hangs on SIGINT when using a TCPStore backed rdzv_backend ,YES,"torch.distributed.launch
torch.distributed.run",1.10.0.dev20210823+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/63726,2021-08-21T13:34:05Z,a problem happened in torch.randperm,YES,<torch.randperm>,1.8.1,cuda: 11.2
pytorch,https://github.com/pytorch/pytorch/issues/63720,2021-08-21T00:47:17Z,Torch.save with zip serialization causes memory bloat,YES,torch.save,1.9.0+cu102,11.0
pytorch,https://github.com/pytorch/pytorch/issues/63648,2021-08-20T17:02:15Z,[JIT] Typing on math.ceil is inaccurate for Scalar input,YES,"<torch.jit.script>  
<torch.ceil>  
<torch.item>  ",<Library Version> 1.9.0,The description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/63405,2021-08-17T17:07:17Z,torch.arange has issue tracing with bool input,YES,torch.arange,1.13.0,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/63226,2021-08-13T12:24:41Z,"cuda.is_available returns True in pycharms python console, False in code",YES,torch.cuda.is_available,1.9.0+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/63145,2021-08-12T11:50:10Z,torch.jit.trace does not work if there is an autograd in the function,YES,"torch.jit.trace
torch.autograd.grad",1.9.0,N/A
pytorch,https://github.com/pytorch/pytorch/issues/63138,2021-08-12T08:40:54Z,Sign in slogdet is set to requires_grad = False even when using complex numbers. ,YES,torch.linalg.slogdet,1.9.0+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/63124,2021-08-12T00:44:32Z,torch.jit.trace quantized bigbird leads to 0INTERNAL ASSERT FAILED runtime error,YES,torch.jit.trace,1.9.0+cu111,11.0.194
pytorch,https://github.com/pytorch/pytorch/issues/63078,2021-08-11T12:44:50Z,torch.cuda.amp fails with torch.sparse.softmax,YES,torch.sparse.softmax,1.9.0,11.1
pytorch,https://github.com/pytorch/pytorch/issues/63034,2021-08-10T18:36:57Z,[bug] nn.functional.pad (circular) ubsan failure,YES,torch.nn.functional.pad,1.10.0,0
pytorch,https://github.com/pytorch/pytorch/issues/62545,2021-08-01T07:31:00Z,Incosistency with args for `nn.functional.max_poolNd` vs `nn.MaxPoolNd` functions,YES,"<torch.nn.functional.max_pool2d>
<torch.nn.MaxPool2d>",1.10.0a0+git6a6830f,"The issue description does not explicitly mention a CUDA version. Therefore, no CUDA version can be identified from the provided text. 

If you are looking for the CUDA version associated with the PyTorch version mentioned (1.10.0a0+git6a6830f), you might need to refer to the PyTorch documentation or release notes for that specific version to determine the compatible CUDA version."
pytorch,https://github.com/pytorch/pytorch/issues/62542,2021-08-01T05:20:26Z,3 Times memory cost when loading the model to torch.nn.parallel.DistributedDataParallel,YES,torch.nn.parallel.DistributedDataParallel,1.9.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/62508,2021-07-30T21:41:32Z,torch.prod internal asserts when passed a tensor that requires_grad (and a dtype),YES,torch.prod,1.10.0,"The issue description does not mention any specific CUDA version. Therefore, there is no CUDA version to provide."
pytorch,https://github.com/pytorch/pytorch/issues/62381,2021-07-29T06:12:04Z,torch.distributed and subprocess do not work together?,YES,<torch.distributed>,1.8.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/62314,2021-07-28T08:23:11Z,dist.destroy_process_group() don't release master_port when there're at least 2 process groups,YES,<torch.distributed.destroy_process_group>,1.8.1,10.1
pytorch,https://github.com/pytorch/pytorch/issues/62283,2021-07-27T18:51:36Z,c10d and discontiguous tensors with mismatch strides,YES,"<API Name>
torch.distributed.send
torch.distributed.recv","<Library Version>
1.13.0","The issue description does not explicitly mention a specific CUDA version. Therefore, it is not possible to provide a CUDA version based on the information given in the description."
pytorch,https://github.com/pytorch/pytorch/issues/62130,2021-07-23T21:40:01Z,"nn.DataParallel not working on A100 with nvidia-driver 470.42.01, Cuda 11.1, Debian 10",YES,<torch.nn.DataParallel>,1.9.0,11.1
pytorch,https://github.com/pytorch/pytorch/issues/62066,2021-07-22T23:43:27Z,Memory Leak Found in Persistent DataLoader,YES,torch.utils.data.DataLoader,1.9.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/62034,2021-07-22T16:45:33Z,cudnn_batch_norm_backward is extremely imprecise for some input shapes,YES,<torch.nn.BatchNorm2d>,1.10.0a0+gita152c12d,CUDA 11.3
pytorch,https://github.com/pytorch/pytorch/issues/62027,2021-07-22T13:12:23Z,`x.to(memory_format=torch.contiguous_format)` does not always return a contiguous tensor,YES,torch.to,pt master,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/61970,2021-07-21T17:13:28Z,FX incorrectly provides `int` instead of `float` for `value` kwarg of `nn.functional.pad`,YES,torch.nn.functional.pad,1.9.0+cu111,11.1
pytorch,https://github.com/pytorch/pytorch/issues/61919,2021-07-20T17:19:44Z,RuntimeError: !ref.requires_grad()INTERNAL ASSERT FAILED,YES,<torch.jit>,1.9.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/61523,2021-07-12T07:32:06Z,F.nll_loss with 16-bit CUDA tensors and reduction=mean produces NaNs,YES,<torch.nn.functional.nll_loss>,1.8.1,11.1
pytorch,https://github.com/pytorch/pytorch/issues/61410,2021-07-08T12:48:17Z,requires grad get lost during transform.,YES,<torch.Tensor>,1.9.0,11.1
pytorch,https://github.com/pytorch/pytorch/issues/61281,2021-07-06T10:26:33Z,DataLoader with IterativeDataset throws an error when providing a BatchSampler,YES,torch.DataLoader,1.13.0,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/60939,2021-06-29T09:43:20Z,Functional multi_head_attention_forward softmax get nan for fp16 mode,YES,"torch.nn.functional.multi_head_attention_forward
torch.nn.functional.softmax",<1.12.0>,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version to provide."
pytorch,https://github.com/pytorch/pytorch/issues/60844,2021-06-28T02:54:54Z,DDP fails if you have multiple forward passes and a single backwards pass with `find_unused_parameters=True`,YES,"<API Name>
torch.nn.parallel.DistributedDataParallel",1.8.0+cu111,11.1
pytorch,https://github.com/pytorch/pytorch/issues/60541,2021-06-23T14:08:20Z,torch.cuda.Event(blocking=True) doesn't work,YES,torch.cuda.Event,1.8,11.1
pytorch,https://github.com/pytorch/pytorch/issues/60440,2021-06-22T06:09:04Z,DDP with cuda rpc failed with DistributedOptimizer Adagrad,YES,torch.optim.Adagrad,1.9.0.dev20210601+cu111,11.1
pytorch,https://github.com/pytorch/pytorch/issues/60277,2021-06-18T17:02:25Z,Sparse CSR tensor should not accept equal column indices in the same row,YES,torch.sparse_csr_tensor,1.12.0,"The issue description does not explicitly mention a specific CUDA version. However, it references the cuSparse library and its expectation for CSR format regarding column indices being sorted and unique within each row. This information is available in the cuSparse documentation, but without a version number specified in the text provided. 

Therefore, based on the information given, there is no specific CUDA version to provide."
pytorch,https://github.com/pytorch/pytorch/issues/60234,2021-06-18T03:22:20Z,Numerical-reproducibility issue in torch.matmul,YES,<torch.matmul>,1.8.1,11.1
pytorch,https://github.com/pytorch/pytorch/issues/60143,2021-06-17T01:17:19Z,"torch.jit.save gives error - RuntimeError: Could not export Python function call 'NMSop'. Remove calls to Python functions before export. Did you forget to add @script or @script_method annotation? If this is a nn.ModuleList, add it to __constants__",YES,torch.jit.save,1.7.1,11.0
pytorch,https://github.com/pytorch/pytorch/issues/59730,2021-06-09T19:49:51Z,Quantized conv2d with dilation and groups much slower than float32,YES,"torch.nn.Conv2d  
torch.quantization.QuantWrapper  
torch.quantization.get_default_qconfig  
torch.quantization.prepare  
torch.quantization.convert  ",1.8.1,11.1
pytorch,https://github.com/pytorch/pytorch/issues/59530,2021-06-07T02:23:23Z,Batch size is hardcoded using torch.jit.trace with LSTMCell,YES,"<torch.jit.trace>
<torch.nn.LSTMCell>",1.8.1+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/59515,2021-06-05T17:21:40Z,Conv1d with large batch size and half precision in cuda returns incorrect result,YES,torch.nn.Conv1d,1.8.1,11.1
pytorch,https://github.com/pytorch/pytorch/issues/59439,2021-06-04T11:29:14Z,NaN values on torch.nn.functional.conv2d (aarch64),YES,torch.nn.functional.conv2d,1.8.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/59256,2021-06-01T19:04:38Z,ProcessGroupGloo creation crashes when world_size > 150,YES,torch.distributed.init_process_group,<1.10.0>,The description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/58841,2021-05-24T08:50:13Z,[bug] torch.topk sometimes supports `float16` and sometimes doesn't,YES,torch.topk,1.10.0,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/58671,2021-05-20T15:13:47Z,torch.load with dill is unable to unserialize from buffer,YES,torch.load,1.8.0,11.1
pytorch,https://github.com/pytorch/pytorch/issues/58522,2021-05-19T00:44:00Z,DDP grads dont have parity with local training when grads are undefined,YES,torch._C._functions.UndefinedGrad,<Library Version> 1.10.0,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/58474,2021-05-18T09:59:18Z,nn.Upsample result mismatch in 1.1.0a0+828a6a3 and 1.9.0,YES,<torch.nn.Upsample>,1.1.0a0+828a6a3,"10.1.163, 11.2"
pytorch,https://github.com/pytorch/pytorch/issues/58212,2021-05-13T03:01:25Z,`torch.autograd.Function` subclasses *sometimes* throw away custom subclasses,YES,<torch.autograd.Function>,1.8.1+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/58136,2021-05-12T13:16:56Z,scatter_add_ 6000-times slower with int64 compared to int32,YES,torch.scatter_add_,1.7.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/58128,2021-05-12T08:30:44Z,the same input with different batchsize got different precision output,YES,<torch._C._nn.linear>,1.8.1,10.1
pytorch,https://github.com/pytorch/pytorch/issues/58109,2021-05-12T02:03:52Z,torch.jit.trace memory leak,YES,<torch.jit.trace>,1.8.1+cu111,11.1
pytorch,https://github.com/pytorch/pytorch/issues/58037,2021-05-11T11:05:55Z,linalg.eigh and linalg.cholesky UPLO flag breaks in backward,YES,"<torch.linalg.eigh>
<torch.linalg.cholesky>",<Library Version> 1.12.0,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/58005,2021-05-11T00:28:20Z,torch.distributed.nn.all_reduce incorrectly scales the gradient,YES,torch.distributed.nn.all_reduce,1.8.1+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/57796,2021-05-07T03:39:35Z,BatchNorm grad calculation is imprecise,YES,<torch.nn.BatchNorm>,1.8.1,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/57721,2021-05-06T11:19:47Z,nn.TransformerEncoder cannot deal with large negative value even when this value is masked by src_key_padding_mask,YES,<torch.nn.TransformerEncoder>,1.7.1,11.0
pytorch,https://github.com/pytorch/pytorch/issues/57690,2021-05-05T22:26:00Z,torch.cdist returns high diagonal values with CUDA,YES,torch.cdist,1.8.1,11.1
pytorch,https://github.com/pytorch/pytorch/issues/57534,2021-05-04T09:32:34Z,`torch.view_as_complex()` does not work when `storage_offset` is odd,YES,torch.view_as_complex,1.8.1+cu111,11.1
pytorch,https://github.com/pytorch/pytorch/issues/57447,2021-05-02T21:55:18Z,`torch._dirichlet_grad` returns `nan` value on cuda device,YES,torch._dirichlet_grad,1.8.1+cu111,11.1
pytorch,https://github.com/pytorch/pytorch/issues/57418,2021-05-01T18:02:56Z,Poor torch.cat performance in the quantized Unet,YES,torch.cat,1.8.1+cu102,10.2
pytorch,https://github.com/pytorch/pytorch/issues/57121,2021-04-28T12:28:23Z,"einsum ""jk,ijkl->il"" is ~16x slower than numpy",YES,<torch.einsum>,1.9.0.dev20210419,N/A
pytorch,https://github.com/pytorch/pytorch/issues/56896,2021-04-26T03:00:30Z,masked_select is x3 slower than reshaping and index_select,YES,"torch.masked_select  
torch.index_select",1.9.0.dev20210223,10.2
pytorch,https://github.com/pytorch/pytorch/issues/56698,2021-04-22T14:46:32Z,Convolution2D may have a numerical error.,YES,<torch.nn.Conv2d>,1.7.1,10.1
pytorch,https://github.com/pytorch/pytorch/issues/56595,2021-04-21T17:20:45Z,"TorchScript breaks (""internal assert failed"") on `torch.set_grad_enabled`",YES,torch.set_grad_enabled,1.8.0,n/a
pytorch,https://github.com/pytorch/pytorch/issues/56480,2021-04-20T13:49:51Z,shared torch.tensor with multiprocesses using python Queue cause coredump ,YES,torch.tensor,1.2.0,"The issue description does not explicitly mention a CUDA version. Therefore, no CUDA version can be identified from the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/56440,2021-04-20T02:03:37Z,Raise exception when torch.clamp min value underflows the input tensor's dtype,YES,torch.clamp,1.10.0,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/56300,2021-04-16T21:34:14Z,torch.where with input Tensor and other Scalar raises type mismatch error,YES,torch.where,1.9.0,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/56275,2021-04-16T16:14:41Z,cumprod gradgradcheck fails in fast_mode=True,YES,<torch.cumprod>,<Library Version> 1.8.0,"The issue description does not explicitly mention a specific CUDA version. Therefore, there is no CUDA version to provide based on the information given in the description."
pytorch,https://github.com/pytorch/pytorch/issues/56246,2021-04-16T09:48:13Z,Silent incorrect running with zero padding for Conv1d,YES,<torch.nn.Conv1d>,1.8,cuDNN 11.1
pytorch,https://github.com/pytorch/pytorch/issues/56237,2021-04-16T06:21:31Z,optim.Adadelta: local variable 'lr' referenced before assignment,YES,torch.optim.Adadelta,1.8.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/56235,2021-04-16T05:19:58Z,`test_cholesky_solve` gradgradcheck fails sometimes,YES,"torch.cholesky_solve  
torch.autograd.gradgradcheck  ",<Library Version> 1.9.0,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version as it is not referenced in the text."
pytorch,https://github.com/pytorch/pytorch/issues/56126,2021-04-15T07:15:52Z,AttributeError: module 'torch._C' has no attribute 'ComplexDoubleStorageBase',YES,torch._C.ComplexDoubleStorageBase,1.7.1+cu101,10.1.243
pytorch,https://github.com/pytorch/pytorch/issues/55944,2021-04-13T19:54:49Z,Sparse-sparse matrix multiplication only works with torch.sparse.mm(),YES,"torch.sparse.mm()
torch.mm()
Tensor.mm()",1.9.0a0+git2f6a806,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/55905,2021-04-13T14:18:49Z,Attempt to use jited `torch.isnan` hit internal assert,YES,torch.isnan,1.9.0a0+git5dba4ff,<CUDA version> OFF
pytorch,https://github.com/pytorch/pytorch/issues/55777,2021-04-12T08:52:13Z,torch.load is very slow with gzip.open,YES,torch.load,1.7.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/55609,2021-04-08T13:35:03Z,requires_grad does not get propagated properly when using the JIT compiler,YES,<torch.jit.script_method>,1.8.1,11.2
pytorch,https://github.com/pytorch/pytorch/issues/55549,2021-04-07T19:55:19Z,`embedding_bag` has unexpected behavior when given `offsets` that are not monotonically increasing,YES,<torch.nn.functional.embedding_bag>,1.9.0a0+gitd490e01,11.0
pytorch,https://github.com/pytorch/pytorch/issues/55541,2021-04-07T19:39:02Z,`torch.jit.interface` does not understand dunder methods (e.g. `__call__`),YES,<torch.jit.interface>,Nightly,N/A
pytorch,https://github.com/pytorch/pytorch/issues/55366,2021-04-06T10:00:00Z,bool_tensor.sum(dtype=torch.int32) creates int32-copy of the original int8 tensor ,YES,"torch.from_numpy  
torch.Tensor.sum",1.9.0,"The CUDA version mentioned in the issue description is not explicitly stated. Therefore, I cannot provide a specific CUDA version based on the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/55356,2021-04-06T05:42:05Z,torch.allclose does not allow different types for comparison,YES,torch.allclose,1.8.1+cu111,11.1
pytorch,https://github.com/pytorch/pytorch/issues/55304,2021-04-05T15:24:37Z,nn.DataParallel should raise error when provided with list of tensors,YES,torch.nn.DataParallel,1.7.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/55161,2021-04-01T17:13:45Z,Internal assert failed: `iter.device(arg).is_cuda()`,YES,torch.min,1.7.1,10.1
pytorch,https://github.com/pytorch/pytorch/issues/54861,2021-03-29T06:19:12Z,Occured error in loss.backward() when using sparse=True in Embedding layer,YES,"torch.Embedding  
torch.tensor.backward  
torch.autograd.backward",<1.7.0>,"The issue description does not explicitly mention a specific CUDA version. Therefore, based on the provided information, it is not possible to identify a CUDA version. 

If you have additional context or details about the environment or dependencies, that might help determine the CUDA version being used."
pytorch,https://github.com/pytorch/pytorch/issues/54843,2021-03-28T08:38:46Z,Inconsistent behaviour with `weight` argument in CrossEntropyLoss and BCEWithLogitsLoss,YES,"<torch.cross_entropy>
<torch.binary_cross_entropy_with_logits>",<Library Version> 1.6.0,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/54815,2021-03-27T00:19:59Z,TensorExpr `LoopNest.get_loops_for` misbehaved after loop distribution transformation,YES,torch._C._te.LoopNest.get_loops_for,<1.10.0>,"The issue description does not mention a specific CUDA version. Therefore, it is not possible to provide a CUDA version based on the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/54799,2021-03-26T21:21:37Z,"Vertices=torch.matmul(vertices.unsqueeze(0), rotations_init), RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemmStridedBatched in CentOS",YES,torch.matmul,1.8.1+cu111,CUDA version: 10.0
pytorch,https://github.com/pytorch/pytorch/issues/54716,2021-03-25T18:44:25Z,Segmentation fault in PyTorch dataloader,YES,torch.utils.data.dataloader,1.8.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/54638,2021-03-25T00:03:12Z,torch.jit.trace() fails on a GCN with sparse inputs and dense layers,YES,<torch.jit.trace>,1.7.1,CUDA version: Could not collect
pytorch,https://github.com/pytorch/pytorch/issues/54625,2021-03-24T20:45:07Z,"[torchscript] nn.Embedding(_weight), tensor.masked_fill_, and torch.autograd.grad causes wrong gradient",YES,"torch.nn.Embedding  
torch.masked_fill_  
torch.autograd.grad",1.9.0.dev20210324,No CUDA version
pytorch,https://github.com/pytorch/pytorch/issues/54622,2021-03-24T20:26:21Z,torch.pow returns incorrect value for 0^0j,YES,torch.pow,1.8.1,"The issue description does not mention any specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/54620,2021-03-24T20:05:41Z,"torch.pow(tensor, tensor) throws RuntimeError for dtype bool ",YES,torch.pow,<Library Version> 1.9.0,"The issue description does not explicitly mention a specific CUDA version. Therefore, it is not possible to provide a CUDA version based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/54578,2021-03-24T07:16:59Z,cublasSgemmStridedBatched failure when calling grad of grad,YES,torch.autograd.grad,1.8.0+cu111,11.1
pytorch,https://github.com/pytorch/pytorch/issues/54524,2021-03-23T18:07:15Z,Store created by dist.new_group doesn't appear to respect timeout,YES,<torch.distributed.new_group>,1.9.0,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/54503,2021-03-23T14:33:38Z,torch.fx may have problem for annotations,YES,torch.fx,1.8.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/54351,2021-03-19T20:53:18Z,Numpy.float64 vs native python float breaks DDP,YES,torch._C._scatter,1.7.1,11.0
pytorch,https://github.com/pytorch/pytorch/issues/54135,2021-03-17T09:35:10Z,torch.kron of a transposed input error,YES,torch.kron,1.8.0+cpu,CUDA runtime version: 9.2.148
pytorch,https://github.com/pytorch/pytorch/issues/54082,2021-03-16T19:13:14Z,Dispatch table for linalg_norm is fishy,YES,torch.linalg.norm,<Library Version> 2.0.0,"The issue description does not explicitly mention any specific CUDA version. Therefore, I cannot provide a CUDA version based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/53904,2021-03-12T16:44:24Z,[JIT] Support `torch.ones([])`,YES,torch.ones,1.13.0,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/53824,2021-03-11T16:41:53Z,[JIT] torch.jit.optimized_execution(True) greatly slows down some operations in PyTorch 1.8.0 ,YES,torch.jit.optimized_execution,1.8.0,11.1
pytorch,https://github.com/pytorch/pytorch/issues/53708,2021-03-10T13:23:48Z,torch.load with Exception,YES,<torch.load>,1.3.1,9.0
pytorch,https://github.com/pytorch/pytorch/issues/53658,2021-03-09T22:20:50Z,torch.distributed with NCCL can hang if the first operation is a barrier,YES,"<torch.distributed.barrier>
<torch.distributed.all_reduce>",1.8.0,11.1
pytorch,https://github.com/pytorch/pytorch/issues/53649,2021-03-09T21:14:21Z,vmap gradgradcheck test fails for unfold operation,YES,"torch.grad  
torch.vmap  
torch.autograd.gradcheck  
torch.autograd.grad  
torch.autograd.gradgradcheck",1.10.0,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/53625,2021-03-09T18:27:35Z,torch.arange() issue,YES,torch.arange(),1.8.1,"The issue description does not mention any specific CUDA version. It only references versions of the PyTorch library (1.8.1 and 1.7.1) but does not provide information about CUDA. Therefore, there is no CUDA version to provide."
pytorch,https://github.com/pytorch/pytorch/issues/53407,2021-03-05T21:18:49Z,"torch.matmul doesn't handle zero-sized inputs in some cases, leading to batched grad failures",YES,torch.matmul,1.12.0,The description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/53360,2021-03-05T14:00:45Z,[bug] torch.cumsum: functional and method variant promotes all ints to Long but inplace don't,YES,"torch.cumsum  
torch.cumsum_",1.8.0,"The issue description provided does not mention any specific CUDA version. Therefore, there is no CUDA version to provide based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/53178,2021-03-03T12:21:45Z,Exceedingly different F.conv2d outputs on cuda/cpu,YES,<torch.nn.functional.conv2d>,1.9.0.dev20210302,10.2
pytorch,https://github.com/pytorch/pytorch/issues/53094,2021-03-02T07:21:11Z,group conv in amp too slower,YES,torch.cuda.amp.autocast,1.7.1,10.1
pytorch,https://github.com/pytorch/pytorch/issues/52851,2021-02-25T18:21:44Z,"`embedding_bag(..., include_last_offset=True)` should always error if `offsets[-1] != input.size()`",YES,torch.nn.functional.embedding_bag,1.8.0a0,11.0
pytorch,https://github.com/pytorch/pytorch/issues/52741,2021-02-24T09:57:45Z,torch.jit.script is swallowing a rfloordiv warning,YES,torch.jit.script,1.9.0a0+8b0ca1a,11.2
pytorch,https://github.com/pytorch/pytorch/issues/52515,2021-02-19T19:48:33Z,torch.bmm incorrect with pytorch 1.7.1 and cuda 11,YES,torch.bmm,1.7.1,11.0
pytorch,https://github.com/pytorch/pytorch/issues/52415,2021-02-18T01:35:26Z,torch.autograd.Function doesn't support non-Tensor outputs,YES,torch.autograd.Function,1.11.0,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/52390,2021-02-17T22:02:11Z,autograd.functional.vjp with function that mutates inputs in-place can cause confusion,YES,torch.autograd.functional.vjp,master,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/52289,2021-02-16T01:40:55Z,DataParallel module fails to handle data with size not divisible by number of GPUs,YES,<torch.nn.DataParallel>,1.7.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/52236,2021-02-12T21:18:23Z,CosineAnnealingWarmRestarts LR scheduler fails when lash_epoch != -1,YES,<torch.optim.lr_scheduler.CosineAnnealingWarmRestarts>,1.7.0a0,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/52211,2021-02-12T16:32:59Z,Conv3D error : CUDNN_STATUS_INTERNAL_ERROR,YES,<torch.nn.Conv3d>,1.7.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/52040,2021-02-10T11:29:18Z,torch.distributed.TCPStore doesn't work with dual IPv4/IPv6 network interface,YES,torch.distributed.TCPStore,1.7.1,"11.2, 8.0"
pytorch,https://github.com/pytorch/pytorch/issues/51933,2021-02-09T02:49:54Z,Performance bugs of transpose2d on A100,YES,<torch.nn.functional.conv_transpose2d>,1.7.1+cu110,11.1
pytorch,https://github.com/pytorch/pytorch/issues/51911,2021-02-08T22:50:13Z,torch.nn.functional.grid_sample outputs NaN,YES,torch.nn.functional.grid_sample,1.8.0.dev20210126+cpu,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/51872,2021-02-08T14:17:19Z,Tensor.nonzero tries to allocate huge amount of memory for tensors on GPU with num_elements close to INT_MAX,YES,torch.Tensor.nonzero,1.7.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/51871,2021-02-08T14:06:59Z,Tensor.nonzero fails on GPU for tensors containing more than INT_MAX elements,YES,torch.nonzero,1.7.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/51849,2021-02-07T04:38:39Z,.numpy() array failes to keep original storage around,YES,<torch.utils.data.DataLoader>,1.7.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/51803,2021-02-05T20:00:31Z,torch.fx.symbolic_trace fails on torch.arange with input-dependent size,YES,torch.arange,1.8.0a0+28c5d90,11.2
pytorch,https://github.com/pytorch/pytorch/issues/51794,2021-02-05T18:12:58Z,"optim.Optimizer should copy ""params"" before modifying them",YES,torch.optim.Optimizer,1.7.1,The description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/51776,2021-02-05T09:38:51Z,BatchNorm3d error : CUDNN_STATUS_NOT_SUPPORTED,YES,torch.nn.BatchNorm3d,1.7.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/51732,2021-02-04T18:15:43Z,torch.nn.functional.ctc_loss crash(segfault) ,YES,torch.nn.functional.ctc_loss,1.8.0.dev20210126+cpu,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/51644,2021-02-03T16:53:23Z,"Segmentation Fault: Garbage collector, cuda memory",YES,torch.cuda.memory_allocated,1.12.0,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/51622,2021-02-03T07:10:28Z,"`fractional_max_pool{2, 3}d` inconsistent between CUDA and CPU",YES,"torch.nn.functional.fractional_max_pool2d  
torch.nn.functional.fractional_max_pool3d",1.10.0,"The issue description does not explicitly mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/51558,2021-02-02T13:44:49Z,torch.reshape fails to keep the memory format,YES,torch.reshape,1.7.0,10.1
pytorch,https://github.com/pytorch/pytorch/issues/51450,2021-02-01T10:58:14Z,torch.median slower than torch.sort on cpu,YES,"<torch.median>
<torch.sort>",1.7.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/51429,2021-01-31T14:09:09Z,"dist.init_process_group('nccl', ...) hangs in some combinations of pytorch+python+cuda version",YES,<torch.distributed.init_process_group>,1.7.1,10.1
pytorch,https://github.com/pytorch/pytorch/issues/51418,2021-01-30T19:44:24Z,Pytorch num_worker>0 code worked first time and then it never worked with same setting again,YES,torch.autograd.backward,1.7.0,CUDA 11.0
pytorch,https://github.com/pytorch/pytorch/issues/51134,2021-01-26T20:14:42Z,segmentation fault in torch.nn.ReplicationPad3d/2d when padding is large,YES,"torch.nn.ReplicationPad3d
torch.nn.ReplicationPad2d",1.8.0.dev20210126+cpu,None
pytorch,https://github.com/pytorch/pytorch/issues/51112,2021-01-26T16:58:18Z,"torch.tensor(x) fails inconsistently, assumes 0 index exists in 'Series'",YES,torch.LongTensor,1.8.0a0fb,9.2.0
pytorch,https://github.com/pytorch/pytorch/issues/51011,2021-01-24T13:40:57Z,DataLoader is slow in spawned processes,YES,torch.utils.data.DataLoader,1.7.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/50890,2021-01-21T17:55:54Z,"torch.unique(x, dim=1, return_inverse=True) returns inverse for only the last sub-tensor along dim",YES,torch.unique,1.8.0a0,11.1
pytorch,https://github.com/pytorch/pytorch/issues/50804,2021-01-20T11:43:49Z,Segmentation fault encountered when using nn.MultiheadAttention with v1.7.1,YES,<torch.nn.MultiheadAttention>,1.7.1,11.0
pytorch,https://github.com/pytorch/pytorch/issues/50219,2021-01-07T19:03:46Z,`requires_grad_` in `no_grad` context returns incorrect value with tensor subclasses,YES,<torch.requires_grad_>,1.7.1,11.0
pytorch,https://github.com/pytorch/pytorch/issues/50213,2021-01-07T17:32:33Z,torch.float_power out= and inplace variant errors on non-matching output dtype instead of casting,YES,torch.float_power,1.8.0.dev20201222,9.2
pytorch,https://github.com/pytorch/pytorch/issues/50098,2021-01-05T16:13:31Z,Errors when coercing complex numbers of various sizes,YES,torch.tensor,1.7.0+cu101,cu101
pytorch,https://github.com/pytorch/pytorch/issues/50097,2021-01-05T15:46:38Z,Cannot print 32-bit complex tensors,YES,torch._tensor_str._str,1.7.0+cu101,cu101
pytorch,https://github.com/pytorch/pytorch/issues/50092,2021-01-05T10:26:28Z,batch_isend_irecv: the receiving end cannot receive large tensors from the sending end correctly,YES,<torch.distributed.batch_isend_irecv>,1.8.0a0,10.1
pytorch,https://github.com/pytorch/pytorch/issues/50090,2021-01-05T08:19:35Z,Torch native_layer_norm OP out-of-bounds access,YES,torch.native_layer_norm,1.7.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/50076,2021-01-04T23:29:10Z,resize_ documentation does not match implementation when memory_format is given,YES,torch.resize_,1.11.0,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/50038,2021-01-04T11:46:43Z,Torch  _remove_batch_dim OP out-of-bounds access,YES,torch._remove_batch_dim,1.7.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/50037,2021-01-04T10:35:26Z,Torch quantized_lstm_cell op out-of-bounds access,YES,torch.quantized_lstm_cell,1.7.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/50036,2021-01-04T08:18:14Z,channels_last format convolution is slower than normal NCHW,YES,torch.nn.Conv2d,1.7.0+cu101,10.1
pytorch,https://github.com/pytorch/pytorch/issues/50034,2021-01-04T07:05:29Z,Multinomial without replacement produces samples that have zero probability,YES,torch.multinomial,<Library Version> 1.13.0,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/49885,2020-12-28T10:39:19Z,LR scheduler `get_lr()` bug,YES,"torch.optim.lr_scheduler.StepwiseLR  
torch.optim.lr_scheduler.LambdaLR  
torch.optim.lr_scheduler.MultiplicativeLR  
torch.optim.lr_scheduler.ExponentialLR  
torch.optim.lr_scheduler.StepLR  
torch.optim.lr_scheduler.MultiStepLR  ",1.7.1,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a CUDA version from the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/49882,2020-12-28T08:19:21Z,nll_loss2d: t >= 0 && t < n_classes assertion is not checked when using GPU tensors and reduction='none',YES,torch.nn.functional.cross_entropy,1.7.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/49865,2020-12-26T08:05:45Z,torch.solve on Jetson is slower than humans,YES,torch.solve,1.7.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/49752,2020-12-22T20:39:24Z,IndexError: Input _features.0.weight is undefined!,YES,torch.onnx.export,Torch 1.6.0,"The CUDA version is not explicitly mentioned in the provided issue description. Therefore, I cannot provide a specific CUDA version."
pytorch,https://github.com/pytorch/pytorch/issues/49727,2020-12-22T07:26:40Z,RandomSampler generator created in every iteration,YES,torch.utils.data.RandomSampler,<Library Version> 1.10.0,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/49630,2020-12-19T03:06:04Z,Bug in CosineAnnealingWarmRestarts,YES,<torch.optim.CosineAnnealingWarmRestarts>,1.7.0,11/6.0.21
pytorch,https://github.com/pytorch/pytorch/issues/49520,2020-12-17T00:50:14Z,Segfault in torch.bincount,YES,torch.bincount,1.5.0,10.1
pytorch,https://github.com/pytorch/pytorch/issues/49431,2020-12-15T22:09:04Z,clip_grad_norm_ performance regression,YES,torch.nn.utils.clip_grad_norm_,1.7.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/49394,2020-12-15T06:19:09Z,Apparent Memory Leak with torch.as_tensor,YES,"torch.as_tensor
torch.tensor",1.7.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/48900,2020-12-06T17:36:49Z,addmm with out= argument returns incorrect result,YES,<torch.addmm>,1.7.0+cu110,11.0
pytorch,https://github.com/pytorch/pytorch/issues/48645,2020-12-01T08:20:22Z,Can sublist a nn.Sequential subclass,YES,torch.nn.Sequential,1.6.0 and 1.7.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/48645,2020-12-01T08:20:22Z,Can sublist a nn.Sequential subclass,YES,torch.nn.Sequential,1.6.0 and 1.7.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/48641,2020-12-01T06:27:32Z,"[bug] `torch.{sinh, cosh}`: Incorrect values for vectorized path",YES,"torch.sinh  
torch.cosh",1.13.0,The provided issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/48439,2020-11-25T07:16:16Z,cudnn convolution modifies the input Tensor metadata inplace when it tries to `.resize_()` it,YES,torch.utils.checkpoint.checkpoint,1.7.0+cu110,11.0
pytorch,https://github.com/pytorch/pytorch/issues/48419,2020-11-24T13:08:51Z,pin_memory=True in DataLoader converts a tuple to list automatically,YES,torch.utils.data.DataLoader,1.7.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/48400,2020-11-23T19:00:17Z,conv_transpose3d returns different result when the input and kernel are mkldnn tensors,YES,<torch.nn.functional.conv_transpose3d>,1.7.0,11.0
pytorch,https://github.com/pytorch/pytorch/issues/48273,2020-11-19T21:01:12Z,torch.multinomial example is incorrect,YES,torch.multinomial,1.13.0,"The issue description does not mention any specific CUDA version. Therefore, there is no CUDA version to provide from the text given."
pytorch,https://github.com/pytorch/pytorch/issues/48251,2020-11-19T10:34:50Z,torch.eye(d) is slow and hogs cpu for d >= 182,YES,torch.eye,1.6.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/48163,2020-11-18T07:04:32Z,nn.functional.interpolate backward in fp16 is extremely slow,YES,torch.nn.functional.interpolate,1.7,cuda9.2
pytorch,https://github.com/pytorch/pytorch/issues/48000,2020-11-16T06:19:48Z,Inconsistent complex results with NumPy when computing non-positive power of 0,YES,torch.tensor,1.8.0a0,11.0
pytorch,https://github.com/pytorch/pytorch/issues/47560,2020-11-07T15:39:37Z,Minumul LR is never reached in ,YES,"torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
torch.optim.lr_scheduler.CosineAnnealingLR",master f23a2a11153a7feba0c3c9aa01f68996569144af,"The issue description does not explicitly mention a CUDA version. Therefore, it is not possible to provide a CUDA version based on the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/47481,2020-11-06T00:55:11Z,[fx] scripting a model with tensor list as input fails,YES,"torch.fx.symbolic_trace  
torch.jit.script  
torch.nn.Module.forward",1.12.0,The description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/47439,2020-11-05T13:25:06Z,how to use torch.utils.checkpoint + gru with variable length sequence?,YES,<torch.utils.checkpoint>,<Library Version> 1.4.0,The description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/47378,2020-11-04T19:08:19Z,torch.vmap giving INTERNAL ASSERT FAILED error,YES,<torch.vmap>,1.8.0a0,10.1
pytorch,https://github.com/pytorch/pytorch/issues/47260,2020-11-03T04:03:44Z,DDP doesn't work with retain_graph = True,YES,"<API Name>
torch.nn.parallel.DistributedDataParallel","<Library Version>
1.12.0","The issue description provided does not mention a specific CUDA version. Therefore, it is not possible to identify a CUDA version from the information given."
pytorch,https://github.com/pytorch/pytorch/issues/47028,2020-10-28T22:51:49Z,torch.autograd.backward() fails to sync with other stream,YES,torch.autograd.backward(),1.8.1,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/46999,2020-10-28T17:26:03Z,"torch.sum(tensor(2.), dim=0) (and probably other reduction functions) doesn't make sense",YES,"torch.sum
torch.nn.functional.softmax",master,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/46918,2020-10-27T15:08:15Z,Jacobians computed by autograd.functional.jacobian with compute_graph sometimes set requires_grad True,YES,torch.autograd.functional.jacobian,1.8.0.dev20201027,10.2
pytorch,https://github.com/pytorch/pytorch/issues/46753,2020-10-23T02:12:05Z,Memory leak when creating new tensors inside nn.DataParallel on multiple GPUs,YES,"<API Name>
torch.nn.DataParallel",1.6.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/46739,2020-10-22T20:54:22Z,torch.utils.tensorboard.SummaryWriter.add_embedding fails for some label_img sizes,YES,<torch.utils.tensorboard.SummaryWriter.add_embedding>,1.6.0,10.1
pytorch,https://github.com/pytorch/pytorch/issues/46604,2020-10-20T18:52:05Z,torch.concat doesn't raise an error in a quantized model,YES,"torch.concat  
torch.cat  
torch.quantization.get_default_qat_qconfig  
torch.quantization.prepare_qat  
torch.quantization.convert  
torch.jit.trace  
torch.jit.export_opnames  ",<Library Version> 1.0,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a CUDA version as requested."
pytorch,https://github.com/pytorch/pytorch/issues/46375,2020-10-15T02:54:19Z,JIT: error in LSTM with `flatten_parameters`,YES,torch.nn.LSTM.flatten_parameters,1.5.0+cu101,10.1
pytorch,https://github.com/pytorch/pytorch/issues/46225,2020-10-13T00:20:33Z,torch.mode when input has nans,YES,torch.mode,1.6.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/46178,2020-10-12T11:12:06Z,"torch.cuda.amp!  when I use @autocast() on DCN(DeformConv), the error ""RuntimeError:expect scalar type Float but Half """,YES,torch.cuda.amp.autocast,1.6.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/46166,2020-10-11T19:39:39Z,Error with DistributedDataParallel with specific model,YES,<torch.nn.parallel.DistributedDataParallel>,1.6.0,10.1
pytorch,https://github.com/pytorch/pytorch/issues/46155,2020-10-11T10:05:13Z,Complex backward returns NaN values,YES,<torch.nn.modules.module>,<1.6.x>,"CUDA 10.1, CUDA 10.2, CUDA 11.1"
pytorch,https://github.com/pytorch/pytorch/issues/45910,2020-10-06T17:29:48Z,with torch.cuda.amp.autocast() get out of memory error when using with torch.no_grad() during validation,YES,"torch.cuda.amp.autocast  
torch.no_grad",<Library Version> 1.10.0,"The description does not specify a particular CUDA version. Therefore, no CUDA version can be identified from the provided text."
pytorch,https://github.com/pytorch/pytorch/issues/45840,2020-10-05T14:46:49Z,Error with DataParallel and dataclass,YES,<torch.nn.DataParallel>,1.6.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/45769,2020-10-02T20:30:31Z,F.conv2d() causes RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR ,YES,torch.F.conv2d,1.6.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/45690,2020-10-01T20:00:14Z,Complex and real results do not agree when computing reciprocal or pow(-1) of 0,YES,"torch.pow  
torch.reciprocal",1.7.0a0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/45569,2020-09-30T14:06:55Z,Distutils Error in torch.hub Load(),YES,torch.hub.load,1.0.0,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/45321,2020-09-25T01:08:45Z,Memory leak when using share_memory_ on cuda device,YES,torch.share_memory_,1.7.0a0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/45136,2020-09-22T16:23:53Z,Multiple torch.load in one file,YES,torch.load,1.6.0,CUDA runtime version: 9.2.148
pytorch,https://github.com/pytorch/pytorch/issues/45125,2020-09-22T14:03:32Z,Boolean indexing of an ndarray with a torch.tensor mask breaks for size=1 ,YES,torch.ones,1.6.0,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/44631,2020-09-14T09:14:46Z,Multi-process Dataloader and multi-parameter exceptions,YES,torch.utils.data.DataLoader,1.11.0,"The issue description provided does not mention a specific CUDA version. Therefore, there is no CUDA version to provide."
pytorch,https://github.com/pytorch/pytorch/issues/44547,2020-09-11T10:49:52Z, floating point exception (core dumped) in training 10^4 steps,YES,"torch::autograd::generated::CudnnConvolutionBackward
at::native::cudnn_convolution_backward_input
at::cudnn_convolution_backward
torch::autograd::generated::CudnnConvolutionBackward::apply
torch::autograd::Engine::evaluate_function
torch::autograd::Engine::thread_main
torch::autograd::Engine::thread_init
torch::autograd::python::PythonEngine::thread_init",1.5,10.2
pytorch,https://github.com/pytorch/pytorch/issues/44528,2020-09-11T02:08:56Z,DataLoader consumes extremely large shared memory (shm) in its initialization.,YES,<torch.utils.data.DataLoader>,1.5.0,10.1
pytorch,https://github.com/pytorch/pytorch/issues/44169,2020-09-04T02:23:15Z,Deadlock with RPC and dist.barrier() for TensorPipeAgent and NCCL.,YES,"torch.rpc.rpc_sync  
torch.distributed.barrier  
torch.distributed.TensorPipeAgent",<Library Version> 1.10.0,"The issue description provided does not mention a specific CUDA version. Therefore, there is no CUDA version to identify from the text."
pytorch,https://github.com/pytorch/pytorch/issues/43765,2020-08-28T13:02:52Z,JIT fails sanity checks during tracing torch.rand_like,YES,torch.rand_like,1.6.0,CUDA version is not explicitly mentioned in the provided issue description.
pytorch,https://github.com/pytorch/pytorch/issues/43754,2020-08-28T00:54:00Z,"Using Dataparallel with multi input error.  Input and parameter tensors are not at the same device, found input tensor at cpu and parameter tensor at cuda:0",YES,torch.nn.DataParallel,<Library Version> 1.9.0,"The issue description does not explicitly mention a specific CUDA version. Therefore, it is not possible to provide a CUDA version based solely on the given information."
pytorch,https://github.com/pytorch/pytorch/issues/43579,2020-08-25T21:57:44Z,"For the same complex dtype and same value, comparing a PyTorch tensor with a NumPy array results in False",YES,"torch.tensor  
torch.from_numpy",1.7.0.dev20200819+cu101,10.1
pytorch,https://github.com/pytorch/pytorch/issues/43567,2020-08-25T19:14:15Z,`torch.svd()` CUDA gives incorrect results when input contains `nan`,YES,torch.svd,1.7.0a0+5ff1ce3,10.2
pytorch,https://github.com/pytorch/pytorch/issues/43115,2020-08-15T22:38:22Z,torch.multinomial with replacement=True produces inaccurate results for large number of categories,YES,<torch.multinomial>,1.8.0,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/43005,2020-08-13T13:00:24Z,`torch.distributions.Categorical` crashes with illegal instruction,YES,torch.distributions.Categorical,1.6.0,CUDA runtime version: 9.0.176
pytorch,https://github.com/pytorch/pytorch/issues/42838,2020-08-11T00:02:09Z,"RuntimeError: ""triangular_solve_cuda"" not implemented for 'Half'",YES,torch.triangular_solve,1.7.0,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/42812,2020-08-10T16:52:11Z,checkpoint_sequential breaks backpropagation,YES,<torch.checkpoint_sequential>,1.6.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/42779,2020-08-08T14:19:05Z,[quant] Quantized AdaptivePool3d is much slower for ChannelsLast3d.,YES,torch.nn.quantized.functional.adaptive_avg_pool3d,1.7.0a0+73642d9,10.2
pytorch,https://github.com/pytorch/pytorch/issues/42663,2020-08-06T07:42:11Z,Why Conv3D is slower than Conv2D when its flops is smaller than Conv2D,YES,"torch.nn.Conv2d
torch.nn.Conv3d",<Library Version> 1.11.0,"The issue description does not mention a specific CUDA version. Therefore, the answer is:

<No CUDA version mentioned>"
pytorch,https://github.com/pytorch/pytorch/issues/42625,2020-08-05T19:47:01Z,F.grid_sample produces weird results on single-pixel images,YES,torch.nn.functional.grid_sample,master,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/42444,2020-08-03T09:47:52Z,torch.random.randperm stuck in multiprocess,YES,torch.random.randperm,1.5.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/42109,2020-07-27T15:21:53Z,Slow `index_add_` on `torch.long` tensors ,YES,<torch.index_add_>,1.5.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/42074,2020-07-26T01:48:59Z,Memory usage of torch.nn.functional.interpolate increased with v1.5.0 when run on numpy input,YES,torch.nn.functional.interpolate,1.5.0,10.1
pytorch,https://github.com/pytorch/pytorch/issues/42049,2020-07-25T05:42:09Z,"Code hangs when using `set_start_method('spawn', force=True)` in `torch.multiprocessing.pool`",YES,<torch.multiprocessing.pool>,1.5.1+cu101,10.1
pytorch,https://github.com/pytorch/pytorch/issues/42039,2020-07-24T22:01:24Z,Segmentation fault (core dumped) when running optimize_for_mobile,YES,<torch.quantization.optimize_for_mobile>,1.7.0a0+42a0b51,CUDA version: Could not collect
pytorch,https://github.com/pytorch/pytorch/issues/41970,2020-07-24T02:32:16Z,torch.random.fork tries to initialize cuda even when no cuda devices are available,YES,torch.random.fork_rng,1.5.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/41869,2020-07-22T20:29:37Z,TorchScript pack_padded_sequence and  pad_packed_sequence run time error,YES,"torch.pack_padded_sequence  
torch.pad_packed_sequence",1.8.1,The provided issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/41694,2020-07-20T21:21:17Z,Torch.multiprocessing.spawn can deadlock,YES,<torch.multiprocessing.spawn>,1.5.0,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/41571,2020-07-17T01:18:54Z,"torch.cuda.BoolTensor uses 8 bits per element, not 1 bit as reported by element_size()",YES,torch.cuda.BoolTensor,1.5.1,10.1
pytorch,https://github.com/pytorch/pytorch/issues/41512,2020-07-16T00:42:51Z,Tensor.new_tensor is not supported,YES,torch.new_tensor,1.9.0,<Cuda version>
pytorch,https://github.com/pytorch/pytorch/issues/41508,2020-07-16T00:03:06Z,nn.MultiheadAttention causes gradients to become NaN under some use cases,YES,<torch.nn.MultiheadAttention>,1.5.1,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/41327,2020-07-13T06:15:32Z,torch.nn.parallel.scatter_gather.gather can't gather outputs that are dataclasses,YES,torch.nn.parallel.scatter_gather.gather,1.5.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/41325,2020-07-12T22:44:50Z,torch.combinations() - Tried to allocate 7869836414.81 GiB,YES,torch.combinations,1.5.0,10.1
pytorch,https://github.com/pytorch/pytorch/issues/41244,2020-07-10T12:00:25Z,torch.tan(complex) on CUDA doesn't handle nonfinite values properly,YES,torch.tan,<Library Version> 1.10.0,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/41128,2020-07-08T17:23:27Z,Memory bug for backward on torch.sparse.mm? ,YES,<torch.sparse.mm>,1.4.0,10.1
pytorch,https://github.com/pytorch/pytorch/issues/40967,2020-07-03T15:06:49Z,Inconsistent behaviour when parameter appears multiple times in parameter list,YES,torch.optim.SGD,1.5,"The issue description does not mention any specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/40932,2020-07-02T18:57:50Z,MultiheadAttention set(-inf) cause 'Nan' in loss computing,YES,<torch.nn.MultiheadAttention>,1.5.1,7.6.4
pytorch,https://github.com/pytorch/pytorch/issues/40914,2020-07-02T13:33:28Z,Inconsistent handling of torch.Size.__add__,YES,torch.Size.__add__,1.10.0,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version to provide."
pytorch,https://github.com/pytorch/pytorch/issues/40756,2020-06-30T02:13:28Z,RuntimeError: rois.device().is_cpu() ASSERT FAILED at /vision/torchvision/csrc/cpu/ROIAlign_cpu.cpp:386,YES,torchvision.ops.roi_align,1.1.0,9.0.176
pytorch,https://github.com/pytorch/pytorch/issues/40667,2020-06-27T22:16:07Z,Segmentation fault in forward pass using DataParallel and multiple GPUs,YES,torch.nn.DataParallel,1.5.1,10.1
pytorch,https://github.com/pytorch/pytorch/issues/40590,2020-06-25T21:43:54Z,Inconsistent behavior between numpy.exp and torch.exp on CPU for complex numbers,YES,torch.exp,1.6.0.dev20200625+cu101,10.1
pytorch,https://github.com/pytorch/pytorch/issues/40561,2020-06-25T14:26:57Z,Cannot manually assign a tensor to .grad from TorchScript,YES,torch.jit.script,1.5.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/40548,2020-06-25T01:30:14Z,"Embedding with DataParallel can return ""incomplete"" results",YES,"torch.Embedding  
torch.DataParallel",1.5 ~ 1.6.x-dev,10.2
pytorch,https://github.com/pytorch/pytorch/issues/40497,2020-06-24T09:33:21Z,Mixed precision causes NaN loss,YES,<torch.autocast>,1.6.0.dev20200623,10.2
pytorch,https://github.com/pytorch/pytorch/issues/40457,2020-06-23T19:39:01Z,DataParallel with Torch 1.5,YES,torch.nn.DataParallel,1.5.1,10.2
pytorch,https://github.com/pytorch/pytorch/issues/40147,2020-06-17T04:35:50Z,SyncBatchNorm doesn't work when I set track_running_stats False,YES,torch.SyncBatchNorm,<Library Version> 1.7.0,"The issue description provided does not mention any specific CUDA version. Therefore, the response is:

<CUDA version>"
pytorch,https://github.com/pytorch/pytorch/issues/39632,2020-06-07T00:14:15Z,F.affine_grid dispatch async issue,YES,torch.F.affine_grid,1.5.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/39443,2020-06-03T12:07:26Z,PyTorch multiprocessing.spawn seems slow with list of tensors,YES,<torch.multiprocessing.spawn>,1.4.0,10.1
pytorch,https://github.com/pytorch/pytorch/issues/39437,2020-06-03T08:32:56Z,error when specifying sparse=True in embedding,YES,torch.embedding,1.12.0,"The description does not mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the information given."
pytorch,https://github.com/pytorch/pytorch/issues/39435,2020-06-03T07:19:59Z,BCEWithLogitsLoss() not equal to BCELoss() with sigmoid(),YES,"<torch.nn.BCEWithLogitsLoss>
<torch.nn.BCELoss>
<torch.sigmoid>",1.5.0+cu101,cu101
pytorch,https://github.com/pytorch/pytorch/issues/39224,2020-05-29T05:45:00Z,Negative stride values in `as_strided`,YES,torch.as_strided,1.5.0+cu101,10.1
pytorch,https://github.com/pytorch/pytorch/issues/38709,2020-05-19T05:51:10Z,NaN Loss for FasterRCNN on Multiclass Object Detection on Custom Dataset COCO,YES,<torchvision.models.FasterRCNN>,1.5.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/38640,2020-05-18T09:51:46Z,Maxunpool seems to give a weird error message,YES,<torch.MaxUnpool2d>,1.10.0,"The issue description does not mention a specific CUDA version. 

<Cuda version>"
pytorch,https://github.com/pytorch/pytorch/issues/38614,2020-05-16T21:43:51Z,`SummaryWriter.add_graph` borks with simple example,YES,<torch.utils.tensorboard.SummaryWriter.add_graph>,<Library Version> 1.4.0,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a specific CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/38553,2020-05-15T16:43:26Z,"When I use cuda(), wg = th.matmul(extra_obs, extra_obs.transpose(-2, -1)) take a mistake",YES,"torch.cuda  
torch.matmul",torch 0.4.1,cuda9.0
pytorch,https://github.com/pytorch/pytorch/issues/38437,2020-05-13T21:37:27Z,[JIT] torch.tensor needs a Tensor overload,YES,torch.tensor,1.8.0,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/38051,2020-05-07T21:21:02Z,Div by zero error not triggered and inf not returned when dividing by 0 for some dtypes,YES,<torch.tensor>,1.5.0+cu101,10.1
pytorch,https://github.com/pytorch/pytorch/issues/38019,2020-05-07T16:22:44Z,"torch.cuda.nccl.init_rank does not handle ""uid"" properly, causing runtime error",YES,torch.cuda.nccl.init_rank,1.3.1+cu92,9.2.148
pytorch,https://github.com/pytorch/pytorch/issues/37985,2020-05-07T00:20:17Z,torch.save incompatible with lzma file,YES,"torch.save  
torch.load",1.5.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/37814,2020-05-05T06:02:18Z,DistributedDataParallel does not support Modules that take no inputs.,YES,<torch.nn.parallel.DistributedDataParallel>,1.5.0,10.1
pytorch,https://github.com/pytorch/pytorch/issues/37734,2020-05-03T15:37:48Z,torch.cdist returns inconsistent result,YES,torch.cdist,1.7.1,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/37585,2020-04-30T16:31:35Z,Unable to use torch.det() inside nn.DataParallel with multiple gpus,YES,"torch.det  
nn.DataParallel",1.5.0,9.2
pytorch,https://github.com/pytorch/pytorch/issues/37246,2020-04-24T18:32:03Z,`max_norm` parameter on nn.Embedding will fail inside nn.DataParallel,YES,torch.nn.Embedding,1.5.0,10.2
pytorch,https://github.com/pytorch/pytorch/issues/37149,2020-04-23T14:39:38Z,Cuda profiler + DataParallel + manual profiling start = strange profiling overhead pattern,YES,<torch.cuda.profiler.start>,1.4.0,CUDA 10.1
pytorch,https://github.com/pytorch/pytorch/issues/37092,2020-04-22T18:29:31Z,nn.Bilinear cannot be used inside nn.Sequential,YES,<torch.nn.Bilinear>,1.5.0.dev20200210+cu100,10.0
pytorch,https://github.com/pytorch/pytorch/issues/36922,2020-04-20T14:49:48Z,Multiprocess DataLoader with DLPack conversion sometimes corrupts memory,YES,"torch.utils.dlpack.to_dlpack  
torch.utils.dlpack.from_dlpack",1.4.0,10.1
pytorch,https://github.com/pytorch/pytorch/issues/36871,2020-04-18T17:31:21Z,Stochasticity for DistributedDataParallel on CPU but not on GPU,YES,torch.manual_seed,1.4.0,10.1
pytorch,https://github.com/pytorch/pytorch/issues/36706,2020-04-16T02:41:52Z,Reduction for `torch.int8` is super slow on CUDA,YES,torch.sum,1.6.0a0+2f5b523,10.2
pytorch,https://github.com/pytorch/pytorch/issues/36436,2020-04-11T15:44:36Z,"After `create_graph=True`, calculating `backward()` on sparse Tensor fails",YES,<torch.autograd.grad>,1.4.0,10.1
pytorch,https://github.com/pytorch/pytorch/issues/35655,2020-03-30T04:23:05Z,Wrong conv2d output on GPU when kernel has many zeros,YES,torch.nn.functional.conv2d,1.4.0,10.0
pytorch,https://github.com/pytorch/pytorch/issues/35208,2020-03-23T12:57:42Z,The BatchNorm error in `DataParallel`,YES,torch.nn.BatchNorm,<1.5.0>,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/34977,2020-03-18T20:27:27Z,`conv2d` is slow with specific shapes of channels_last tensors,YES,torch.conv2d,<Library Version> 1.6.0,10.0
pytorch,https://github.com/pytorch/pytorch/issues/34544,2020-03-10T18:24:57Z,Strange behaviour of F.interpolate with bicubic mode.,YES,<torch.F.interpolate>,1.1.0,"The issue description does not explicitly mention a CUDA version. Therefore, I cannot provide a CUDA version based on the given text."
pytorch,https://github.com/pytorch/pytorch/issues/34257,2020-03-04T20:20:54Z,CustomFromMask pruning stores a copy of the user-provided mask,YES,<torch.nn.utils.prune.custom_from_mask>,1.5.0.dev20200303,10.1
pytorch,https://github.com/pytorch/pytorch/issues/34001,2020-02-29T04:49:26Z,_broadcast_coalesced_reshape doesn't respect zero-size tensor,YES,torch.nn.parallel.replicate._broadcast_coalesced_reshape,1.4.0,10.1
pytorch,https://github.com/pytorch/pytorch/issues/33571,2020-02-20T19:48:11Z,empty_sparse shouldn't be called with memory layout but is,YES,<torch.empty_sparse>,1.9.0,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/33491,2020-02-19T11:56:49Z,jit.trace checker fails for LSTM ,YES,torch.jit.trace,1.4.0,10.0
pytorch,https://github.com/pytorch/pytorch/issues/33373,2020-02-15T08:25:37Z,torch.clamp_ not inplace during backward,YES,torch.clamp_,1.3.0,10.1.243
pytorch,https://github.com/pytorch/pytorch/issues/33360,2020-02-14T22:04:12Z,Dropout of attention weights in function F.multi_head_attention_forward() breaks sum-to-1 constraint,YES,torch.nn.functional.dropout,1.4.0,"The issue description does not explicitly mention a specific CUDA version. Therefore, I cannot provide a CUDA version based on the information provided."
pytorch,https://github.com/pytorch/pytorch/issues/32695,2020-01-28T12:44:05Z,torch.tensordot has inconsistent signature with torch script,YES,torch.tensordot,1.3.1,10.1.243
pytorch,https://github.com/pytorch/pytorch/issues/32444,2020-01-21T18:07:19Z,DataParallel does not work with sparse parameters,YES,<torch.nn.parallel.replicate>,1.4.0,10.0
pytorch,https://github.com/pytorch/pytorch/issues/32370,2020-01-17T20:23:34Z,Backward `Functional.conv3d` is slow when cuDNN is enabled,YES,torch.functional.conv3d,1.4.0,10.1
pytorch,https://github.com/pytorch/pytorch/issues/32264,2020-01-16T03:18:14Z,torch.nn.functional.threshold not work with LongTensor,YES,"<torch.nn.functional.threshold>  
<torch.nn.Threshold>",1.0.0,9.0.176
pytorch,https://github.com/pytorch/pytorch/issues/32162,2020-01-14T08:10:42Z,Using Tensor.to(device) after distributed all_reduce intermittently causes deadlock with NCCL ,YES,"<torch.distributed.all_reduce>
<torch.Tensor.to>",1.3.1,10.1.243
pytorch,https://github.com/pytorch/pytorch/issues/31895,2020-01-06T21:19:00Z,"torch.masked_select out argument can easily be misused, because output shape is dynamically computed",YES,torch.masked_select,1.3.1,10.1.243
pytorch,https://github.com/pytorch/pytorch/issues/31866,2020-01-05T03:52:10Z,Negative indices in chunk could cause Out of Range access on loss.backward in JIT,YES,torch.chunk,1.3.1,10.1.243
pytorch,https://github.com/pytorch/pytorch/issues/31779,2020-01-02T17:25:49Z,torch.poisson returns floating point tensor,YES,torch.poisson,1.10.0,The description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/31758,2020-01-01T12:15:03Z,DataLoader: Segmentation Fault (core dumped),YES,<torch.DataLoader>,1.1.0,CUDA version: 10.0.130
pytorch,https://github.com/pytorch/pytorch/issues/31689,2019-12-29T11:23:32Z,DataLoader does not consider default floating point type,YES,"torch.set_default_dtype  
torch.utils.data.DataLoader  
torch.utils.data._utils.collate.default_collate",1.3.1,10.0.130
pytorch,https://github.com/pytorch/pytorch/issues/31615,2019-12-26T02:02:43Z,'torch.load' report 'bad pickle data',YES,torch.load,1.0.1,no
pytorch,https://github.com/pytorch/pytorch/issues/31557,2019-12-22T15:28:23Z,[docs] F.ctc_loss docs to warn clearly about invalid inf-causing inputs; zero_infinity to become enabled by default,YES,torch.F.ctc_loss,1.9.0,"The issue description provided does not mention any specific CUDA version. Therefore, there is no CUDA version to identify from the text."
pytorch,https://github.com/pytorch/pytorch/issues/31478,2019-12-19T17:10:03Z,DataParallel has different tensor copy behavior if batch size = 1,YES,torch.nn.parallel.DataParallel,1.9.0,"The issue description does not mention any specific CUDA version. Therefore, there is no CUDA version to provide."
pytorch,https://github.com/pytorch/pytorch/issues/30968,2019-12-09T17:01:02Z,Categorical.sample too slow,YES,"torch.distributions.categorical.Categorical.sample
torch.multinomial",1.3.1,10.0.130
pytorch,https://github.com/pytorch/pytorch/issues/30934,2019-12-07T16:11:34Z,Spurious negative output in convolution of positive tensors,YES,<torch.nn.functional.conv2d>,1.1.0,9.0
pytorch,https://github.com/pytorch/pytorch/issues/30899,2019-12-06T20:00:51Z,Adding max_norm constraint to an Embedding layer leads to an error,YES,torch.nn.Embedding,1.3.1,10.1.243
pytorch,https://github.com/pytorch/pytorch/issues/30754,2019-12-04T21:29:41Z,"JIT, nn.utils.weight_norm and {save,load}_state_dict produce wrong results",YES,torch.nn.utils.weight_norm,1.4.0a0+19cd90d,10.1.243
pytorch,https://github.com/pytorch/pytorch/issues/30569,2019-11-29T14:11:05Z,"Assertion `index >= -sizes[i] && index < sizes[i] && ""index out of bounds""` failed.",YES,"torch.randperm  
torch.randn",1.4.0a0+dd52f50,9.0
pytorch,https://github.com/pytorch/pytorch/issues/30565,2019-11-29T07:44:21Z,`F.interpolate` returns unexpected result when dealing with output size `1`,YES,"```
F.interpolate
F.avg_pool2d
```",1.2.0,10.0.130
pytorch,https://github.com/pytorch/pytorch/issues/30461,2019-11-26T11:27:46Z,`torch.multiprocessing.spawn` fails when `join=False`,YES,<torch.multiprocessing.spawn>,1.3.0a0+f25d01e,10.0.130
pytorch,https://github.com/pytorch/pytorch/issues/29843,2019-11-14T21:44:09Z,torch.distributions.normal.Normal is not JIT supported,YES,torch.distributions.normal.Normal,1.10.0,"The issue description does not mention a specific CUDA version. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/29816,2019-11-14T15:53:27Z,AdamSparse fails to run,YES,torch.optim.SparseAdam,1.3.1,No CUDA
pytorch,https://github.com/pytorch/pytorch/issues/29510,2019-11-09T21:34:26Z,torch.stack: bad shape error message,YES,torch.stack,<1.3.0>,10.1.243
pytorch,https://github.com/pytorch/pytorch/issues/29429,2019-11-08T01:10:37Z,NLLLoss reduce=True returning nan in float16,YES,torch.nn.NLLLoss,torch 1.3.1,"The issue description does not explicitly mention a specific CUDA version. Therefore, no CUDA version can be identified from the provided information."
pytorch,https://github.com/pytorch/pytorch/issues/29372,2019-11-07T13:52:19Z,torch.std() returns nan for single item tensors.,YES,torch.std,PyTorch 1.3,The issue description does not mention a specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/29369,2019-11-07T12:00:03Z,nn.Conv(n)d constructor doesn't check for the number of kernel dimensions,YES,"<torch.nn.Conv1d>
<torch.nn.Conv2d>",1.2.0,"10.0.130, 10.1.243"
pytorch,https://github.com/pytorch/pytorch/issues/29137,2019-11-04T18:37:41Z,"torch.sum(tensor, dim=()) is different from np.sum(arr, axis=())",YES,torch.sum,<Library Version> master,The issue description does not mention any specific CUDA version.
pytorch,https://github.com/pytorch/pytorch/issues/28993,2019-10-31T20:58:17Z,"torch.mean(x, dims=[]) has incorrect gradient in 1.2",YES,torch.mean,1.2,"The issue description does not mention a specific CUDA version. It primarily focuses on the PyTorch version (1.2) and the bug related to the gradient calculation. Therefore, there is no CUDA version referenced in the description."
pytorch,https://github.com/pytorch/pytorch/issues/8837,2018-06-25T02:11:05Z,Inconsistency in implementation of _LRScheduler ,YES,<torch.optim.lr_scheduler._LRScheduler>,<Library Version> 1.9.0,"The issue description provided does not mention any specific CUDA version. Therefore, there is no CUDA version to identify from the text."
pytorch,https://github.com/pytorch/pytorch/issues/5280,2018-02-17T00:50:35Z,BatchNorm1d raises RuntimeError (CUDNN_STATUS_BAD_PARAM) on 3D input.,YES,<torch.nn.BatchNorm1d>,0.3.1,8.0
pytorch,https://github.com/pytorch/pytorch/issues/5157,2018-02-09T16:03:39Z,BCELoss - weight parameter shape incorrect,YES,<torch.nn.BCELoss>,0.4.0a0+492e26f,"The description does not mention a specific CUDA version. Therefore, the answer is:

<none>"
pytorch,https://github.com/pytorch/pytorch/issues/2539,2017-08-25T18:27:41Z,"DataLoader converts cuda FloatTensor into cpu DoubleTensor when shape is (n,)",YES,"<torch.DataLoader>  
<torch.TensorDataset>",<0.2.0>,0.2.0
